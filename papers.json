[
  {
    "title": "olmOCR 2: Unit Test Rewards for Document OCR",
    "authors": [
      "Jake Poznanski",
      "Luca Soldaini",
      "Kyle Lo"
    ],
    "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifi...",
    "published": "Oct 22",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2510.19817v1",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo"
    ]
  },
  {
    "title": "Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation",
    "authors": [
      "Yejin Choi",
      "Jaewoo Park",
      "Janghan Yoon",
      "Saejin Kim",
      "Jaehyun Jeon",
      "Youngjae Yu"
    ],
    "summary": "Rapid advances in Multimodal Large Language Models (MLLMs) have expanded information retrieval beyond purely textual inputs, enabling retrieval from complex real world documents that combine text and visuals. However, most documents are private either owned by individuals or confined within corporat...",
    "published": "Aug 23",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2508.17079v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Prompting as Scientific Inquiry",
    "authors": [
      "Ari Holtzman",
      "Chenhao Tan"
    ],
    "summary": "Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as s...",
    "published": "Jun 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2507.00163v2",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
    "authors": [
      "Alex L. Zhang",
      "Thomas L. Griffiths",
      "Karthik R. Narasimhan",
      "Ofir Press"
    ],
    "summary": "Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafte...",
    "published": "May 23",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2505.18134v2",
    "queried_author": "Karthik R Narasimhan",
    "matching_authors": [
      "Karthik R Narasimhan"
    ]
  },
  {
    "title": "Do Language Models Use Their Depth Efficiently?",
    "authors": [
      "R\u00f3bert Csord\u00e1s",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "summary": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same ...",
    "published": "May 20",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2505.13898v3",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
    "authors": [
      "Sumuk Shashidhar",
      "Cl\u00e9mentine Fourrier",
      "Alina Lozovskia",
      "Thomas Wolf",
      "Gokhan Tur",
      "Dilek Hakkani-T\u00fcr"
    ],
    "summary": "Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. We introd...",
    "published": "Apr 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2504.01833v1",
    "queried_author": "Cl\u00e9mentine Fourrier",
    "matching_authors": [
      "Cl\u00e9mentine Fourrier"
    ]
  },
  {
    "title": "Privacy Auditing of Large Language Models",
    "authors": [
      "Ashwinee Panda",
      "Xinyu Tang",
      "Milad Nasr",
      "Christopher A. Choquette-Choo",
      "Prateek Mittal"
    ],
    "summary": "Current techniques for privacy auditing of large language models (LLMs) have limited efficacy -- they rely on basic approaches to generate canaries which leads to weak membership inference attacks that in turn give loose lower bounds on the empirical privacy leakage. We develop canaries that are far...",
    "published": "Mar 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2503.06808v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "authors": [
      "Hamish Ivison",
      "Muru Zhang",
      "Faeze Brahman",
      "Pang Wei Koh",
      "Pradeep Dasigi"
    ],
    "summary": "Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typical...",
    "published": "Mar 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2503.01807v2",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison"
    ]
  },
  {
    "title": "High-Dimensional Markov-switching Ordinary Differential Processes",
    "authors": [
      "Katherine Tsai",
      "Mladen Kolar",
      "Sanmi Koyejo"
    ],
    "summary": "We investigate the parameter recovery of Markov-switching ordinary differential processes from discrete observations, where the differential equations are nonlinear additive models. This framework has been widely applied in biological systems, control systems, and other domains; however, limited res...",
    "published": "Dec 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2501.00087v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Towards Visual Text Design Transfer Across Languages",
    "authors": [
      "Yejin Choi",
      "Jiwan Chung",
      "Sumin Shim",
      "Giyeong Oh",
      "Youngjae Yu"
    ],
    "summary": "Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthet...",
    "published": "Oct 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2410.18823v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education",
    "authors": [
      "Seungyoon Kim",
      "Seungone Kim"
    ],
    "summary": "Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by providing direct feedback to enhance writing skills, altho...",
    "published": "Jul 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2407.17022v1",
    "queried_author": "Seungone Kim",
    "matching_authors": [
      "Seungone Kim"
    ]
  },
  {
    "title": "In-Context Learning of Energy Functions",
    "authors": [
      "Rylan Schaeffer",
      "Mikail Khona",
      "Sanmi Koyejo"
    ],
    "summary": "In-context learning is a powerful capability of certain machine learning models that arguably underpins the success of today's frontier AI models. However, in-context learning is critically limited to settings where the in-context distribution of interest $p_\u03b8^{ICL}( x|\\mathcal{D})$ can be straightf...",
    "published": "Jun 18",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2406.12785v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties",
    "authors": [
      "Evan Lohn",
      "Sean Welleck"
    ],
    "summary": "AI agents have shown initial promise in automating mathematical theorem proving in proof assistants such as Lean. The same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. Automating the writing of code, speci...",
    "published": "Jun 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2406.11915v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Talking Heads: Understanding Inter-layer Communication in Transformer Language Models",
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "summary": "Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find t...",
    "published": "Jun 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2406.09519v4",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "authors": [
      "Tri Dao",
      "Albert Gu"
    ],
    "summary": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely rela...",
    "published": "May 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2405.21060v1",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "The Call for Socially Aware Language Technologies",
    "authors": [
      "Diyi Yang",
      "Dirk Hovy",
      "David Jurgens",
      "Barbara Plank"
    ],
    "summary": "Language technologies have made enormous progress, especially with the introduction of large language models (LLMs). On traditional tasks such as machine translation and sentiment analysis, these models perform at near-human level. These advances can, however, exacerbate a variety of issues that mod...",
    "published": "May 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2405.02411v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Causally Inspired Regularization Enables Domain General Representations",
    "authors": [
      "Olawale Salaudeen",
      "Sanmi Koyejo"
    ],
    "summary": "Given a causal graph representing the data-generating process shared across different domains/distributions, enforcing sufficient graph-implied conditional independencies can identify domain-general (non-spurious) feature representations. For the standard input-output predictive setting, we categori...",
    "published": "Apr 25",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2404.16277v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Disentangling Length from Quality in Direct Preference Optimization",
    "authors": [
      "Ryan Park",
      "Rafael Rafailov",
      "Stefano Ermon",
      "Chelsea Finn"
    ],
    "summary": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is le...",
    "published": "Mar 28",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2403.19159v2",
    "queried_author": "Rafael Rafailov",
    "matching_authors": [
      "Rafael Rafailov"
    ]
  },
  {
    "title": "Few-Shot Recalibration of Language Models",
    "authors": [
      "Xiang Lisa Li",
      "Urvashi Khandelwal",
      "Kelvin Guu"
    ],
    "summary": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscal...",
    "published": "Mar 27",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2403.18286v1",
    "queried_author": "Xiang Lisa Li",
    "matching_authors": [
      "Xiang Lisa Li"
    ]
  },
  {
    "title": "Language models scale reliably with over-training and on downstream tasks",
    "authors": [
      "Samir Yitzhak Gadre",
      "Georgios Smyrnis",
      "Vaishaal Shankar",
      "Suchin Gururangan",
      "Mitchell Wortsman",
      "Rulin Shao",
      "Jean Mercat",
      "Alex Fang",
      "Jeffrey Li",
      "Sedrick Keh",
      "Rui Xin",
      "Marianna Nezhurina",
      "Igor Vasiljevic",
      "Jenia Jitsev",
      "Luca Soldaini",
      "Alexandros G. Dimakis",
      "Gabriel Ilharco",
      "Pang Wei Koh",
      "Shuran Song",
      "Thomas Kollar",
      "Yair Carmon",
      "Achal Dave",
      "Reinhard Heckel",
      "Niklas Muennighoff",
      "Ludwig Schmidt"
    ],
    "summary": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is ...",
    "published": "Mar 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2403.08540v2",
    "queried_author": "Samir Yitzhak Gadre",
    "matching_authors": [
      "Samir Yitzhak Gadre"
    ]
  },
  {
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "authors": [
      "Ashwinee Panda",
      "Christopher A. Choquette-Choo",
      "Zhengming Zhang",
      "Yaoqing Yang",
      "Prateek Mittal"
    ],
    "summary": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call \"neural phishing\". This attack enables an adversary to target and extr...",
    "published": "Mar 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2403.00871v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "QuRating: Selecting High-Quality Data for Training Language Models",
    "authors": [
      "Alexander Wettig",
      "Aatmik Gupta",
      "Saumya Malik",
      "Danqi Chen"
    ],
    "summary": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualiti...",
    "published": "Feb 15",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2402.09739v3",
    "queried_author": "Alexander Wettig",
    "matching_authors": [
      "Alexander Wettig"
    ]
  },
  {
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "authors": [
      "Albert Gu",
      "Tri Dao"
    ],
    "summary": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state ...",
    "published": "Dec 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2312.00752v2",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu",
      "Tri Dao"
    ]
  },
  {
    "title": "GAIA: a benchmark for General AI Assistants",
    "authors": [
      "Gr\u00e9goire Mialon",
      "Cl\u00e9mentine Fourrier",
      "Craig Swift",
      "Thomas Wolf",
      "Yann LeCun",
      "Thomas Scialom"
    ],
    "summary": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA ques...",
    "published": "Nov 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2311.12983v1",
    "queried_author": "Cl\u00e9mentine Fourrier",
    "matching_authors": [
      "Cl\u00e9mentine Fourrier"
    ]
  },
  {
    "title": "Towards Evaluating AI Systems for Moral Status Using Self-Reports",
    "authors": [
      "Ethan Perez",
      "Robert Long"
    ],
    "summary": "As AI systems become more advanced and widely deployed, there will likely be increasing debate over whether AI systems could have conscious experiences, desires, or other states of potential moral significance. It is important to inform these discussions with empirical evidence to the extent possibl...",
    "published": "Nov 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2311.08576v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "A Material Lens on Coloniality in NLP",
    "authors": [
      "William Held",
      "Camille Harris",
      "Michael Best",
      "Diyi Yang"
    ],
    "summary": "Coloniality, the continuation of colonial harms beyond \"official\" colonization, has pervasive effects across society and scientific fields. Natural Language Processing (NLP) is no exception to this broad phenomenon. In this work, we argue that coloniality is implicitly embedded in and amplified by N...",
    "published": "Nov 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2311.08391v1",
    "queried_author": "William Held",
    "matching_authors": [
      "William Held"
    ]
  },
  {
    "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "summary": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a highe...",
    "published": "Oct 12",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2310.08744v3",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Artificial Intelligence and Aesthetic Judgment",
    "authors": [
      "Jessica Hullman",
      "Ari Holtzman",
      "Andrew Gelman"
    ],
    "summary": "Generative AIs produce creative outputs in the style of human expression. We argue that encounters with the outputs of modern generative AI models are mediated by the same kinds of aesthetic judgments that organize our interactions with artwork. The interpretation procedure we use on art we find in ...",
    "published": "Aug 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2309.12338v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?",
    "authors": [
      "Ari Holtzman",
      "Peter West",
      "Luke Zettlemoyer"
    ],
    "summary": "Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex system...",
    "published": "Jul 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2308.00189v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "authors": [
      "Tri Dao"
    ],
    "summary": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the ma...",
    "published": "Jul 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2307.08691v1",
    "queried_author": "Tri Dao",
    "matching_authors": [
      "Tri Dao"
    ]
  },
  {
    "title": "Reproducibility in NLP: What Have We Learned from the Checklist?",
    "authors": [
      "Ian Magnusson",
      "Noah A. Smith",
      "Jesse Dodge"
    ],
    "summary": "Scientific progress in NLP rests on the reproducibility of researchers' claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,...",
    "published": "Jun 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2306.09562v1",
    "queried_author": "Ian Magnusson",
    "matching_authors": [
      "Ian Magnusson"
    ]
  },
  {
    "title": "Backpack Language Models",
    "authors": [
      "John Hewitt",
      "John Thickstun",
      "Christopher D. Manning",
      "Percy Liang"
    ],
    "summary": "We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative li...",
    "published": "May 26",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.16765v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt"
    ]
  },
  {
    "title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "summary": "A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model...",
    "published": "May 25",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.16130v3",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
    "authors": [
      "Kevin Lin",
      "Kyle Lo",
      "Joseph E. Gonzalez",
      "Dan Klein"
    ],
    "summary": "When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs -- complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of...",
    "published": "May 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.15053v1",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo"
    ]
  },
  {
    "title": "Adapting Language Models to Compress Contexts",
    "authors": [
      "Alexis Chevalier",
      "Alexander Wettig",
      "Anirudh Ajith",
      "Danqi Chen"
    ],
    "summary": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are...",
    "published": "May 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.14788v2",
    "queried_author": "Alexander Wettig",
    "matching_authors": [
      "Alexander Wettig"
    ]
  },
  {
    "title": "Anchor Prediction: Automatic Refinement of Internet Links",
    "authors": [
      "Nelson F. Liu",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "summary": "Internet links enable users to deepen their understanding of a topic by providing convenient access to related information. However, the majority of links are unanchored -- they link to a target webpage as a whole, and readers may expend considerable effort localizing the specific parts of the targe...",
    "published": "May 23",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.14337v2",
    "queried_author": "Nelson F. Liu",
    "matching_authors": [
      "Nelson F. Liu"
    ]
  },
  {
    "title": "Privacy-Preserving In-Context Learning for Large Language Models",
    "authors": [
      "Tong Wu",
      "Ashwinee Panda",
      "Jiachen T. Wang",
      "Prateek Mittal"
    ],
    "summary": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-cont...",
    "published": "May 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2305.01639v2",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Evaluating Verifiability in Generative Search Engines",
    "authors": [
      "Nelson F. Liu",
      "Tianyi Zhang",
      "Percy Liang"
    ],
    "summary": "Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and ...",
    "published": "Apr 19",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2304.09848v2",
    "queried_author": "Nelson F. Liu",
    "matching_authors": [
      "Nelson F. Liu"
    ]
  },
  {
    "title": "CB2: Collaborative Natural Language Interaction Research Platform",
    "authors": [
      "Jacob Sharf",
      "Mustafa Omer Gul",
      "Yoav Artzi"
    ],
    "summary": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at h...",
    "published": "Mar 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2303.08127v3",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
    "authors": [
      "Bilal Chughtai",
      "Lawrence Chan",
      "Neel Nanda"
    ],
    "summary": "Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a no...",
    "published": "Feb 06",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2302.03025v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Progress measures for grokking via mechanistic interpretability",
    "authors": [
      "Neel Nanda",
      "Lawrence Chan",
      "Tom Lieberum",
      "Jess Smith",
      "Jacob Steinhardt"
    ],
    "summary": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous q...",
    "published": "Jan 12",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2301.05217v3",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Language Models as Agent Models",
    "authors": [
      "Jacob Andreas"
    ],
    "summary": "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact...",
    "published": "Dec 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2212.01681v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors",
    "authors": [
      "Hamish Ivison",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Pradeep Dasigi"
    ],
    "summary": "Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a smal...",
    "published": "Dec 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2212.00196v2",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison"
    ]
  },
  {
    "title": "MTEB: Massive Text Embedding Benchmark",
    "authors": [
      "Niklas Muennighoff",
      "Nouamane Tazi",
      "Lo\u00efc Magne",
      "Nils Reimers"
    ],
    "summary": "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking...",
    "published": "Oct 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2210.07316v3",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "Are Sample-Efficient NLP Models More Robust?",
    "authors": [
      "Nelson F. Liu",
      "Ananya Kumar",
      "Percy Liang",
      "Robin Jia"
    ],
    "summary": "Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-of-distribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks...",
    "published": "Oct 12",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2210.06456v2",
    "queried_author": "Nelson F. Liu",
    "matching_authors": [
      "Nelson F. Liu"
    ]
  },
  {
    "title": "Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers",
    "authors": [
      "William Held",
      "Diyi Yang"
    ],
    "summary": "Multilingual transformer-based models demonstrate remarkable zero and few-shot transfer across languages by learning and reusing language-agnostic features. However, as a fixed-size model acquires more languages, its performance across all languages degrades, a phenomenon termed interference. Often ...",
    "published": "Oct 11",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2210.05709v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "William Held"
    ]
  },
  {
    "title": "A Kernel-Based View of Language Model Fine-Tuning",
    "authors": [
      "Sadhika Malladi",
      "Alexander Wettig",
      "Dingli Yu",
      "Danqi Chen",
      "Sanjeev Arora"
    ],
    "summary": "It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result...",
    "published": "Oct 11",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2210.05643v4",
    "queried_author": "Alexander Wettig",
    "matching_authors": [
      "Alexander Wettig"
    ]
  },
  {
    "title": "Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization",
    "authors": [
      "Seungone Kim",
      "Se June Joo",
      "Hyungjoo Chae",
      "Chaehyeong Kim",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ],
    "summary": "In this paper, we propose to leverage the unique characteristics of dialogues sharing commonsense knowledge across participants, to resolve the difficulties in summarizing them. We present SICK, a framework that uses commonsense inferences as additional context. Compared to previous work that solely...",
    "published": "Sep 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2209.00930v1",
    "queried_author": "Seungone Kim",
    "matching_authors": [
      "Seungone Kim"
    ]
  },
  {
    "title": "Can Language Models perform Abductive Commonsense Reasoning?",
    "authors": [
      "Seungone Kim"
    ],
    "summary": "Abductive Reasoning is a task of inferring the most plausible hypothesis given a set of observations. In literature, the community has approached to solve this challenge by classifying/generating a likely hypothesis that does not contradict with a past observation and future observation. Some of the...",
    "published": "Jul 07",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2207.05155v1",
    "queried_author": "Seungone Kim",
    "matching_authors": [
      "Seungone Kim"
    ]
  },
  {
    "title": "The Authenticity Gap in Human Evaluation",
    "authors": [
      "Kawin Ethayarajh",
      "Dan Jurafsky"
    ],
    "summary": "Human ratings are the gold standard in NLG evaluation. The standard protocol is to collect ratings of generated text, average across annotators, and rank NLG systems by their average scores. However, little consideration has been given as to whether this approach faithfully captures human preference...",
    "published": "May 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2205.11930v2",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning",
    "authors": [
      "Orion Weller",
      "Kevin Seppi",
      "Matt Gardner"
    ],
    "summary": "Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training ...",
    "published": "May 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2205.08124v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "SUBS: Subtree Substitution for Compositional Semantic Parsing",
    "authors": [
      "Jingfeng Yang",
      "Le Zhang",
      "Diyi Yang"
    ],
    "summary": "Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial...",
    "published": "May 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2205.01538v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models",
    "authors": [
      "Terra Blevins",
      "Luke Zettlemoyer"
    ],
    "summary": "English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate ...",
    "published": "Apr 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2204.08110v4",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language",
    "authors": [
      "Jacob Eisenstein"
    ],
    "summary": "Spurious correlations are a threat to the trustworthiness of natural language processing systems, motivating research into methods for identifying and eliminating them. However, addressing the problem of spurious correlations requires more clarity on what they are and how they arise in language data...",
    "published": "Apr 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2204.04487v2",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Entity-Centric Query Refinement",
    "authors": [
      "David Wadden",
      "Nikita Gupta",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "summary": "We introduce the task of entity-centric query refinement. Given an input query whose answer is a (potentially large) collection of entities, the task output is a small set of query refinements meant to assist the user in efficient domain exploration and entity discovery. We propose a method to creat...",
    "published": "Apr 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2204.00743v2",
    "queried_author": "David Wadden",
    "matching_authors": [
      "David Wadden"
    ]
  },
  {
    "title": "Linking Emergent and Natural Languages via Corpus Transfer",
    "authors": [
      "Shunyu Yao",
      "Mo Yu",
      "Yang Zhang",
      "Karthik R Narasimhan",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ],
    "summary": "The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game fra...",
    "published": "Mar 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2203.13344v1",
    "queried_author": "Karthik R Narasimhan",
    "matching_authors": [
      "Karthik R Narasimhan"
    ]
  },
  {
    "title": "CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation",
    "authors": [
      "Samir Yitzhak Gadre",
      "Mitchell Wortsman",
      "Gabriel Ilharco",
      "Ludwig Schmidt",
      "Shuran Song"
    ],
    "summary": "For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language-driven zero-sho...",
    "published": "Mar 20",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2203.10421v2",
    "queried_author": "Samir Yitzhak Gadre",
    "matching_authors": [
      "Samir Yitzhak Gadre"
    ]
  },
  {
    "title": "Hyperdecoders: Instance-specific decoders for multi-task NLP",
    "authors": [
      "Hamish Ivison",
      "Matthew E. Peters"
    ],
    "summary": "We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder adaptation for every input instance, allowing the network a larger d...",
    "published": "Mar 15",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2203.08304v3",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison"
    ]
  },
  {
    "title": "SGPT: GPT Sentence Embeddings for Semantic Search",
    "authors": [
      "Niklas Muennighoff"
    ],
    "summary": "Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of ...",
    "published": "Feb 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2202.08904v5",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow",
    "authors": [
      "Maarten Sap",
      "Anna Jafarpour",
      "Yejin Choi",
      "Noah A. Smith",
      "James W. Pennebaker",
      "Eric Horvitz"
    ],
    "summary": "Lifelong experiences and learned knowledge lead to shared expectations about how common situations tend to unfold. Such knowledge of narrative event flow enables people to weave together a story. However, comparable computational tools to evaluate the flow of events in narratives are limited. We qua...",
    "published": "Jan 07",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2201.02662v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Training Neural Networks with Fixed Sparse Masks",
    "authors": [
      "Yi-Lin Sung",
      "Varun Nair",
      "Colin Raffel"
    ],
    "summary": "During typical gradient-based training of deep neural networks, all of the model's parameters are updated at each iteration. Recent work has shown that it is possible to update only a small subset of the model's parameters during training, which can alleviate storage and communication requirements. ...",
    "published": "Nov 18",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2111.09839v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Merging Models with Fisher-Weighted Averaging",
    "authors": [
      "Michael Matena",
      "Colin Raffel"
    ],
    "summary": "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this \"merging\" operation can be seen as choosing parameters that approximately maximize the joint likeliho...",
    "published": "Nov 18",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2111.09832v2",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "authors": [
      "Albert Gu",
      "Karan Goel",
      "Christopher R\u00e9"
    ],
    "summary": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long ...",
    "published": "Oct 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2111.00396v3",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference",
    "authors": [
      "William Held",
      "Dan Iter",
      "Dan Jurafsky"
    ],
    "summary": "Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full $n^2$ pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle inte...",
    "published": "Oct 11",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2110.05362v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky",
      "William Held"
    ]
  },
  {
    "title": "An Empirical Investigation of Learning from Biased Toxicity Labels",
    "authors": [
      "Neel Nanda",
      "Jonathan Uesato",
      "Sven Gowal"
    ],
    "summary": "Collecting annotations from human raters often results in a trade-off between the quantity of labels one wishes to gather and the quality of these labels. As such, it is often only possible to gather a small amount of high-quality labels. In this paper, we study how different training strategies can...",
    "published": "Oct 04",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2110.01577v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Expected Validation Performance and Estimation of a Random Variable's Maximum",
    "authors": [
      "Jesse Dodge",
      "Suchin Gururangan",
      "Dallas Card",
      "Roy Schwartz",
      "Noah A. Smith"
    ],
    "summary": "Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., a...",
    "published": "Oct 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2110.00613v1",
    "queried_author": "Jesse Dodge",
    "matching_authors": [
      "Jesse Dodge"
    ]
  },
  {
    "title": "Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results",
    "authors": [
      "Ian H. Magnusson",
      "Scott E. Friedman"
    ],
    "summary": "Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim,...",
    "published": "Sep 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2109.10453v1",
    "queried_author": "Ian Magnusson",
    "matching_authors": [
      "Ian Magnusson"
    ]
  },
  {
    "title": "Conditional probing: measuring usable information beyond a baseline",
    "authors": [
      "John Hewitt",
      "Kawin Ethayarajh",
      "Percy Liang",
      "Christopher D. Manning"
    ],
    "summary": "Probing experiments investigate the extent to which neural representations make properties -- like part-of-speech -- predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual w...",
    "published": "Sep 19",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2109.09234v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt"
    ]
  },
  {
    "title": "Invertible Frowns: Video-to-Video Facial Emotion Translation",
    "authors": [
      "Ian Magnusson",
      "Aruna Sankaranarayanan",
      "Andrew Lippman"
    ],
    "summary": "We present Wav2Lip-Emotion, a video-to-video translation architecture that modifies facial expressions of emotion in videos of speakers. Previous work modifies emotion in images, uses a single image to produce a video with animated emotion, or puppets facial expressions in videos with landmarks from...",
    "published": "Sep 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2109.08061v2",
    "queried_author": "Ian Magnusson",
    "matching_authors": [
      "Ian Magnusson"
    ]
  },
  {
    "title": "Overview and Insights from the SciVer Shared Task on Scientific Claim Verification",
    "authors": [
      "David Wadden",
      "Kyle Lo"
    ],
    "summary": "We present an overview of the SciVer shared task, presented at the 2nd Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared task, systems were provided a scientific claim and a corpus of research abstracts, and asked to identify which articles SUPPORT or REFUTE the claim as wel...",
    "published": "Jul 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2107.08188v1",
    "queried_author": "David Wadden",
    "matching_authors": [
      "David Wadden"
    ]
  },
  {
    "title": "Visual Adversarial Imitation Learning using Variational Models",
    "authors": [
      "Rafael Rafailov",
      "Tianhe Yu",
      "Aravind Rajeswaran",
      "Chelsea Finn"
    ],
    "summary": "Reward function specification, which requires considerable human effort and iteration, remains a major impediment for learning behaviors through deep reinforcement learning. In contrast, providing visual demonstrations of desired behaviors often presents an easier and more natural way to teach agent...",
    "published": "Jul 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2107.08829v2",
    "queried_author": "Rafael Rafailov",
    "matching_authors": [
      "Rafael Rafailov"
    ]
  },
  {
    "title": "Diagnosing the Impact of AI on Radiology in China",
    "authors": [
      "Niklas Muennighoff"
    ],
    "summary": "Artificial Intelligence will significantly impact the work environment of radiologists. I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025. However, it won't increase beyond that 50% level, as radiologists remain key for human-centered aspects of their job...",
    "published": "Jun 15",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2106.07921v1",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "True Few-Shot Learning with Language Models",
    "authors": [
      "Ethan Perez",
      "Douwe Kiela",
      "Kyunghyun Cho"
    ],
    "summary": "Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (\"prompts\"). Here, we evaluate the few-shot...",
    "published": "May 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2105.11447v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery",
    "authors": [
      "Samir Yitzhak Gadre",
      "Kiana Ehsani",
      "Shuran Song"
    ],
    "summary": "People often use physical intuition when manipulating articulated objects, irrespective of object semantics. Motivated by this observation, we identify an important embodied task where an agent must play with objects to recover their parts. To this end, we introduce Act the Part (AtP) to learn how t...",
    "published": "May 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2105.01047v1",
    "queried_author": "Samir Yitzhak Gadre",
    "matching_authors": [
      "Samir Yitzhak Gadre"
    ]
  },
  {
    "title": "Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification",
    "authors": [
      "Wilson Fearn",
      "Orion Weller",
      "Kevin Seppi"
    ],
    "summary": "Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of th...",
    "published": "Apr 08",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2104.03848v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
    "authors": [
      "Mike Lewis",
      "Shruti Bhosale",
      "Tim Dettmers",
      "Naman Goyal",
      "Luke Zettlemoyer"
    ],
    "summary": "We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only...",
    "published": "Mar 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2103.16716v1",
    "queried_author": "Mike Lewis",
    "matching_authors": [
      "Mike Lewis"
    ]
  },
  {
    "title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length",
    "authors": [
      "Ethan Perez",
      "Douwe Kiela",
      "Kyunghyun Cho"
    ],
    "summary": "We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program...",
    "published": "Mar 05",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2103.03872v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Decontextualization: Making Sentences Stand-Alone",
    "authors": [
      "Eunsol Choi",
      "Jennimaria Palomaki",
      "Matthew Lamm",
      "Tom Kwiatkowski",
      "Dipanjan Das",
      "Michael Collins"
    ],
    "summary": "Models for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of s...",
    "published": "Feb 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2102.05169v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Modeling Context in Answer Sentence Selection Systems on a Latency Budget",
    "authors": [
      "Rujun Han",
      "Luca Soldaini",
      "Alessandro Moschitti"
    ],
    "summary": "Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted fro...",
    "published": "Jan 28",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2101.12093v2",
    "queried_author": "Luca Soldaini",
    "matching_authors": [
      "Luca Soldaini"
    ]
  },
  {
    "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "authors": [
      "Xiang Lisa Li",
      "Percy Liang"
    ],
    "summary": "Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tu...",
    "published": "Jan 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2101.00190v1",
    "queried_author": "Xiang Lisa Li",
    "matching_authors": [
      "Xiang Lisa Li"
    ]
  },
  {
    "title": "Offline Reinforcement Learning from Images with Latent Space Models",
    "authors": [
      "Rafael Rafailov",
      "Tianhe Yu",
      "Aravind Rajeswaran",
      "Chelsea Finn"
    ],
    "summary": "Offline reinforcement learning (RL) refers to the problem of learning policies from a static dataset of environment interactions. Offline RL enables extensive use and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-...",
    "published": "Dec 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2012.11547v1",
    "queried_author": "Rafael Rafailov",
    "matching_authors": [
      "Rafael Rafailov"
    ]
  },
  {
    "title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction",
    "authors": [
      "Xinyao Ma",
      "Maarten Sap",
      "Hannah Rashkin",
      "Yejin Choi"
    ],
    "summary": "Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (\"She daydreams about being a doctor\") while a man is portrayed as more pr...",
    "published": "Oct 26",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2010.13816v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "A Frustratingly Easy Approach for Entity and Relation Extraction",
    "authors": [
      "Zexuan Zhong",
      "Danqi Chen"
    ],
    "summary": "End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we pre...",
    "published": "Oct 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2010.12812v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Evaluating Factuality in Generation with Dependency-level Entailment",
    "authors": [
      "Tanya Goyal",
      "Greg Durrett"
    ],
    "summary": "Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to identify factual errors; however, these sentence-leve...",
    "published": "Oct 12",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2010.05478v2",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "MLE-guided parameter search for task loss minimization in neural sequence modeling",
    "authors": [
      "Sean Welleck",
      "Kyunghyun Cho"
    ],
    "summary": "Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empi...",
    "published": "Jun 04",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2006.03158v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models",
    "authors": [
      "Dan Iter",
      "Kelvin Guu",
      "Larry Lansing",
      "Dan Jurafsky"
    ],
    "summary": "Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose CONPONO, an inter-sentence objective for pretraining language models that models dis...",
    "published": "May 20",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2005.10389v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "The Cascade Transformer: an Application for Efficient Answer Sentence Selection",
    "authors": [
      "Luca Soldaini",
      "Alessandro Moschitti"
    ],
    "summary": "Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce...",
    "published": "May 05",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2005.02534v2",
    "queried_author": "Luca Soldaini",
    "matching_authors": [
      "Luca Soldaini"
    ]
  },
  {
    "title": "Neural Syntactic Preordering for Controlled Paraphrase Generation",
    "authors": [
      "Tanya Goyal",
      "Greg Durrett"
    ],
    "summary": "Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities ...",
    "published": "May 05",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2005.02013v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "ExpBERT: Representation Engineering with Natural Language Explanations",
    "authors": [
      "Shikhar Murty",
      "Pang Wei Koh",
      "Percy Liang"
    ],
    "summary": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNL...",
    "published": "May 05",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2005.01932v1",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Mapping Three Decades of Intellectual Change in Academia",
    "authors": [
      "Daniel Ramage",
      "Christopher D. Manning",
      "Daniel A. McFarland"
    ],
    "summary": "Research on the development of science has focused on the creation of multidisciplinary teams. However, while this coming together of people is symmetrical, the ideas, methods, and vocabulary of science have a directional flow. We present a statistical model of the text of dissertation abstracts fro...",
    "published": "Apr 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2004.01291v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "Calibration of Pre-trained Transformers",
    "authors": [
      "Shrey Desai",
      "Greg Durrett"
    ],
    "summary": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model...",
    "published": "Mar 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2003.07892v3",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge: Results after 1 Year Follow-up",
    "authors": [
      "Razvan V. Marinescu",
      "Neil P. Oxtoby",
      "Alexandra L. Young",
      "Esther E. Bron",
      "Arthur W. Toga",
      "Michael W. Weiner",
      "Frederik Barkhof",
      "Nick C. Fox",
      "Arman Eshaghi",
      "Tina Toni",
      "Marcin Salaterski",
      "Veronika Lunina",
      "Manon Ansart",
      "Stanley Durrleman",
      "Pascal Lu",
      "Samuel Iddi",
      "Dan Li",
      "Wesley K. Thompson",
      "Michael C. Donohue",
      "Aviv Nahon",
      "Yarden Levy",
      "Dan Halbersberg",
      "Mariya Cohen",
      "Huiling Liao",
      "Tengfei Li",
      "Kaixian Yu",
      "Hongtu Zhu",
      "Jose G. Tamez-Pena",
      "Aya Ismail",
      "Timothy Wood",
      "Hector Corrada Bravo",
      "Minh Nguyen",
      "Nanbo Sun",
      "Jiashi Feng",
      "B. T. Thomas Yeo",
      "Gang Chen",
      "Ke Qi",
      "Shiyang Chen",
      "Deqiang Qiu",
      "Ionut Buciuman",
      "Alex Kelner",
      "Raluca Pop",
      "Denisa Rimocea",
      "Mostafa M. Ghazi",
      "Mads Nielsen",
      "Sebastien Ourselin",
      "Lauge Sorensen",
      "Vikram Venkatraghavan",
      "Keli Liu",
      "Christina Rabe",
      "Paul Manser",
      "Steven M. Hill",
      "James Howlett",
      "Zhiyue Huang",
      "Steven Kiddle",
      "Sach Mukherjee",
      "Anais Rouanet",
      "Bernd Taschler",
      "Brian D. M. Tom",
      "Simon R. White",
      "Noel Faux",
      "Suman Sedai",
      "Javier de Velasco Oriol",
      "Edgar E. V. Clemente",
      "Karol Estrada",
      "Leon Aksman",
      "Andre Altmann",
      "Cynthia M. Stonnington",
      "Yalin Wang",
      "Jianfeng Wu",
      "Vivek Devadas",
      "Clementine Fourrier",
      "Lars Lau Raket",
      "Aristeidis Sotiras",
      "Guray Erus",
      "Jimit Doshi",
      "Christos Davatzikos",
      "Jacob Vogel",
      "Andrew Doyle",
      "Angela Tam",
      "Alex Diaz-Papkovich",
      "Emmanuel Jammeh",
      "Igor Koval",
      "Paul Moore",
      "Terry J. Lyons",
      "John Gallacher",
      "Jussi Tohka",
      "Robert Ciszek",
      "Bruno Jedynak",
      "Kruti Pandya",
      "Murat Bilgel",
      "William Engels",
      "Joseph Cole",
      "Polina Golland",
      "Stefan Klein",
      "Daniel C. Alexander"
    ],
    "summary": "We present the findings of \"The Alzheimer's Disease Prediction Of Longitudinal Evolution\" (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants wer...",
    "published": "Feb 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2002.03419v2",
    "queried_author": "Cl\u00e9mentine Fourrier",
    "matching_authors": [
      "Cl\u00e9mentine Fourrier"
    ]
  },
  {
    "title": "Torch-Struct: Deep Structured Prediction Library",
    "authors": [
      "Alexander M. Rush"
    ],
    "summary": "The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured predic...",
    "published": "Feb 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/2002.00876v1",
    "queried_author": "Alexander M Rush",
    "matching_authors": [
      "Alexander M Rush",
      "Alexander M. Rush"
    ]
  },
  {
    "title": "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
    "authors": [
      "Antoine Bosselut",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "summary": "Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it.\n  In this paper, we present initial stud...",
    "published": "Nov 10",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1911.03876v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "MUSE Analysis of Gas around Galaxies (MAGG) -- I: Survey design and the environment of a near pristine gas cloud at z~3.5",
    "authors": [
      "Emma K. Lofthouse",
      "Michele Fumagalli",
      "Matteo Fossati",
      "John M. O'Meara",
      "Michael T. Murphy",
      "Lise Christensen",
      "J. Xavier Prochaska",
      "Sebastiano Cantalupo",
      "Richard M. Bielby",
      "Ryan J. Cooke",
      "Elisabeta Lusso",
      "Simon L. Morris"
    ],
    "summary": "We present the design, methods, and first results of the MUSE Analysis of Gas around Galaxies (MAGG) survey, a large programme on the Multi Unit Spectroscopic Explorer (MUSE) instrument at the Very Large Telescope (VLT) which targets 28 z > 3.2 quasars to investigate the connection between optically...",
    "published": "Oct 29",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1910.13458v1",
    "queried_author": "John Xavier Morris",
    "matching_authors": [
      "John Xavier Morris"
    ]
  },
  {
    "title": "Specializing Word Embeddings (for Parsing) by Information Bottleneck",
    "authors": [
      "Xiang Lisa Li",
      "Jason Eisner"
    ],
    "summary": "Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information th...",
    "published": "Oct 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1910.00163v1",
    "queried_author": "Xiang Lisa Li",
    "matching_authors": [
      "Xiang Lisa Li"
    ]
  },
  {
    "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "authors": [
      "David Wadden",
      "Ulme Wennberg",
      "Yi Luan",
      "Hannaneh Hajishirzi"
    ],
    "summary": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local...",
    "published": "Sep 08",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1909.03546v2",
    "queried_author": "David Wadden",
    "matching_authors": [
      "David Wadden"
    ]
  },
  {
    "title": "Designing and Interpreting Probes with Control Tasks",
    "authors": [
      "John Hewitt",
      "Percy Liang"
    ],
    "summary": "Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task?...",
    "published": "Sep 08",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1909.03368v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt"
    ]
  },
  {
    "title": "RNN Architecture Learning with Sparse Regularization",
    "authors": [
      "Jesse Dodge",
      "Roy Schwartz",
      "Hao Peng",
      "Noah A. Smith"
    ],
    "summary": "Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rationa...",
    "published": "Sep 06",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1909.03011v1",
    "queried_author": "Jesse Dodge",
    "matching_authors": [
      "Jesse Dodge"
    ]
  },
  {
    "title": "Humor Detection: A Transformer Gets the Last Laugh",
    "authors": [
      "Orion Weller",
      "Kevin Seppi"
    ],
    "summary": "Much previous work has been done in attempting to identify humor in text. In this paper we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of approaching this problem by building a model that learns to identify humorous jokes based ...",
    "published": "Aug 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1909.00252v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "Embedding time expressions for deep temporal ordering models",
    "authors": [
      "Tanya Goyal",
      "Greg Durrett"
    ],
    "summary": "Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these models often overlook explicit temporal signals, such as dates and time windows. Rule-based methods can be used to identify the temporal links between these time exp...",
    "published": "Jun 19",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1906.08287v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "Better Character Language Modeling Through Morphology",
    "authors": [
      "Terra Blevins",
      "Luke Zettlemoyer"
    ],
    "summary": "We incorporate morphological supervision into character language models (CLMs) via multitasking and show that this addition improves bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data are disjoint. Analyzing the CLMs shows that inflecte...",
    "published": "Jun 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1906.01037v2",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Efficient Adaptation of Pretrained Transformers for Abstractive Summarization",
    "authors": [
      "Andrew Hoang",
      "Antoine Bosselut",
      "Asli Celikyilmaz",
      "Yejin Choi"
    ],
    "summary": "Large-scale learning of transformer language models has yielded improvements on a variety of natural language understanding tasks. Whether they can be effectively adapted for summarization, however, has been less explored, as the learned representations are less seamlessly integrated into existing n...",
    "published": "Jun 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1906.00138v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Sequential Graph Dependency Parser",
    "authors": [
      "Sean Welleck",
      "Kyunghyun Cho"
    ],
    "summary": "We propose a method for non-projective dependency parsing by incrementally predicting a set of edges. Since the edges do not have a pre-specified order, we propose a set-based learning method. Our method blends graph, transition, and easy-first parsing, including a prior state of the parser as a spe...",
    "published": "May 27",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1905.10930v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "SocialIQA: Commonsense Reasoning about Social Interactions",
    "authors": [
      "Maarten Sap",
      "Hannah Rashkin",
      "Derek Chen",
      "Ronan LeBras",
      "Yejin Choi"
    ],
    "summary": "We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan...",
    "published": "Apr 22",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1904.09728v3",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Modeling Drug-Disease Relations with Linguistic and Knowledge Graph Constraints",
    "authors": [
      "Bruno Godefroy",
      "Christopher Potts"
    ],
    "summary": "FDA drug labels are rich sources of information about drugs and drug-disease relations, but their complexity makes them challenging texts to analyze in isolation. To overcome this, we situate these labels in two health knowledge graphs: one built from precise structured information about drugs and d...",
    "published": "Mar 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1904.00313v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Measuring Compositionality in Representation Learning",
    "authors": [
      "Jacob Andreas"
    ],
    "summary": "Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learne...",
    "published": "Feb 19",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1902.07181v2",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Contextual Word Representations: A Contextual Introduction",
    "authors": [
      "Noah A. Smith"
    ],
    "summary": "This introduction aims to tell the story of how we put words into computers. It is part of the story of the field of natural language processing (NLP), a branch of artificial intelligence. It targets a wide audience with a basic understanding of computer programming, but avoids a detailed mathematic...",
    "published": "Feb 15",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1902.06006v3",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Do ImageNet Classifiers Generalize to ImageNet?",
    "authors": [
      "Benjamin Recht",
      "Rebecca Roelofs",
      "Ludwig Schmidt",
      "Vaishaal Shankar"
    ],
    "summary": "We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current...",
    "published": "Feb 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1902.10811v2",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses",
    "authors": [
      "Pang Wei Koh",
      "Jacob Steinhardt",
      "Percy Liang"
    ],
    "summary": "Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models' training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. In...",
    "published": "Nov 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1811.00741v2",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Dialogue Natural Language Inference",
    "authors": [
      "Sean Welleck",
      "Jason Weston",
      "Arthur Szlam",
      "Kyunghyun Cho"
    ],
    "summary": "Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialog...",
    "published": "Nov 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1811.00671v2",
    "queried_author": "Sean J Welleck",
    "matching_authors": [
      "Sean J Welleck"
    ]
  },
  {
    "title": "Stress-Testing Neural Models of Natural Language Inference with Multiply-Quantified Sentences",
    "authors": [
      "Atticus Geiger",
      "Ignacio Cases",
      "Lauri Karttunen",
      "Christopher Potts"
    ],
    "summary": "Standard evaluations of deep learning models for semantics using naturalistic corpora are limited in what they can tell us about the fidelity of the learned representations, because the corpora rarely come with good measures of semantic complexity. To overcome this limitation, we present a method fo...",
    "published": "Oct 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1810.13033v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Combining Distant and Direct Supervision for Neural Relation Extraction",
    "authors": [
      "Iz Beltagy",
      "Kyle Lo",
      "Waleed Ammar"
    ],
    "summary": "In relation extraction with distant supervision, noisy labels make it difficult to train quality models. Previous neural models addressed this problem using an attention mechanism that attends to sentences that are likely to express the relations. We improve such models by combining the distant supe...",
    "published": "Oct 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1810.12956v2",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo"
    ]
  },
  {
    "title": "A case for deep learning in semantics",
    "authors": [
      "Christopher Potts"
    ],
    "summary": "Pater's target article builds a persuasive case for establishing stronger ties between theoretical linguistics and connectionism (deep learning). This commentary extends his arguments to semantics, focusing in particular on issues of learning, compositionality, and lexical meaning.",
    "published": "Sep 10",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1809.03068v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Neural Compositional Denotational Semantics for Question Answering",
    "authors": [
      "Nitish Gupta",
      "Mike Lewis"
    ],
    "summary": "Answering compositional questions requiring multi-step reasoning is challenging. We introduce an end-to-end differentiable model for interpreting questions about a knowledge graph (KG), which is inspired by formal approaches to semantics. Each span of text is represented by a denotation in a KG and ...",
    "published": "Aug 29",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1808.09942v1",
    "queried_author": "Mike Lewis",
    "matching_authors": [
      "Mike Lewis"
    ]
  },
  {
    "title": "Ultra-Fine Entity Typing",
    "authors": [
      "Eunsol Choi",
      "Omer Levy",
      "Yejin Choi",
      "Luke Zettlemoyer"
    ],
    "summary": "We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at l...",
    "published": "Jul 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1807.04905v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Helping or Hurting? Predicting Changes in Users' Risk of Self-Harm Through Online Community Interactions",
    "authors": [
      "Luca Soldaini",
      "Timothy Walsh",
      "Arman Cohan",
      "Julien Han",
      "Nazli Goharian"
    ],
    "summary": "In recent years, online communities have formed around suicide and self-harm prevention. While these communities offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate suicidal ideation. In this work, we focus on how interaction ...",
    "published": "Apr 19",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1804.07253v1",
    "queried_author": "Luca Soldaini",
    "matching_authors": [
      "Luca Soldaini"
    ]
  },
  {
    "title": "Deep Communicating Agents for Abstractive Summarization",
    "authors": [
      "Asli Celikyilmaz",
      "Antoine Bosselut",
      "Xiaodong He",
      "Yejin Choi"
    ],
    "summary": "We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subse...",
    "published": "Mar 27",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1803.10357v3",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
    "authors": [
      "Shaojie Bai",
      "J. Zico Kolter",
      "Vladlen Koltun"
    ],
    "summary": "For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which...",
    "published": "Mar 04",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1803.01271v2",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Saliency-based Sequential Image Attention with Multiset Prediction",
    "authors": [
      "Sean Welleck",
      "Jialin Mao",
      "Kyunghyun Cho",
      "Zheng Zhang"
    ],
    "summary": "Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and ...",
    "published": "Nov 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1711.05165v1",
    "queried_author": "Sean J Welleck",
    "matching_authors": [
      "Sean J Welleck"
    ]
  },
  {
    "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
    "authors": [
      "Eric Wong",
      "J. Zico Kolter"
    ],
    "summary": "We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as we...",
    "published": "Nov 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1711.00851v3",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Gaussian Quadrature for Kernel Features",
    "authors": [
      "Tri Dao",
      "Christopher De Sa",
      "Christopher R\u00e9"
    ],
    "summary": "Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(\u03b5...",
    "published": "Sep 08",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1709.02605v3",
    "queried_author": "Tri Dao",
    "matching_authors": [
      "Tri Dao"
    ]
  },
  {
    "title": "Zero-Shot Activity Recognition with Verb Attribute Induction",
    "authors": [
      "Rowan Zellers",
      "Yejin Choi"
    ],
    "summary": "In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb \"salute\" has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the intern...",
    "published": "Jul 29",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1707.09468v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "End-to-end Neural Coreference Resolution",
    "authors": [
      "Kenton Lee",
      "Luheng He",
      "Mike Lewis",
      "Luke Zettlemoyer"
    ],
    "summary": "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions...",
    "published": "Jul 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1707.07045v2",
    "queried_author": "Mike Lewis",
    "matching_authors": [
      "Mike Lewis"
    ]
  },
  {
    "title": "Open Loop Hyperparameter Optimization and Determinantal Point Processes",
    "authors": [
      "Jesse Dodge",
      "Kevin Jamieson",
      "Noah A. Smith"
    ],
    "summary": "Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \\emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated. Examples include grid search, uniform random search, low discrepancy sequen...",
    "published": "Jun 06",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1706.01566v4",
    "queried_author": "Jesse Dodge",
    "matching_authors": [
      "Jesse Dodge"
    ]
  },
  {
    "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning",
    "authors": [
      "Dipendra Misra",
      "John Langford",
      "Yoav Artzi"
    ],
    "summary": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and vis...",
    "published": "Apr 28",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1704.08795v2",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Reading Wikipedia to Answer Open-Domain Questions",
    "authors": [
      "Danqi Chen",
      "Adam Fisch",
      "Jason Weston",
      "Antoine Bordes"
    ],
    "summary": "This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) ...",
    "published": "Mar 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1704.00051v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Understanding Black-box Predictions via Influence Functions",
    "authors": [
      "Pang Wei Koh",
      "Percy Liang"
    ],
    "summary": "How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a g...",
    "published": "Mar 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1703.04730v3",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial",
    "authors": [
      "Graham Neubig"
    ],
    "summary": "This tutorial introduces a new and powerful set of techniques variously called \"neural machine translation\" or \"neural sequence-to-sequence models\". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who ...",
    "published": "Mar 05",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1703.01619v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Training a Subsampling Mechanism in Expectation",
    "authors": [
      "Colin Raffel",
      "Dieterich Lawson"
    ],
    "summary": "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation. We test this approach on a simple toy problem and discuss its shortcomings.",
    "published": "Feb 22",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1702.06914v3",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Unsupervised Learning for Lexicon-Based Classification",
    "authors": [
      "Jacob Eisenstein"
    ],
    "summary": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification...",
    "published": "Nov 21",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1611.06933v1",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Hierarchical Question Answering for Long Documents",
    "authors": [
      "Eunsol Choi",
      "Daniel Hewlett",
      "Alexandre Lacoste",
      "Illia Polosukhin",
      "Jakob Uszkoreit",
      "Jonathan Berant"
    ],
    "summary": "We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documen...",
    "published": "Nov 06",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1611.01839v2",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016",
    "authors": [
      "Graham Neubig"
    ],
    "summary": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we ...",
    "published": "Oct 20",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1610.06542v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Input Convex Neural Networks",
    "authors": [
      "Brandon Amos",
      "Lei Xu",
      "J. Zico Kolter"
    ],
    "summary": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via opt...",
    "published": "Sep 22",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1609.07152v3",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "MUSE searches for galaxies near very metal-poor gas clouds at z~3: new constraints for cold accretion models",
    "authors": [
      "Michele Fumagalli",
      "Sebastiano Cantalupo",
      "Avishai Dekel",
      "Simon L. Morris",
      "John M. O'Meara",
      "J. Xavier Prochaska",
      "Tom Theuns"
    ],
    "summary": "We report on the search for galaxies in the proximity of two very metal-poor gas clouds at z~3 towards the quasar Q0956+122. With a 5-hour MUSE integration in a ~500x500 kpc^2 region centred at the quasar position, we achieve a >80% complete spectroscopic survey of continuum-detected galaxies with m...",
    "published": "Jul 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1607.03893v2",
    "queried_author": "John Xavier Morris",
    "matching_authors": [
      "John Xavier Morris"
    ]
  },
  {
    "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
    "authors": [
      "Hendrik Strobelt",
      "Sebastian Gehrmann",
      "Hanspeter Pfister",
      "Alexander M. Rush"
    ],
    "summary": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the change...",
    "published": "Jun 23",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1606.07461v2",
    "queried_author": "Alexander M Rush",
    "matching_authors": [
      "Alexander M Rush",
      "Alexander M. Rush"
    ]
  },
  {
    "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization",
    "authors": [
      "Akanksha",
      "Jacob Eisenstein"
    ],
    "summary": "This paper describes the Georgia Tech team's approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in ...",
    "published": "Jun 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1606.04503v1",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
    "authors": [
      "Danqi Chen",
      "Jason Bolton",
      "Christopher D. Manning"
    ],
    "summary": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by cr...",
    "published": "Jun 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1606.02858v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
    "authors": [
      "Graham Neubig",
      "Chris Dyer"
    ],
    "summary": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often ach...",
    "published": "Jun 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1606.00499v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation",
    "authors": [
      "Tejas D. Kulkarni",
      "Karthik R. Narasimhan",
      "Ardavan Saeedi",
      "Joshua B. Tenenbaum"
    ],
    "summary": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explo...",
    "published": "Apr 20",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1604.06057v2",
    "queried_author": "Karthik R Narasimhan",
    "matching_authors": [
      "Karthik R Narasimhan"
    ]
  },
  {
    "title": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
    "authors": [
      "Greg Durrett",
      "Taylor Berg-Kirkpatrick",
      "Dan Klein"
    ],
    "summary": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deleti...",
    "published": "Mar 29",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1603.08887v2",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Learning Executable Semantic Parsers for Natural Language Understanding",
    "authors": [
      "Percy Liang"
    ],
    "summary": "For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are i...",
    "published": "Mar 22",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1603.06677v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Efficient AUC Optimization for Information Ranking Applications",
    "authors": [
      "Sean J. Welleck"
    ],
    "summary": "Adequate evaluation of an information retrieval system to estimate future performance is a crucial task. Area under the ROC curve (AUC) is widely used to evaluate the generalization of a retrieval system. However, the objective function optimized in many retrieval systems is the error rate and not t...",
    "published": "Nov 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1511.05202v3",
    "queried_author": "Sean J Welleck",
    "matching_authors": [
      "Sean J Welleck"
    ]
  },
  {
    "title": "A Neural Attention Model for Abstractive Sentence Summarization",
    "authors": [
      "Alexander M. Rush",
      "Sumit Chopra",
      "Jason Weston"
    ],
    "summary": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each ...",
    "published": "Sep 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1509.00685v2",
    "queried_author": "Alexander M Rush",
    "matching_authors": [
      "Alexander M Rush",
      "Alexander M. Rush"
    ]
  },
  {
    "title": "Neural CRF Parsing",
    "authors": [
      "Greg Durrett",
      "Dan Klein"
    ],
    "summary": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse featu...",
    "published": "Jul 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1507.03641v1",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture of k Gaussians, for any Constant k",
    "authors": [
      "Jerry Li",
      "Ludwig Schmidt"
    ],
    "summary": "Learning a Gaussian mixture model (GMM) is a fundamental problem in machine learning, learning theory, and statistics. One notion of learning a GMM is proper learning: here, the goal is to find a mixture of $k$ Gaussians $\\mathcal{M}$ that is close to the density $f$ of the unknown distribution from...",
    "published": "Jun 03",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1506.01367v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Sample-Optimal Density Estimation in Nearly-Linear Time",
    "authors": [
      "Jayadev Acharya",
      "Ilias Diakonikolas",
      "Jerry Li",
      "Ludwig Schmidt"
    ],
    "summary": "We design a new, fast algorithm for agnostically learning univariate probability distributions whose densities are well approximated by piecewise polynomial functions. Let $f$ be the density function of an arbitrary univariate distribution, and suppose that $f$ is $\\mathrm{OPT}$-close in $L_1$-dista...",
    "published": "Jun 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1506.00671v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "An Empirical Comparison of Parsing Methods for Stanford Dependencies",
    "authors": [
      "Lingpeng Kong",
      "Noah A. Smith"
    ],
    "summary": "Stanford typed dependencies are a widely desired representation of natural language sentences, but parsing is one of the major computational bottlenecks in text analysis systems. In light of the evolving definition of the Stanford dependencies and developments in statistical dependency parsing algor...",
    "published": "Apr 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1404.4314v1",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Cornell SPF: Cornell Semantic Parsing Framework",
    "authors": [
      "Yoav Artzi"
    ],
    "summary": "The Cornell Semantic Parsing Framework (SPF) is a learning and inference framework for mapping natural language to formal representation of its meaning.",
    "published": "Nov 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1311.3011v2",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Lambda Dependency-Based Compositional Semantics",
    "authors": [
      "Percy Liang"
    ],
    "summary": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in ...",
    "published": "Sep 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1309.4408v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Semantic Understanding of Professional Soccer Commentaries",
    "authors": [
      "Hannaneh Hajishirzi",
      "Mohammad Rastegari",
      "Ali Farhadi",
      "Jessica K. Hodgins"
    ],
    "summary": "This paper presents a novel approach to the problem of semantic parsing via learning the correspondences between complex sentences and rich sets of events. Our main intuition is that correct correspondences tend to occur more frequently. Our model benefits from a discriminative notion of similarity ...",
    "published": "Oct 16",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1210.4854v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "A high molecular fraction in a sub-damped absorber at z=0.56",
    "authors": [
      "Neil H. M. Crighton",
      "Jill Bechtold",
      "Robert F. Carswell",
      "Romeel Dav\u00e9",
      "Craig B. Foltz",
      "Buell T. Jannuzi",
      "Simon L. Morris",
      "John M. O'Meara",
      "J. Xavier Prochaska",
      "Joop Schaye",
      "Nicolas Tejos"
    ],
    "summary": "Measuring rest-frame ultraviolet rotational transitions from the Lyman and Werner bands in absorption against a bright background continuum is one of the few ways to directly measure molecular hydrogen (H2). Here we report the detection of Lyman-Werner absorption from H2 at z=0.56 in a sub-damped Ly...",
    "published": "Oct 02",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1210.0905v2",
    "queried_author": "John Xavier Morris",
    "matching_authors": [
      "John Xavier Morris"
    ]
  },
  {
    "title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars",
    "authors": [
      "Luke S. Zettlemoyer",
      "Michael Collins"
    ],
    "summary": "This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, alo...",
    "published": "Jul 04",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1207.1420v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Adversarial Evaluation for Models of Natural Language",
    "authors": [
      "Noah A. Smith"
    ],
    "summary": "We now have a rich and growing set of modeling tools and algorithms for inducing linguistic structure from text that is less than fully annotated. In this paper, we discuss some of the weaknesses of our current methodology. We present a new abstract framework for evaluating natural language processi...",
    "published": "Jul 01",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1207.0245v2",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "The Complexity of Learning Principles and Parameters Grammars",
    "authors": [
      "Jacob Andreas"
    ],
    "summary": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we ...",
    "published": "Jun 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1207.0052v3",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Sampling First Order Logical Particles",
    "authors": [
      "Hannaneh Hajishirzi",
      "Eyal Amir"
    ],
    "summary": "Approximate inference in dynamic systems is the problem of estimating the state of the system given a sequence of actions and partial observations. High precision estimation is fundamental in many applications like diagnosis, natural language processing, tracking, planning, and robotics. In this pap...",
    "published": "Jun 13",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1206.3264v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "The Infinite Latent Events Model",
    "authors": [
      "David Wingate",
      "Noah Goodman",
      "Daniel Roy",
      "Joshua Tenenbaum"
    ],
    "summary": "We present the Infinite Latent Events Model, a nonparametric hierarchical Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with binary state representations and noisy-OR-like transitions. The distribution can be used to learn structure in discrete timeseries data by simultan...",
    "published": "May 09",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1205.2604v1",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "Reasoning about RoboCup Soccer Narratives",
    "authors": [
      "Hannaneh Hajishirzi",
      "Julia Hockenmaier",
      "Erik T. Mueller",
      "Eyal Amir"
    ],
    "summary": "This paper presents an approach for learning to translate simple narratives, i.e., texts (sequences of sentences) describing dynamic systems, into coherent sequences of events without the need for labeled training data. Our approach incorporates domain knowledge in the form of preconditions and effe...",
    "published": "Feb 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1202.3728v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Inducing Probabilistic Programs by Bayesian Program Merging",
    "authors": [
      "Irvin Hwang",
      "Andreas Stuhlm\u00fcller",
      "Noah D. Goodman"
    ],
    "summary": "This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin w...",
    "published": "Oct 25",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1110.5667v1",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "Learning Dependency-Based Compositional Semantics",
    "authors": [
      "Percy Liang",
      "Michael I. Jordan",
      "Dan Klein"
    ],
    "summary": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are t...",
    "published": "Sep 30",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/1109.6841v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Unraveling L_{n,k}: Grassmannian Kinematics",
    "authors": [
      "Jared Kaplan"
    ],
    "summary": "It was recently proposed that the leading singularities of the S-Matrix of N = 4 super Yang-Mills theory arise as the residues of a contour integral over a Grassmannian manifold, with space-time locality encoded through residue theorems generalizing Cauchy's theorem to more than one variable. We pro...",
    "published": "Dec 07",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/0912.0957v1",
    "queried_author": "Jared Kaplan",
    "matching_authors": [
      "Jared Kaplan"
    ]
  },
  {
    "title": "IDF revisited: A simple new derivation within the Robertson-Sp\u00e4rck Jones probabilistic model",
    "authors": [
      "Lillian Lee"
    ],
    "summary": "There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's probabilistic model are based on strong or complex assumptions. We show that a more intuitively plausibl...",
    "published": "May 08",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/0705.1161v1",
    "queried_author": "Lillian Lee",
    "matching_authors": [
      "Lillian Lee"
    ]
  },
  {
    "title": "Dark Matter Generation and Split Supersymmetry",
    "authors": [
      "Jared Kaplan"
    ],
    "summary": "We analyze a simple Split Supersymmetry scenario where fermion masses come from anomaly mediation, yielding m_s ~ 1000 TeV, m_{3/2} ~ 100 TeV, and m_f ~ 1 TeV. We consider non-thermal dark matter production in the presence of moduli, and we find that the decay chains of moduli to LSPs and moduli to ...",
    "published": "Jan 31",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/hep-ph/0601262v3",
    "queried_author": "Jared Kaplan",
    "matching_authors": [
      "Jared Kaplan"
    ]
  },
  {
    "title": "Overtwisted open books from sobering arcs",
    "authors": [
      "Noah Goodman"
    ],
    "summary": "We study open books on three manifolds which are compatible with an overtwisted contact structure. We show that the existence of certain arcs, called sobering arcs, is a sufficient condition for an open book to be overtwisted, and is necessary up to stabilization by positive Hopf-bands. Using these ...",
    "published": "Jul 24",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/math/0407420v2",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "Extracting Data from Behind Horizons with the AdS/CFT Correspondence",
    "authors": [
      "Jared Kaplan"
    ],
    "summary": "Recent work has shown that boundary correlators in AdS-Schwarzschild can probe the geometry near the singularity. In this paper we aim to analyze the specific signatures of the singularity, show how significant they can be, and uncover the origins of these large effects in explicitly outside the hor...",
    "published": "Feb 07",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/hep-th/0402066v1",
    "queried_author": "Jared Kaplan",
    "matching_authors": [
      "Jared Kaplan"
    ]
  },
  {
    "title": "Measures of Distributional Similarity",
    "authors": [
      "Lillian Lee"
    ],
    "summary": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; a...",
    "published": "Jan 18",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/cs/0001012v1",
    "queried_author": "Lillian Lee",
    "matching_authors": [
      "Lillian Lee"
    ]
  },
  {
    "title": "Probabilistic Parsing Using Left Corner Language Models",
    "authors": [
      "Christopher D. Manning",
      "Bob Carpenter"
    ],
    "summary": "We introduce a novel parser based on a probabilistic version of a left-corner parser. The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations. We develop the underlying theory and explain how a grammar can be induced from ...",
    "published": "Nov 17",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/cmp-lg/9711003v1",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication",
    "authors": [
      "Lillian Lee"
    ],
    "summary": "Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time $O(|G||w|^{3 - \\myeps})$ on a grammar $G$ and a string $w$ can be used to multiply $m \\times m$ Boolean matrices in time $O(m^{3 - \\myeps/3})$. In the process we a...",
    "published": "Aug 14",
    "pdf_url": null,
    "arxiv_url": "http://arxiv.org/abs/cmp-lg/9708008v1",
    "queried_author": "Lillian Lee",
    "matching_authors": [
      "Lillian Lee"
    ]
  }
]