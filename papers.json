[
  {
    "title": "Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads",
    "authors": [
      "Shaswat Patel",
      "Vishvesh Trivedi",
      "Yue Han",
      "Yihuai Hong",
      "Eunsol Choi"
    ],
    "summary": "Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.",
    "published": "Feb 25",
    "pdf_url": "https://arxiv.org/pdf/2602.22453v1",
    "arxiv_url": "http://arxiv.org/abs/2602.22453v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "SumTablets: A Transliteration Dataset of Sumerian Tablets",
    "authors": [
      "Cole Simmons",
      "Richard Diehl Martinez",
      "Dan Jurafsky"
    ],
    "summary": "Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate pot...",
    "published": "Feb 25",
    "pdf_url": "https://arxiv.org/pdf/2602.22200v1",
    "arxiv_url": "http://arxiv.org/abs/2602.22200v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
    "authors": [
      "Melody Ma",
      "John Hewitt"
    ],
    "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowledge reasoning by default: adding a simple \"think step-by-step\" cue demonstrates statistically significant improvement in knowledge recall but not math. Motivated by this, we propose training models to reason over their parametric knowledge using world-knowledge question answering as a verifiable reward. After reinforcement learning on TriviaQA (+9.9%), performance also improves on Natural Questions, HotpotQA, SimpleQA, and StrategyQA by 4.2%, 2.1%, 0.6%, and 3.0%, respectively. Reasoning models are under-optimized for parametric knowledge access, but can be easily trained to reason better.",
    "published": "Feb 25",
    "pdf_url": "https://arxiv.org/pdf/2602.22193v1",
    "arxiv_url": "http://arxiv.org/abs/2602.22193v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt"
    ]
  },
  {
    "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
    "authors": [
      "Xi Ye",
      "Wuwei Zhang",
      "Fangcong Yin",
      "Howard Yen",
      "Danqi Chen"
    ],
    "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.",
    "published": "Feb 25",
    "pdf_url": "https://arxiv.org/pdf/2602.22175v1",
    "arxiv_url": "http://arxiv.org/abs/2602.22175v1",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning",
    "authors": [
      "Ningyuan Yang",
      "Weihua Du",
      "Weiwei Sun",
      "Sean Welleck",
      "Yiming Yang"
    ],
    "summary": "Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign",
    "published": "Feb 25",
    "pdf_url": "https://arxiv.org/pdf/2602.21492v1",
    "arxiv_url": "http://arxiv.org/abs/2602.21492v1",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
    "authors": [
      "Yining Hong",
      "Huang Huang",
      "Manling Li",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Yejin Choi"
    ],
    "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
    "published": "Feb 24",
    "pdf_url": "https://arxiv.org/pdf/2602.21198v1",
    "arxiv_url": "http://arxiv.org/abs/2602.21198v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery",
    "authors": [
      "David Anugraha",
      "Vishakh Padmakumar",
      "Diyi Yang"
    ],
    "summary": "Qualitative insights from user experiences are critical for informing product and policy decisions, but collecting such data at scale is constrained by the time and availability of experts to conduct semi-structured interviews. Recent work has explored using large language models (LLMs) to automate interviewing, yet existing systems lack a principled mechanism for balancing systematic coverage of predefined topics with adaptive exploration, or the ability to pursue follow-ups, deep dives, and emergent themes that arise organically during conversation. In this work, we formulate adaptive semi-structured interviewing as an optimization problem over the interviewer's behavior. We define interview utility as a trade-off between coverage of a predefined interview topic guide, discovery of relevant emergent themes, and interview cost measured by length. Based on this formulation, we introduce SparkMe, a multi-agent LLM interviewer that performs deliberative planning via simulated conversation rollouts to select questions with high expected utility. We evaluate SparkMe through controlled experiments with LLM-based interviewees, showing that it achieves higher interview utility, improving topic guide coverage (+4.7% over the best baseline) and eliciting richer emergent insights while using fewer conversational turns than prior LLM interviewing approaches. We further validate SparkMe in a user study with 70 participants across 7 professions on the impact of AI on their workflows. Domain experts rate SparkMe as producing high-quality adaptive interviews that surface helpful professio...",
    "published": "Feb 24",
    "pdf_url": "https://arxiv.org/pdf/2602.21136v1",
    "arxiv_url": "http://arxiv.org/abs/2602.21136v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Transcoder Adapters for Reasoning-Model Diffing",
    "authors": [
      "Nathan Hu",
      "Jake Ward",
      "Thomas Icard",
      "Christopher Potts"
    ],
    "summary": "While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., \"wait\"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.",
    "published": "Feb 24",
    "pdf_url": "https://arxiv.org/pdf/2602.20904v1",
    "arxiv_url": "http://arxiv.org/abs/2602.20904v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness",
    "authors": [
      "Peter Hase",
      "Christopher Potts"
    ],
    "summary": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training",
    "published": "Feb 24",
    "pdf_url": "https://arxiv.org/pdf/2602.20710v1",
    "arxiv_url": "http://arxiv.org/abs/2602.20710v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Agents of Chaos",
    "authors": [
      "Natalie Shapira",
      "Chris Wendler",
      "Avery Yen",
      "Gabriele Sarti",
      "Koyena Pal",
      "Olivia Floody",
      "Adam Belfki",
      "Alex Loftus",
      "Aditya Ratan Jannali",
      "Nikhil Prakash",
      "Jasmine Cui",
      "Giordano Rogers",
      "Jannik Brinkmann",
      "Can Rager",
      "Amir Zur",
      "Michael Ripa",
      "Aruna Sankaranarayanan",
      "David Atkinson",
      "Rohit Gandikota",
      "Jaden Fiotto-Kaufman",
      "EunJeong Hwang",
      "Hadas Orgad",
      "P Sam Sahil",
      "Negev Taglicht",
      "Tomer Shabtay",
      "Atai Ambus",
      "Nitay Alon",
      "Shiri Oron",
      "Ayelet Gordon-Tapiero",
      "Yotam Kaplan",
      "Vered Shwartz",
      "Tamar Rott Shaham",
      "Christoph Riedl",
      "Reuth Mirsky",
      "Maarten Sap",
      "David Manheim",
      "Tomer Ullman",
      "David Bau"
    ],
    "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
    "published": "Feb 23",
    "pdf_url": "https://arxiv.org/pdf/2602.20021v1",
    "arxiv_url": "http://arxiv.org/abs/2602.20021v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining",
    "authors": [
      "Jeffrey Li",
      "Josh Gardner",
      "Doug Kang",
      "Fangping Shi",
      "Karanjeet Singh",
      "Chun-Liang Li",
      "Herumb Shandilya",
      "David Hall",
      "Oncel Tuzel",
      "Percy Liang",
      "Ludwig Schmidt",
      "Hadi Pour Ansari",
      "Fartash Faghri"
    ],
    "summary": "One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.",
    "published": "Feb 23",
    "pdf_url": "https://arxiv.org/pdf/2602.19548v1",
    "arxiv_url": "http://arxiv.org/abs/2602.19548v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt",
      "Percy Liang"
    ]
  },
  {
    "title": "Learning to Detect Language Model Training Data via Active Reconstruction",
    "authors": [
      "Junjie Oscar Yin",
      "John X. Morris",
      "Vitaly Shmatikov",
      "Sewon Min",
      "Hannaneh Hajishirzi"
    ],
    "summary": "Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.",
    "published": "Feb 22",
    "pdf_url": "https://arxiv.org/pdf/2602.19020v1",
    "arxiv_url": "http://arxiv.org/abs/2602.19020v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "John X. Morris",
      "Junjie Oscar Yin"
    ]
  },
  {
    "title": "DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-Tuning",
    "authors": [
      "Fangyuan Xu",
      "Sihao Chen",
      "Zinan Lin",
      "Taiwei Shi",
      "Sydney Graham",
      "Pei Zhou",
      "Mengting Wan",
      "Alex Stein",
      "Virginia Estellers",
      "Charles Chen",
      "Morris Sharp",
      "Richard Speyer",
      "Tadas Baltrusaitis",
      "Jennifer Neville",
      "Eunsol Choi",
      "Longqi Yang"
    ],
    "summary": "Differentially private (DP) synthetic data generation plays a pivotal role in developing large language models (LLMs) on private data, where data owners cannot provide eyes-on access to individual examples. Generating DP synthetic data typically involves a difficult trade-off. On one hand, DP finetuning methods train an LLM as a synthetic data generator with formal privacy guarantees, yet it still requires the raw content of private examples for model training. However, methods that avoid direct exposure to private data are bounded by an off-the-shelf, un-finetuned model, whose outputs often lack domain fidelity. Can we train an LLM to generate high-quality synthetic text without eyes-on access to individual private examples? In this work, we introduce Differentially Private Reinforcement Fine-Tuning (DP-RFT), an online reinforcement learning algorithm for synthetic data generation with LLMs. DP-RFT leverages DP-protected nearest-neighbor votes from an eyes-off private corpus as a reward signal for on-policy synthetic samples generated by an LLM. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO). We evaluate DP-RFT for long-form and domain-specific synthetic data generation, such as news articles, meeting transcripts, and medical article abstracts. Our experiments show that DP-RFT closes the gap between private evolution and DP finetuning methods in terms of the fidelity and downstream utility of the generated synthetic data, while respecting the private data boundary.",
    "published": "Feb 20",
    "pdf_url": "https://arxiv.org/pdf/2602.18633v1",
    "arxiv_url": "http://arxiv.org/abs/2602.18633v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering",
    "authors": [
      "Deniz Qian",
      "Hung-Ting Chen",
      "Eunsol Choi"
    ],
    "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.",
    "published": "Feb 20",
    "pdf_url": "https://arxiv.org/pdf/2602.18425v1",
    "arxiv_url": "http://arxiv.org/abs/2602.18425v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
    "authors": [
      "Yutong Xin",
      "Qiaochu Chen",
      "Greg Durrett",
      "I\u015fil Dillig"
    ],
    "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.",
    "published": "Feb 20",
    "pdf_url": "https://arxiv.org/pdf/2602.18307v1",
    "arxiv_url": "http://arxiv.org/abs/2602.18307v1",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs",
    "authors": [
      "Vincent Grari",
      "Ciprian Tomoiaga",
      "Sylvain Lamprier",
      "Tatsunori Hashimoto",
      "Marcin Detyniecki"
    ],
    "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.",
    "published": "Feb 20",
    "pdf_url": "https://arxiv.org/pdf/2602.18137v1",
    "arxiv_url": "http://arxiv.org/abs/2602.18137v1",
    "queried_author": "Tatsunori Hashimoto",
    "matching_authors": [
      "Tatsunori Hashimoto"
    ]
  },
  {
    "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models",
    "authors": [
      "Dhruba Ghosh",
      "Yuhui Zhang",
      "Ludwig Schmidt"
    ],
    "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.",
    "published": "Feb 19",
    "pdf_url": "https://arxiv.org/pdf/2602.17871v1",
    "arxiv_url": "http://arxiv.org/abs/2602.17871v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
    "authors": [
      "Xinghong Fu",
      "Yanhong Li",
      "Georgios Papaioannou",
      "Yoon Kim"
    ],
    "summary": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.",
    "published": "Feb 19",
    "pdf_url": "https://arxiv.org/pdf/2602.17634v1",
    "arxiv_url": "http://arxiv.org/abs/2602.17634v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Modeling Distinct Human Interaction in Web Agents",
    "authors": [
      "Faria Huq",
      "Zora Zhiruo Wang",
      "Zhanqiu Guo",
      "Venu Arvind Arangarajan",
      "Tianyue Ou",
      "Frank Xu",
      "Shuyan Zhou",
      "Graham Neubig",
      "Jeffrey P. Bigham"
    ],
    "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
    "published": "Feb 19",
    "pdf_url": "https://arxiv.org/pdf/2602.17588v1",
    "arxiv_url": "http://arxiv.org/abs/2602.17588v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
    "authors": [
      "Hongjue Zhao",
      "Haosen Sun",
      "Jiangtao Kong",
      "Xiaochang Li",
      "Qineng Wang",
      "Liwei Jiang",
      "Qi Zhu",
      "Tarek Abdelzaher",
      "Yejin Choi",
      "Manling Li",
      "Huajie Shao"
    ],
    "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: (i) the lack of a unified theoretical framework for guiding the design of steering directions, and (ii) an over-reliance on one-step steering that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based theoretical framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a barrier function from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows empirical advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for multi-step and adaptive steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical f...",
    "published": "Feb 19",
    "pdf_url": "https://arxiv.org/pdf/2602.17560v2",
    "arxiv_url": "http://arxiv.org/abs/2602.17560v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
    "authors": [
      "Yiqing Xie",
      "Emmy Liu",
      "Gaokai Zhang",
      "Nachiket Kotalwar",
      "Shubham Gandhi",
      "Sathwik Acharya",
      "Xingyao Wang",
      "Carolyn Rose",
      "Graham Neubig",
      "Daniel Fried"
    ],
    "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16819v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16819v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
    "authors": [
      "Wenxuan Ding",
      "Nicholas Tomlin",
      "Greg Durrett"
    ],
    "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16699v2",
    "arxiv_url": "http://arxiv.org/abs/2602.16699v2",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
    "authors": [
      "Potsawee Manakul",
      "Woody Haosheng Gan",
      "Martijn Bartelds",
      "Guangzhi Sun",
      "William Held",
      "Diyi Yang"
    ],
    "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16687v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16687v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "William Held"
    ]
  },
  {
    "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
    "authors": [
      "Mubashara Akhtar",
      "Anka Reuel",
      "Prajna Soni",
      "Sanchit Ahuja",
      "Pawan Sasanka Ammanamanchi",
      "Ruchit Rawal",
      "Vil\u00e9m Zouhar",
      "Srishti Yadav",
      "Chenxi Whitehouse",
      "Dayeon Ki",
      "Jennifer Mickel",
      "Leshem Choshen",
      "Marek \u0160uppa",
      "Jan Batzner",
      "Jenny Chim",
      "Jeba Sania",
      "Yanan Long",
      "Hossein A. Rahmani",
      "Christina Knight",
      "Yiyang Nan",
      "Jyoutir Raj",
      "Yu Fan",
      "Shubham Singh",
      "Subramanyam Sahoo",
      "Eliya Habba",
      "Usman Gohar",
      "Siddhesh Pawar",
      "Robert Scholz",
      "Arjun Subramonian",
      "Jingwei Ni",
      "Mykel Kochenderfer",
      "Sanmi Koyejo",
      "Mrinmaya Sachan",
      "Stella Biderman",
      "Zeerak Talat",
      "Avijit Ghosh",
      "Irene Solaiman"
    ],
    "summary": "Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16763v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16763v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
    "authors": [
      "Nivya Talokar",
      "Ayush K Tarun",
      "Murari Mandal",
      "Maksym Andriushchenko",
      "Antoine Bosselut"
    ],
    "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16346v2",
    "arxiv_url": "http://arxiv.org/abs/2602.16346v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
    "authors": [
      "Zexue He",
      "Yu Wang",
      "Churan Zhi",
      "Yuanzhe Hu",
      "Tzu-Ping Chen",
      "Lang Yin",
      "Ze Chen",
      "Tong Arthur Wu",
      "Siru Ouyang",
      "Zihan Wang",
      "Jiaxin Pei",
      "Julian McAuley",
      "Yejin Choi",
      "Alex Pentland"
    ],
    "summary": "Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16313v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16313v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Fast KV Compaction via Attention Matching",
    "authors": [
      "Adam Zweiger",
      "Xinghong Fu",
      "Han Guo",
      "Yoon Kim"
    ],
    "summary": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.",
    "published": "Feb 18",
    "pdf_url": "https://arxiv.org/pdf/2602.16284v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16284v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
    "authors": [
      "Shankar Padmanabhan",
      "Mustafa Omer Gul",
      "Tanya Goyal"
    ],
    "summary": "Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \\methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",
    "published": "Feb 17",
    "pdf_url": "https://arxiv.org/pdf/2602.16093v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16093v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "MAEB: Massive Audio Embedding Benchmark",
    "authors": [
      "Adnan El Assadi",
      "Isaac Chung",
      "Chenghao Xiao",
      "Roman Solomatin",
      "Animesh Jha",
      "Rahul Chand",
      "Silky Singh",
      "Kaitlyn Wang",
      "Ali Sartaz Khan",
      "Marc Moussa Nasser",
      "Sufen Fong",
      "Pengfei He",
      "Alan Xiao",
      "Ayush Sunil Munot",
      "Aditya Shrivastava",
      "Artem Gazizov",
      "Niklas Muennighoff",
      "Kenneth Enevoldsen"
    ],
    "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
    "published": "Feb 17",
    "pdf_url": "https://arxiv.org/pdf/2602.16008v1",
    "arxiv_url": "http://arxiv.org/abs/2602.16008v1",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "Discovering Implicit Large Language Model Alignment Objectives",
    "authors": [
      "Edward Chen",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "summary": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.",
    "published": "Feb 17",
    "pdf_url": "https://arxiv.org/pdf/2602.15338v1",
    "arxiv_url": "http://arxiv.org/abs/2602.15338v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models",
    "authors": [
      "Avinandan Bose",
      "Shuyue Stella Li",
      "Faeze Brahman",
      "Pang Wei Koh",
      "Simon Shaolei Du",
      "Yulia Tsvetkov",
      "Maryam Fazel",
      "Lin Xiao",
      "Asli Celikyilmaz"
    ],
    "summary": "Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-...",
    "published": "Feb 16",
    "pdf_url": "https://arxiv.org/pdf/2602.15012v1",
    "arxiv_url": "http://arxiv.org/abs/2602.15012v1",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Attention Head Entropy of LLMs Predicts Answer Correctness",
    "authors": [
      "Sophie Ostmeier",
      "Brian Axelrod",
      "Maya Varma",
      "Asad Aali",
      "Yabin Zhang",
      "Magdalini Paschali",
      "Sanmi Koyejo",
      "Curtis Langlotz",
      "Akshay Chaudhari"
    ],
    "summary": "Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.",
    "published": "Feb 14",
    "pdf_url": "https://arxiv.org/pdf/2602.13699v1",
    "arxiv_url": "http://arxiv.org/abs/2602.13699v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer",
    "authors": [
      "Edward Chen",
      "Natalie Dullerud",
      "Pang Wei Koh",
      "Thomas Niedermayr",
      "Elizabeth Kidd",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "summary": "In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.",
    "published": "Feb 14",
    "pdf_url": "https://arxiv.org/pdf/2602.13666v1",
    "arxiv_url": "http://arxiv.org/abs/2602.13666v1",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Small Reward Models via Backward Inference",
    "authors": [
      "Yike Wang",
      "Faeze Brahman",
      "Shangbin Feng",
      "Teng Xiao",
      "Hannaneh Hajishirzi",
      "Yulia Tsvetkov"
    ],
    "summary": "Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.",
    "published": "Feb 14",
    "pdf_url": "https://arxiv.org/pdf/2602.13551v2",
    "arxiv_url": "http://arxiv.org/abs/2602.13551v2",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "authors": [
      "Jiahao Lu",
      "Ziwei Xu",
      "William Tjhi",
      "Junnan Li",
      "Antoine Bosselut",
      "Pang Wei Koh",
      "Mohan Kankanhalli"
    ],
    "summary": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.\n  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to...",
    "published": "Feb 13",
    "pdf_url": "https://arxiv.org/pdf/2602.13033v2",
    "arxiv_url": "http://arxiv.org/abs/2602.13033v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "The Appeal and Reality of Recycling LoRAs with Adaptive Merging",
    "authors": [
      "Haokun Liu",
      "Gyung Hyun Je",
      "Marco Ciccone",
      "Zhenlin Xu",
      "Prasanth YSS",
      "Colin Raffel"
    ],
    "summary": "The widespread availability of fine-tuned LoRA modules for open pre-trained models has led to an interest in methods that can adaptively merge LoRAs to improve performance. These methods typically include some way of selecting LoRAs from a pool and tune merging coefficients based on a task-specific dataset. While adaptive merging methods have demonstrated improvements in some settings, no past work has attempted to recycle LoRAs found \"in the wild\" on model repositories like the Hugging Face Hub. To address this gap, we consider recycling from a pool of nearly 1,000 user-contributed LoRAs trained from the Llama 3.1 8B-Instruct language model. Our empirical study includes a range of adaptive and non-adaptive merging methods in addition to a new method designed via a wide search over the methodological design space. We demonstrate that adaptive merging methods can improve performance over the base model but provide limited benefit over training a new LoRA on the same data used to set merging coefficients. We additionally find not only that the specific choice of LoRAs to merge has little importance, but that using LoRAs with randomly initialized parameter values yields similar performance. This raises the possibility that adaptive merging from recycled LoRAs primarily works via some kind of regularization effect, rather than by enabling positive cross-task transfer. To better understand why past work has proven successful, we confirm that positive transfer is indeed possible when there are highly relevant LoRAs in the pool. We release the model checkpoints and code online.",
    "published": "Feb 12",
    "pdf_url": "https://arxiv.org/pdf/2602.12323v1",
    "arxiv_url": "http://arxiv.org/abs/2602.12323v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "authors": [
      "Mayee F. Chen",
      "Tyler Murray",
      "David Heineman",
      "Matt Jordan",
      "Hannaneh Hajishirzi",
      "Christopher R\u00e9",
      "Luca Soldaini",
      "Kyle Lo"
    ],
    "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
    "published": "Feb 12",
    "pdf_url": "https://arxiv.org/pdf/2602.12237v1",
    "arxiv_url": "http://arxiv.org/abs/2602.12237v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9",
      "Hannaneh Hajishirzi",
      "Kyle Lo",
      "Luca Soldaini"
    ]
  },
  {
    "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids",
    "authors": [
      "Aviv Bick",
      "Eric P. Xing",
      "Albert Gu"
    ],
    "summary": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.",
    "published": "Feb 11",
    "pdf_url": "https://arxiv.org/pdf/2602.11374v1",
    "arxiv_url": "http://arxiv.org/abs/2602.11374v1",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "Cross-Sectional Asset Retrieval via Future-Aligned Soft Contrastive Learning",
    "authors": [
      "Hyeongmin Lee",
      "Chanyeol Choi",
      "Jihoon Kwon",
      "Yoon Kim",
      "Alejandro Lopez-Lira",
      "Wonbin Ahn",
      "Yongjae Lee"
    ],
    "summary": "Asset retrieval--finding similar assets in a financial universe--is central to quantitative investment decision-making. Existing approaches define similarity through historical price patterns or sector classifications, but such backward-looking criteria provide no guarantee about future behavior. We argue that effective asset retrieval should be future-aligned: the retrieved assets should be those most likely to exhibit correlated future returns. To this end, we propose Future-Aligned Soft Contrastive Learning (FASCL), a representation learning framework whose soft contrastive loss uses pairwise future return correlations as continuous supervision targets. We further introduce an evaluation protocol designed to directly assess whether retrieved assets share similar future trajectories. Experiments on 4,229 US equities demonstrate that FASCL consistently outperforms 13 baselines across all future-behavior metrics. The source code will be available soon.",
    "published": "Feb 11",
    "pdf_url": "https://arxiv.org/pdf/2602.10711v1",
    "arxiv_url": "http://arxiv.org/abs/2602.10711v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning",
    "authors": [
      "Arnav Shah",
      "Junzhe Li",
      "Parsa Idehpour",
      "Adibvafa Fallahpour",
      "Brandon Wang",
      "Sukjun Hwang",
      "Bo Wang",
      "Patrick D. Hsu",
      "Hani Goodarzi",
      "Albert Gu"
    ],
    "summary": "Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \\times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.",
    "published": "Feb 11",
    "pdf_url": "https://arxiv.org/pdf/2602.10603v2",
    "arxiv_url": "http://arxiv.org/abs/2602.10603v2",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "Simple LLM Baselines are Competitive for Model Diffing",
    "authors": [
      "Elias Kempf",
      "Simon Schrodi",
      "Bartosz Cywi\u0144ski",
      "Thomas Brox",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "summary": "Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.",
    "published": "Feb 10",
    "pdf_url": "https://arxiv.org/pdf/2602.10371v1",
    "arxiv_url": "http://arxiv.org/abs/2602.10371v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs",
    "authors": [
      "Joesph An",
      "Phillip Keung",
      "Jiaqi Wang",
      "Orevaoghene Ahia",
      "Noah A. Smith"
    ],
    "summary": "Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.",
    "published": "Feb 10",
    "pdf_url": "https://arxiv.org/pdf/2602.10230v1",
    "arxiv_url": "http://arxiv.org/abs/2602.10230v1",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability",
    "authors": [
      "Aaditya Vikram Prasad",
      "Connor Watts",
      "Jack Merullo",
      "Dhruvil Gala",
      "Owen Lewis",
      "Thomas McGrath",
      "Ekdeep Singh Lubana"
    ],
    "summary": "Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model (when run in tandem with our probing harness), while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.",
    "published": "Feb 10",
    "pdf_url": "https://arxiv.org/pdf/2602.10067v3",
    "arxiv_url": "http://arxiv.org/abs/2602.10067v3",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Overview of the TREC 2025 RAGTIME Track",
    "authors": [
      "Dawn Lawrie",
      "Sean MacAvaney",
      "James Mayfield",
      "Luca Soldaini",
      "Eugene Yang",
      "Andrew Yates"
    ],
    "summary": "The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.",
    "published": "Feb 10",
    "pdf_url": "https://arxiv.org/pdf/2602.10024v1",
    "arxiv_url": "http://arxiv.org/abs/2602.10024v1",
    "queried_author": "Luca Soldaini",
    "matching_authors": [
      "Luca Soldaini"
    ]
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "authors": [
      "Ali Hatamizadeh",
      "Shrimai Prabhumoye",
      "Igor Gitman",
      "Ximing Lu",
      "Seungju Han",
      "Wei Ping",
      "Yejin Choi",
      "Jan Kautz"
    ],
    "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of ite...",
    "published": "Feb 09",
    "pdf_url": "https://arxiv.org/pdf/2602.09000v1",
    "arxiv_url": "http://arxiv.org/abs/2602.09000v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
    "authors": [
      "Yapei Chang",
      "Kyle Lo",
      "Mohit Iyyer",
      "Luca Soldaini"
    ],
    "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
    "published": "Feb 09",
    "pdf_url": "https://arxiv.org/pdf/2602.08808v1",
    "arxiv_url": "http://arxiv.org/abs/2602.08808v1",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo",
      "Luca Soldaini"
    ]
  },
  {
    "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard",
    "authors": [
      "Anna Soligo",
      "Edward Turner",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.",
    "published": "Feb 08",
    "pdf_url": "https://arxiv.org/pdf/2602.07852v1",
    "arxiv_url": "http://arxiv.org/abs/2602.07852v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Mimetic Initialization of MLPs",
    "authors": [
      "Asher Trockman",
      "J. Zico Kolter"
    ],
    "summary": "Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.",
    "published": "Feb 06",
    "pdf_url": "https://arxiv.org/pdf/2602.07156v1",
    "arxiv_url": "http://arxiv.org/abs/2602.07156v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model",
    "authors": [
      "Jacqueline He",
      "Jonathan Hayase",
      "Wen-tau Yih",
      "Sewoong Oh",
      "Luke Zettlemoyer",
      "Pang Wei Koh"
    ],
    "summary": "Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.",
    "published": "Feb 06",
    "pdf_url": "https://arxiv.org/pdf/2602.07120v1",
    "arxiv_url": "http://arxiv.org/abs/2602.07120v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "When RL Meets Adaptive Speculative Training: A Unified Training-Serving System",
    "authors": [
      "Junxiong Wang",
      "Fengxiang Bie",
      "Jisen Li",
      "Zhongzhu Zhou",
      "Zelei Shao",
      "Yubo Wang",
      "Yinghui Liu",
      "Qingyang Wu",
      "Avner May",
      "Sri Yanamandra",
      "Yineng Zhang",
      "Ce Zhang",
      "Tri Dao",
      "Percy Liang",
      "Ben Athiwaratkun",
      "Shuaiwen Leon Song",
      "Chenfeng Xu",
      "Xiaoxia Wu"
    ],
    "summary": "Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.\n  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 ...",
    "published": "Feb 06",
    "pdf_url": "https://arxiv.org/pdf/2602.06932v1",
    "arxiv_url": "http://arxiv.org/abs/2602.06932v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Tri Dao"
    ]
  },
  {
    "title": "BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks",
    "authors": [
      "Nishant Balepur",
      "Bhavya Rajasekaran",
      "Jane Oh",
      "Michael Xie",
      "Atrey Desai",
      "Vipul Gupta",
      "Steven James Moore",
      "Eunsol Choi",
      "Rachel Rudinger",
      "Jordan Lee Boyd-Graber"
    ],
    "summary": "Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.",
    "published": "Feb 05",
    "pdf_url": "https://arxiv.org/pdf/2602.06221v1",
    "arxiv_url": "http://arxiv.org/abs/2602.06221v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Large Language Model Reasoning Failures",
    "authors": [
      "Peiyang Song",
      "Pengrui Han",
      "Noah Goodman"
    ],
    "summary": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.",
    "published": "Feb 05",
    "pdf_url": "https://arxiv.org/pdf/2602.06176v1",
    "arxiv_url": "http://arxiv.org/abs/2602.06176v1",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "The Story is Not the Science: Execution-Grounded Evaluation of Mechanistic Interpretability Research",
    "authors": [
      "Xiaoyan Bai",
      "Alexander Baumgartner",
      "Haojia Sun",
      "Ari Holtzman",
      "Chenhao Tan"
    ],
    "summary": "Reproducibility crises across sciences highlight the limitations of the paper-centric review system in assessing the rigor and reproducibility of research. AI agents that autonomously design and generate large volumes of research outputs exacerbate these challenges. In this work, we address the growing challenges of scalability and rigor by flipping the dynamic and developing AI agents as research evaluators. We propose the first execution-grounded evaluation framework that verifies research beyond narrative review by examining code and data alongside the paper. We use mechanistic interpretability research as a testbed, build standardized research output, and develop MechEvalAgent, an automated evaluation framework that assesses the coherence of the experimental process, the reproducibility of results, and the generalizability of findings. We show that our framework achieves above 80% agreement with human judges, identifies substantial methodological problems, and surfaces 51 additional issues that human reviewers miss. Our work demonstrates the potential of AI agents to transform research evaluation and pave the way for rigorous scientific practices.",
    "published": "Feb 05",
    "pdf_url": "https://arxiv.org/pdf/2602.18458v1",
    "arxiv_url": "http://arxiv.org/abs/2602.18458v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Multi-Token Prediction via Self-Distillation",
    "authors": [
      "John Kirchenbauer",
      "Abhimanyu Hans",
      "Brian Bartoldson",
      "Micah Goldblum",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
    "published": "Feb 05",
    "pdf_url": "https://arxiv.org/pdf/2602.06019v1",
    "arxiv_url": "http://arxiv.org/abs/2602.06019v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning",
    "authors": [
      "Haojin Wang",
      "Yike Wang",
      "Shangbin Feng",
      "Hannaneh Hajishirzi",
      "Yulia Tsvetkov"
    ],
    "summary": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.",
    "published": "Feb 05",
    "pdf_url": "https://arxiv.org/pdf/2602.05307v1",
    "arxiv_url": "http://arxiv.org/abs/2602.05307v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
    "authors": [
      "Pingyue Zhang",
      "Zihan Huang",
      "Yue Wang",
      "Jieyu Zhang",
      "Letian Xue",
      "Zihan Wang",
      "Qineng Wang",
      "Keshigeyan Chandrasegaran",
      "Ruohan Zhang",
      "Yejin Choi",
      "Ranjay Krishna",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Manling Li"
    ],
    "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
    "published": "Feb 04",
    "pdf_url": "https://arxiv.org/pdf/2602.07055v1",
    "arxiv_url": "http://arxiv.org/abs/2602.07055v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
    "authors": [
      "Xinyu Yang",
      "Junlin Han",
      "Rishi Bommasani",
      "Jinqi Luo",
      "Wenjie Qu",
      "Wangchunshu Zhou",
      "Adel Bibi",
      "Xiyao Wang",
      "Jaehong Yoon",
      "Elias Stengel-Eskin",
      "Shengbang Tong",
      "Lingfeng Shen",
      "Rafael Rafailov",
      "Runjia Li",
      "Zhaoyang Wang",
      "Yiyang Zhou",
      "Chenhang Cui",
      "Yu Wang",
      "Wenhao Zheng",
      "Huichi Zhou",
      "Jindong Gu",
      "Zhaorun Chen",
      "Peng Xia",
      "Tony Lee",
      "Thomas Zollo",
      "Vikash Sehwag",
      "Jixuan Leng",
      "Jiuhai Chen",
      "Yuxin Wen",
      "Huan Zhang",
      "Zhun Deng",
      "Linjun Zhang",
      "Pavel Izmailov",
      "Pang Wei Koh",
      "Yulia Tsvetkov",
      "Andrew Wilson",
      "Jiaheng Zhang",
      "James Zou",
      "Cihang Xie",
      "Hao Wang",
      "Philip Torr",
      "Julian McAuley",
      "David Alvarez-Melis",
      "Florian Tram\u00e8r",
      "Kaidi Xu",
      "Suman Jana",
      "Chris Callison-Burch",
      "Rene Vidal",
      "Filippos Kokkinos",
      "Mohit Bansal",
      "Beidi Chen",
      "Huaxiu Yao"
    ],
    "summary": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
    "published": "Feb 04",
    "pdf_url": "https://arxiv.org/pdf/2602.08145v1",
    "arxiv_url": "http://arxiv.org/abs/2602.08145v1",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh",
      "Pavel Izmailov",
      "Rafael Rafailov"
    ]
  },
  {
    "title": "Forecasting Future Language: Context Design for Mention Markets",
    "authors": [
      "Sumin Kim",
      "Jihoon Kwon",
      "Yoon Kim",
      "Nicole Kagan",
      "Raffi Khatchadourian",
      "Wonbin Ahn",
      "Alejandro Lopez-Lira",
      "Jaewon Lee",
      "Yoontae Hwang",
      "Oscar Levy",
      "Yongjae Lee",
      "Chanyeol Choi"
    ],
    "summary": "Mention markets, a type of prediction market in which contracts resolve based on whether a specified keyword is mentioned during a future public event, require accurate probabilistic forecasts of keyword-mention outcomes. While recent work shows that large language models (LLMs) can generate forecasts competitive with human forecasters, it remains unclear how input context should be designed to support accurate prediction. In this paper, we study this question through experiments on earnings-call mention markets, which require forecasting whether a company will mention a specified keyword during its upcoming call. We run controlled comparisons varying (i) which contextual information is provided (news and/or prior earnings-call transcripts) and (ii) how \\textit{market probability}, (i.e., prediction market contract price) is used. We introduce Market-Conditioned Prompting (MCP), which explicitly treats the market-implied probability as a prior and instructs the LLM to update this prior using textual evidence, rather than re-predicting the base rate from scratch. In our experiments, we find three insights: (1) richer context consistently improves forecasting performance; (2) market-conditioned prompting (MCP), which treats the market probability as a prior and updates it using textual evidence, yields better-calibrated forecasts; and (3) a mixture of the market probability and MCP (MixMCP) outperforms the market baseline. By dampening the LLM's posterior update with the market prior, MixMCP yields more robust predictions than either the market or the LLM alone.",
    "published": "Feb 04",
    "pdf_url": "https://arxiv.org/pdf/2602.21229v1",
    "arxiv_url": "http://arxiv.org/abs/2602.21229v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Learning to Reason in 13 Parameters",
    "authors": [
      "John X. Morris",
      "Niloofar Mireshghallah",
      "Mark Ibrahim",
      "Saeed Mahloujifar"
    ],
    "summary": "Recent research has shown that language models can learn to \\textit{reason}, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. We question whether even rank=1 LoRA is necessary for learning to reason and propose TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within our new parameterization, we are able to train the 8B parameter size of Qwen2.5 to 91\\% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). We find this trend holds in general: we are able to recover 90\\% of performance improvements while training $1000x$ fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, we are only able to achieve such strong performance with RL: models trained using SFT require $100-1000x$ larger updates to reach the same performance.",
    "published": "Feb 04",
    "pdf_url": "https://arxiv.org/pdf/2602.04118v1",
    "arxiv_url": "http://arxiv.org/abs/2602.04118v1",
    "queried_author": "John X. Morris",
    "matching_authors": [
      "John X. Morris",
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "BASS: Benchmarking Audio LMs for Musical Structure and Semantic Reasoning",
    "authors": [
      "Min Jang",
      "Orevaoghene Ahia",
      "Nazif Tamer",
      "Sachin Kumar",
      "Yulia Tsvetkov",
      "Noah A. Smith"
    ],
    "summary": "Music understanding is a complex task that often requires reasoning over both structural and semantic elements of audio. We introduce BASS, designed to evaluate music understanding and reasoning in audio language models across four broad categories: structural segmentation, lyric transcription, musicological analysis, and artist collaboration. BASS comprises 2658 questions spanning 12 tasks, 1993 unique songs and covering over 138 hours of music from a wide range of genres and tracks, crafted to assess musicological knowledge and reasoning in real-world scenarios. We evaluate 14 open-source and frontier multimodal LMs, finding that even state-of-the-art models struggle on higher-level reasoning tasks such as structural segmentation and artist collaboration, while performing best on lyric transcription. Our analysis reveals that current models leverage linguistic priors effectively but remain limited in reasoning over musical structure, vocal, and musicological attributes. BASS provides an evaluation framework with widespread applications in music recommendation and search and has the potential to guide the development of audio LMs.",
    "published": "Feb 03",
    "pdf_url": "https://arxiv.org/pdf/2602.04085v1",
    "arxiv_url": "http://arxiv.org/abs/2602.04085v1",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Antidistillation Fingerprinting",
    "authors": [
      "Yixuan Even Xu",
      "John Kirchenbauer",
      "Yash Savani",
      "Asher Trockman",
      "Alexander Robey",
      "Tom Goldstein",
      "Fei Fang",
      "J. Zico Kolter"
    ],
    "summary": "Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.",
    "published": "Feb 03",
    "pdf_url": "https://arxiv.org/pdf/2602.03812v1",
    "arxiv_url": "http://arxiv.org/abs/2602.03812v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Reasoning with Latent Tokens in Diffusion Language Models",
    "authors": [
      "Andre He",
      "Sean Welleck",
      "Daniel Fried"
    ],
    "summary": "Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.",
    "published": "Feb 03",
    "pdf_url": "https://arxiv.org/pdf/2602.03769v1",
    "arxiv_url": "http://arxiv.org/abs/2602.03769v1",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Can Large Language Models Generalize Procedures Across Representations?",
    "authors": [
      "Fangru Lin",
      "Valentin Hofmann",
      "Xingchen Wan",
      "Weixing Wang",
      "Zifeng Ding",
      "Anthony G. Cohn",
      "Janet B. Pierrehumbert"
    ],
    "summary": "Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.",
    "published": "Feb 03",
    "pdf_url": "https://arxiv.org/pdf/2602.03542v1",
    "arxiv_url": "http://arxiv.org/abs/2602.03542v1",
    "queried_author": "Valentin Hofmann",
    "matching_authors": [
      "Valentin Hofmann"
    ]
  },
  {
    "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch",
    "authors": [
      "Hyunwoo Kim",
      "Niloofar Mireshghallah",
      "Michael Duan",
      "Rui Xin",
      "Shuyue Stella Li",
      "Jaehun Jung",
      "David Acuna",
      "Qi Pang",
      "Hanshen Xiao",
      "G. Edward Suh",
      "Sewoong Oh",
      "Yulia Tsvetkov",
      "Pang Wei Koh",
      "Yejin Choi"
    ],
    "summary": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.",
    "published": "Feb 03",
    "pdf_url": "https://arxiv.org/pdf/2602.03183v1",
    "arxiv_url": "http://arxiv.org/abs/2602.03183v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah",
      "Pang Wei Koh",
      "Yejin Choi"
    ]
  },
  {
    "title": "An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence",
    "authors": [
      "Qizhen Zhang",
      "Ankush Garg",
      "Jakob Foerster",
      "Niladri Chatterji",
      "Kshitiz Malik",
      "Mike Lewis"
    ],
    "summary": "Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.",
    "published": "Feb 02",
    "pdf_url": "https://arxiv.org/pdf/2602.02400v1",
    "arxiv_url": "http://arxiv.org/abs/2602.02400v1",
    "queried_author": "Mike Lewis",
    "matching_authors": [
      "Mike Lewis"
    ]
  },
  {
    "title": "The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors",
    "authors": [
      "Rapha\u00ebl Sarfati",
      "Eric Bigelow",
      "Daniel Wurgaft",
      "Jack Merullo",
      "Atticus Geiger",
      "Owen Lewis",
      "Tom McGrath",
      "Ekdeep Singh Lubana"
    ],
    "summary": "Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved \"belief manifolds\" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.",
    "published": "Feb 02",
    "pdf_url": "https://arxiv.org/pdf/2602.02315v1",
    "arxiv_url": "http://arxiv.org/abs/2602.02315v1",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Physiology as Language: Translating Respiration to Sleep EEG",
    "authors": [
      "Kaiwen Zha",
      "Chao Li",
      "Hao He",
      "Peng Cao",
      "Tianhong Li",
      "Ali Mirzazadeh",
      "Ellen Zhang",
      "Jong Woo Lee",
      "Yoon Kim",
      "Dina Katabi"
    ],
    "summary": "This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.",
    "published": "Jan 31",
    "pdf_url": "https://arxiv.org/pdf/2602.00526v1",
    "arxiv_url": "http://arxiv.org/abs/2602.00526v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Are you going to finish that? A Practical Study of the Partial Token Problem",
    "authors": [
      "Hao Xu",
      "Alisa Liu",
      "Jonathan Hayase",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "summary": "Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and \"word\" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is \"backed-off\" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.",
    "published": "Jan 30",
    "pdf_url": "https://arxiv.org/pdf/2601.23223v2",
    "arxiv_url": "http://arxiv.org/abs/2601.23223v2",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith",
      "Yejin Choi"
    ]
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?",
    "authors": [
      "Alexander H\u00e4gele",
      "Aryo Pradipta Gema",
      "Henry Sleight",
      "Ethan Perez",
      "Jascha Sohl-Dickstein"
    ],
    "summary": "As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \\emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \\emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.",
    "published": "Jan 30",
    "pdf_url": "https://arxiv.org/pdf/2601.23045v1",
    "arxiv_url": "http://arxiv.org/abs/2601.23045v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text",
    "authors": [
      "Ximing Lu",
      "David Acuna",
      "Jaehun Jung",
      "Jian Hu",
      "Di Zhang",
      "Shizhe Diao",
      "Yunheng Zou",
      "Shaokun Zhang",
      "Brandon Cui",
      "Mingjie Liu",
      "Hyunwoo Kim",
      "Prithviraj Ammanabrolu",
      "Jan Kautz",
      "Yi Dong",
      "Yejin Choi"
    ],
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automa...",
    "published": "Jan 30",
    "pdf_url": "https://arxiv.org/pdf/2601.22975v2",
    "arxiv_url": "http://arxiv.org/abs/2601.22975v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "authors": [
      "Ajay Patel",
      "Colin Raffel",
      "Chris Callison-Burch"
    ],
    "summary": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
    "published": "Jan 29",
    "pdf_url": "https://arxiv.org/pdf/2601.22146v1",
    "arxiv_url": "http://arxiv.org/abs/2601.22146v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "authors": [
      "Enyi Jiang",
      "Yibo Jacky Zhang",
      "Yinglun Xu",
      "Andreas Haupt",
      "Nancy Amato",
      "Sanmi Koyejo"
    ],
    "summary": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
    "published": "Jan 29",
    "pdf_url": "https://arxiv.org/pdf/2601.22083v2",
    "arxiv_url": "http://arxiv.org/abs/2601.22083v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents",
    "authors": [
      "Yang Song",
      "Anoushka Vyas",
      "Zirui Wei",
      "Sina Khoshfetrat Pakazad",
      "Henrik Ohlsson",
      "Graham Neubig"
    ],
    "summary": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.\n  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.\n  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.",
    "published": "Jan 29",
    "pdf_url": "https://arxiv.org/pdf/2601.21372v1",
    "arxiv_url": "http://arxiv.org/abs/2601.21372v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "MoCo: A One-Stop Shop for Model Collaboration Research",
    "authors": [
      "Shangbin Feng",
      "Yuyang Bai",
      "Ziyuan Yang",
      "Yike Wang",
      "Zhaoxuan Tan",
      "Jiajie Yan",
      "Zhenyu Lei",
      "Wenxuan Ding",
      "Weijia Shi",
      "Haojin Wang",
      "Zhenting Qi",
      "Yuru Jiang",
      "Heng Wang",
      "Chengsong Huang",
      "Yu Fei",
      "Jihan Yao",
      "Yilun Du",
      "Luke Zettlemoyer",
      "Yejin Choi",
      "Yulia Tsvetkov"
    ],
    "summary": "Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.",
    "published": "Jan 29",
    "pdf_url": "https://arxiv.org/pdf/2601.21257v1",
    "arxiv_url": "http://arxiv.org/abs/2601.21257v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer",
      "Yejin Choi"
    ]
  },
  {
    "title": "What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering",
    "authors": [
      "Jim Maar",
      "Denis Paperno",
      "Callum Stuart McDougall",
      "Neel Nanda"
    ],
    "summary": "Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. \"-ight\") or answer to a question (\"whale\") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.",
    "published": "Jan 28",
    "pdf_url": "https://arxiv.org/pdf/2601.20164v1",
    "arxiv_url": "http://arxiv.org/abs/2601.20164v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy",
    "authors": [
      "Claire O'Brien",
      "Jessica Seto",
      "Dristi Roy",
      "Aditya Dwivedi",
      "Sunishchal Dev",
      "Kevin Zhu",
      "Sean O'Brien",
      "Ashwinee Panda",
      "Ryan Lagasse"
    ],
    "summary": "Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior, decode them into residual space, and fine-tune only those neurons using gradient masking. We demonstrate this approach on the task of reducing sycophantic behavior, where our method matches or exceeds state-of-the-art performance on four benchmarks (Syco-Bench, NLP, POLI, PHIL) using Gemma-2-2B and 9B models. Our results show that sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available",
    "published": "Jan 26",
    "pdf_url": "https://arxiv.org/pdf/2601.18939v1",
    "arxiv_url": "http://arxiv.org/abs/2601.18939v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning",
    "authors": [
      "Lintang Sutawika",
      "Gokul Swamy",
      "Zhiwei Steven Wu",
      "Graham Neubig"
    ],
    "summary": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \\textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \\texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than\n  of the training data across the single-language, multilingual, and generalization to unseen language settings.",
    "published": "Jan 26",
    "pdf_url": "https://arxiv.org/pdf/2601.18722v1",
    "arxiv_url": "http://arxiv.org/abs/2601.18722v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Demographic Probing of Large Language Models Lacks Construct Validity",
    "authors": [
      "Manuel Tonneau",
      "Neil K. R. Seghal",
      "Niyati Malhotra",
      "Victor Orozco-Olvera",
      "Ana Mar\u00eda Mu\u00f1oz Boudet",
      "Lakshmi Subramanian",
      "Sharath Chandra Guntuku",
      "Valentin Hofmann"
    ],
    "summary": "Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.",
    "published": "Jan 26",
    "pdf_url": "https://arxiv.org/pdf/2601.18486v1",
    "arxiv_url": "http://arxiv.org/abs/2601.18486v1",
    "queried_author": "Valentin Hofmann",
    "matching_authors": [
      "Valentin Hofmann"
    ]
  },
  {
    "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "authors": [
      "Fangyuan Xu",
      "Rujun Han",
      "Yanfei Chen",
      "Zifeng Wang",
      "I-Hung Hsu",
      "Jun Yan",
      "Vishy Tirumalashetty",
      "Eunsol Choi",
      "Tomas Pfister",
      "Chen-Yu Lee"
    ],
    "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.",
    "published": "Jan 26",
    "pdf_url": "https://arxiv.org/pdf/2601.18202v1",
    "arxiv_url": "http://arxiv.org/abs/2601.18202v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Learning to Discover at Test Time",
    "authors": [
      "Mert Yuksekgonul",
      "Daniel Koceja",
      "Xinhao Li",
      "Federico Bianchi",
      "Jed McCaleb",
      "Xiaolong Wang",
      "Jan Kautz",
      "Yejin Choi",
      "James Zou",
      "Carlos Guestrin",
      "Yu Sun"
    ],
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "published": "Jan 22",
    "pdf_url": "https://arxiv.org/pdf/2601.16175v2",
    "arxiv_url": "http://arxiv.org/abs/2601.16175v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "authors": [
      "Moo Jin Kim",
      "Yihuai Gao",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Yunhao Ge",
      "Grace Lam",
      "Percy Liang",
      "Shuran Song",
      "Ming-Yu Liu",
      "Chelsea Finn",
      "Jinwei Gu"
    ],
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given po...",
    "published": "Jan 22",
    "pdf_url": "https://arxiv.org/pdf/2601.16163v1",
    "arxiv_url": "http://arxiv.org/abs/2601.16163v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge",
    "authors": [
      "Yiyang Feng",
      "Zeming Chen",
      "Haotian Wu",
      "Jiawei Zhou",
      "Antoine Bosselut"
    ],
    "summary": "A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.",
    "published": "Jan 21",
    "pdf_url": "https://arxiv.org/pdf/2601.15495v1",
    "arxiv_url": "http://arxiv.org/abs/2601.15495v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
    "authors": [
      "Jaydeep Borkar",
      "Karan Chadha",
      "Niloofar Mireshghallah",
      "Yuchen Zhang",
      "Irina-Elena Veliche",
      "Archi Mitra",
      "David A. Smith",
      "Zheng Xu",
      "Diego Garcia-Olano"
    ],
    "summary": "Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.",
    "published": "Jan 21",
    "pdf_url": "https://arxiv.org/pdf/2601.15394v1",
    "arxiv_url": "http://arxiv.org/abs/2601.15394v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity",
    "authors": [
      "Maciej Kilian",
      "Oleg Mkrtchyan",
      "Luke Zettlemoyer",
      "Akshat Shrivastava",
      "Armen Aghajanyan"
    ],
    "summary": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.",
    "published": "Jan 21",
    "pdf_url": "https://arxiv.org/pdf/2601.15370v1",
    "arxiv_url": "http://arxiv.org/abs/2601.15370v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Towards Execution-Grounded Automated AI Research",
    "authors": [
      "Chenglei Si",
      "Zitong Yang",
      "Yejin Choi",
      "Emmanuel Cand\u00e8s",
      "Diyi Yang",
      "Tatsunori Hashimoto"
    ],
    "summary": "Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-gr...",
    "published": "Jan 20",
    "pdf_url": "https://arxiv.org/pdf/2601.14525v1",
    "arxiv_url": "http://arxiv.org/abs/2601.14525v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Tatsunori Hashimoto",
      "Yejin Choi"
    ]
  },
  {
    "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet",
    "authors": [
      "Arpandeep Khatua",
      "Hao Zhu",
      "Peter Tran",
      "Arya Prabhudesai",
      "Frederic Sadrieh",
      "Johann K. Lieberwirth",
      "Xinkai Yu",
      "Yicheng Fu",
      "Michael J. Ryan",
      "Jiaxin Pei",
      "Diyi Yang"
    ],
    "summary": "Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelli...",
    "published": "Jan 19",
    "pdf_url": "https://arxiv.org/pdf/2601.13295v2",
    "arxiv_url": "http://arxiv.org/abs/2601.13295v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "authors": [
      "Mike A. Merrill",
      "Alexander G. Shaw",
      "Nicholas Carlini",
      "Boxuan Li",
      "Harsh Raj",
      "Ivan Bercovich",
      "Lin Shi",
      "Jeong Yeon Shin",
      "Thomas Walshe",
      "E. Kelly Buchanan",
      "Junhong Shen",
      "Guanghao Ye",
      "Haowei Lin",
      "Jason Poulos",
      "Maoyu Wang",
      "Marianna Nezhurina",
      "Jenia Jitsev",
      "Di Lu",
      "Orfeas Menis Mastromichalakis",
      "Zhiwei Xu",
      "Zizhao Chen",
      "Yue Liu",
      "Robert Zhang",
      "Leon Liangyu Chen",
      "Anurag Kashyap",
      "Jan-Lucas Uslu",
      "Jeffrey Li",
      "Jianbo Wu",
      "Minghao Yan",
      "Song Bian",
      "Vedang Sharma",
      "Ke Sun",
      "Steven Dillmann",
      "Akshay Anand",
      "Andrew Lanpouthakoun",
      "Bardia Koopah",
      "Changran Hu",
      "Etash Guha",
      "Gabriel H. S. Dreiman",
      "Jiacheng Zhu",
      "Karl Krauth",
      "Li Zhong",
      "Niklas Muennighoff",
      "Robert Amanfu",
      "Shangyin Tan",
      "Shreyas Pimpalgaonkar",
      "Tushar Aggarwal",
      "Xiangning Lin",
      "Xin Lan",
      "Xuandong Zhao",
      "Yiqing Liang",
      "Yuanli Wang",
      "Zilong Wang",
      "Changzhi Zhou",
      "David Heineman",
      "Hange Liu",
      "Harsh Trivedi",
      "John Yang",
      "Junhong Lin",
      "Manish Shetty",
      "Michael Yang",
      "Nabil Omi",
      "Negin Raoof",
      "Shanda Li",
      "Terry Yue Zhuo",
      "Wuwei Lin",
      "Yiwei Dai",
      "Yuxin Wang",
      "Wenhao Chai",
      "Shang Zhou",
      "Dariush Wahdany",
      "Ziyu She",
      "Jiaming Hu",
      "Zhikang Dong",
      "Yuxuan Zhu",
      "Sasha Cui",
      "Ahson Saiyed",
      "Arinbj\u00f6rn Kolbeinsson",
      "Jesse Hu",
      "Christopher Michael Rytting",
      "Ryan Marten",
      "Yixin Wang",
      "Alex Dimakis",
      "Andy Konwinski",
      "Ludwig Schmidt"
    ],
    "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
    "published": "Jan 17",
    "pdf_url": "https://arxiv.org/pdf/2601.11868v1",
    "arxiv_url": "http://arxiv.org/abs/2601.11868v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt",
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "Beyond Tokens: Concept-Level Training Objectives for LLMs",
    "authors": [
      "Laya Iyer",
      "Pranav Somani",
      "Alice Guo",
      "Dan Jurafsky",
      "Chen Shani"
    ],
    "summary": "The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \\textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\\rightarrow$ \\textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \\textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.",
    "published": "Jan 16",
    "pdf_url": "https://arxiv.org/pdf/2601.11791v2",
    "arxiv_url": "http://arxiv.org/abs/2601.11791v2",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Building Production-Ready Probes For Gemini",
    "authors": [
      "J\u00e1nos Kram\u00e1r",
      "Joshua Engels",
      "Zheng Wang",
      "Bilal Chughtai",
      "Rohin Shah",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "summary": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architectures that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant distribution shifts, including multi-turn conversations, long context prompts, and adaptive red teaming. Our results demonstrate that while our novel architectures address context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
    "published": "Jan 16",
    "pdf_url": "https://arxiv.org/pdf/2601.11516v4",
    "arxiv_url": "http://arxiv.org/abs/2601.11516v4",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Massively Multilingual Joint Segmentation and Glossing",
    "authors": [
      "Michael Ginn",
      "Lindia Tjuatja",
      "Enora Rice",
      "Ali Marashian",
      "Maria Valentini",
      "Jasmine Xu",
      "Graham Neubig",
      "Alexis Palmer"
    ],
    "summary": "Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.\n  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.",
    "published": "Jan 16",
    "pdf_url": "https://arxiv.org/pdf/2601.10925v2",
    "arxiv_url": "http://arxiv.org/abs/2601.10925v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model",
    "authors": [
      "Jordan Taylor",
      "William Agnew",
      "Maarten Sap",
      "Sarah E. Fox",
      "Haiyi Zhu"
    ],
    "summary": "Visual generative AI models are trained using a one-size-fits-all measure of aesthetic appeal. However, what is deemed \"aesthetic\" is inextricably linked to personal taste and cultural values, raising the question of whose taste is represented in visual generative AI models. In this work, we study an aesthetic evaluation model--LAION Aesthetic Predictor (LAP)--that is widely used to curate datasets to train visual generative image models, like Stable Diffusion, and evaluate the quality of AI-generated images. To understand what LAP measures, we audited the model across three datasets. First, we examined the impact of aesthetic filtering on the LAION-Aesthetics Dataset (approximately 1.2B images), which was curated from LAION-5B using LAP. We find that the LAP disproportionally filters in images with captions mentioning women, while filtering out images with captions mentioning men or LGBTQ+ people. Then, we used LAP to score approximately 330k images across two art datasets, finding the model rates realistic images of landscapes, cityscapes, and portraits from western and Japanese artists most highly. In doing so, the algorithmic gaze of this aesthetic evaluation model reinforces the imperial and male gazes found within western art history. In order to understand where these biases may have originated, we performed a digital ethnography of public materials related to the creation of LAP. We find that the development of LAP reflects the biases we found in our audits, such as the aesthetic scores used to train LAP primarily coming from English-speaking photographers and weste...",
    "published": "Jan 14",
    "pdf_url": "https://arxiv.org/pdf/2601.09896v2",
    "arxiv_url": "http://arxiv.org/abs/2601.09896v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm",
    "authors": [
      "Jing-Jing Li",
      "Joel Mire",
      "Eve Fleisig",
      "Valentina Pyatkin",
      "Anne Collins",
      "Maarten Sap",
      "Sydney Levine"
    ],
    "summary": "Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions -- the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond \"one-size-fits-all\" safety toward pluralistically safe AI.",
    "published": "Jan 13",
    "pdf_url": "https://arxiv.org/pdf/2601.08951v2",
    "arxiv_url": "http://arxiv.org/abs/2601.08951v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "AI as Entertainment",
    "authors": [
      "Cody Kommers",
      "Ari Holtzman"
    ],
    "summary": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may f...",
    "published": "Jan 13",
    "pdf_url": "https://arxiv.org/pdf/2601.08768v1",
    "arxiv_url": "http://arxiv.org/abs/2601.08768v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "The Roots of Performance Disparity in Multilingual Language Models: Intrinsic Modeling Difficulty or Design Choices?",
    "authors": [
      "Chen Shani",
      "Yuval Reif",
      "Nathan Roll",
      "Dan Jurafsky",
      "Ekaterina Shutova"
    ],
    "summary": "Multilingual language models (LMs) promise broader NLP access, yet current systems deliver uneven performance across the world's languages. This survey examines why these gaps persist and whether they reflect intrinsic linguistic difficulty or modeling artifacts. We organize the literature around two questions: do linguistic disparities arise from representation and allocation choices (e.g., tokenization, encoding, data exposure, parameter sharing) rather than inherent complexity; and which design choices mitigate inequities across typologically diverse languages. We review linguistic features, such as orthography, morphology, lexical diversity, syntax, information density, and typological distance, linking each to concrete modeling mechanisms. Gaps often shrink when segmentation, encoding, and data exposure are normalized, suggesting much apparent difficulty stems from current modeling choices. We synthesize these insights into design recommendations for tokenization, sampling, architectures, and evaluation to support more balanced multilingual LMs.",
    "published": "Jan 12",
    "pdf_url": "https://arxiv.org/pdf/2601.07220v2",
    "arxiv_url": "http://arxiv.org/abs/2601.07220v2",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "When Should We Introduce Safety Interventions During Pretraining?",
    "authors": [
      "Dylan Sam",
      "Sachin Goyal",
      "Pratyush Maini",
      "Alexander Robey",
      "J. Zico Kolter"
    ],
    "summary": "Prior work has shown that safety interventions applied during pretraining, such as removing and rephrasing harmful content, can substantially improve the robustness of the resulting models. In this paper, we study the fundamental question that prior work has overlooked: \"When during pretraining should safety interventions be introduced?\" We keep the underlying data sources and pretraining interventions fixed, varying the intervention start time (after 0%, 20%, or 60% of pretraining tokens). We find that the optimal start time is not one-size-fits-all: with standard top-k decoding, introducing interventions after a short initial phase of safe-only pretraining (20%-60%) often yields the strongest robustness, with the clearest benefits emerging after downstream, benign finetuning. In contrast, for safety-aware inference, interventions starting from the beginning improve steerability towards safer generations. Finally, we observe that earlier interventions reshape internal representations: linear probes more cleanly separate safe vs harmful examples. Our results are the first to establish intervention timing as a key curriculum design choice for safety.",
    "published": "Jan 11",
    "pdf_url": "https://arxiv.org/pdf/2601.07087v2",
    "arxiv_url": "http://arxiv.org/abs/2601.07087v2",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Categorize Early, Integrate Late: Divergent Processing Strategies in Automatic Speech Recognition",
    "authors": [
      "Nathan Roll",
      "Pranav Bhalerao",
      "Martijn Bartelds",
      "Arjun Pawar",
      "Yuka Tatsumi",
      "Tolulope Ogunremi",
      "Chen Shani",
      "Calbert Graham",
      "Meghan Sumner",
      "Dan Jurafsky"
    ],
    "summary": "In speech language modeling, two architectures dominate the frontier: the Transformer and the Conformer. However, it remains unknown whether their comparable performance stems from convergent processing strategies or distinct architectural inductive biases. We introduce Architectural Fingerprinting, a probing framework that isolates the effect of architecture on representation, and apply it to a controlled suite of 24 pre-trained encoders (39M-3.3B parameters). Our analysis reveals divergent hierarchies: Conformers implement a \"Categorize Early\" strategy, resolving phoneme categories 29% earlier in depth and speaker gender by 16% depth. In contrast, Transformers \"Integrate Late,\" deferring phoneme, accent, and duration encoding to deep layers (49-57%). These fingerprints suggest design heuristics: Conformers' front-loaded categorization may benefit low-latency streaming, while Transformers' deep integration may favor tasks requiring rich context and cross-utterance normalization.",
    "published": "Jan 11",
    "pdf_url": "https://arxiv.org/pdf/2601.06972v1",
    "arxiv_url": "http://arxiv.org/abs/2601.06972v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Neural Nonmyopic Bayesian Optimization in Dynamic Cost Settings",
    "authors": [
      "Sang T. Truong",
      "Duc Q. Nguyen",
      "Willie Neiswanger",
      "Ryan-Rhys Griffiths",
      "Stefano Ermon",
      "Nick Haber",
      "Sanmi Koyejo"
    ],
    "summary": "Bayesian optimization (BO) is a common framework for optimizing black-box functions, yet most existing methods assume static query costs and rely on myopic acquisition strategies. We introduce LookaHES, a nonmyopic BO framework designed for dynamic, history-dependent cost environments, where evaluation costs vary with prior actions, such as travel distance in spatial tasks or edit distance in sequence design. LookaHES combines a multi-step variant of $H$-Entropy Search with pathwise sampling and neural policy optimization, enabling long-horizon planning beyond twenty steps without the exponential complexity of existing nonmyopic methods. The key innovation is the integration of neural policies, including large language models, to effectively navigate structured, combinatorial action spaces such as protein sequences. These policies amortize lookahead planning and can be integrated with domain-specific constraints during rollout. Empirically, LookaHES outperforms strong myopic and nonmyopic baselines across nine synthetic benchmarks from two to eight dimensions and two real-world tasks: geospatial optimization using NASA night-light imagery and protein sequence design with constrained token-level edits. In short, LookaHES provides a general, scalable, and cost-aware solution for robust long-horizon optimization in complex decision spaces, which makes it a useful tool for researchers in machine learning, statistics, and applied domains. Our implementation is available at https://github.com/sangttruong/nonmyopia.",
    "published": "Jan 10",
    "pdf_url": "https://arxiv.org/pdf/2601.06505v1",
    "arxiv_url": "http://arxiv.org/abs/2601.06505v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "authors": [
      "Xuezhe Ma",
      "Shicheng Wen",
      "Linghao Jin",
      "Bilge Acun",
      "Ruihang Lai",
      "Bohan Hou",
      "Will Lin",
      "Hao Zhang",
      "Songlin Yang",
      "Ryan Lee",
      "Mengxi Wu",
      "Jonathan May",
      "Luke Zettlemoyer",
      "Carole-Jean Wu"
    ],
    "summary": "Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to $4\\times$ longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm",
    "published": "Jan 10",
    "pdf_url": "https://arxiv.org/pdf/2601.06463v1",
    "arxiv_url": "http://arxiv.org/abs/2601.06463v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue",
    "authors": [
      "Laya Iyer",
      "Kriti Aggarwal",
      "Sanmi Koyejo",
      "Gail Heyman",
      "Desmond C. Ong",
      "Subhabrata Mukherjee"
    ],
    "summary": "Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empi...",
    "published": "Jan 09",
    "pdf_url": "https://arxiv.org/pdf/2601.19922v2",
    "arxiv_url": "http://arxiv.org/abs/2601.19922v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Peter Belcak",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "published": "Jan 08",
    "pdf_url": "https://arxiv.org/pdf/2601.05242v1",
    "arxiv_url": "http://arxiv.org/abs/2601.05242v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks",
    "authors": [
      "Hoagy Cunningham",
      "Jerry Wei",
      "Zihan Wang",
      "Andrew Persic",
      "Alwin Peng",
      "Jordan Abderrachid",
      "Raj Agarwal",
      "Bobby Chen",
      "Austin Cohen",
      "Andy Dau",
      "Alek Dimitriev",
      "Rob Gilson",
      "Logan Howard",
      "Yijin Hua",
      "Jared Kaplan",
      "Jan Leike",
      "Mu Lin",
      "Christopher Liu",
      "Vladimir Mikulik",
      "Rohit Mittapalli",
      "Clare O'Hara",
      "Jin Pan",
      "Nikhil Saxena",
      "Alex Silverstein",
      "Yue Song",
      "Xunjie Yu",
      "Giulio Zhou",
      "Ethan Perez",
      "Mrinank Sharma"
    ],
    "summary": "We introduce enhanced Constitutional Classifiers that deliver production-grade jailbreak robustness with dramatically reduced computational costs and refusal rates compared to previous-generation defenses. Our system combines several key insights. First, we develop exchange classifiers that evaluate model responses in their full conversational context, which addresses vulnerabilities in last-generation systems that examine outputs in isolation. Second, we implement a two-stage classifier cascade where lightweight classifiers screen all traffic and escalate only suspicious exchanges to more expensive classifiers. Third, we train efficient linear probe classifiers and ensemble them with external classifiers to simultaneously improve robustness and reduce computational costs. Together, these techniques yield a production-grade system achieving a 40x computational cost reduction compared to our baseline exchange classifier, while maintaining a 0.05% refusal rate on production traffic. Through extensive red-teaming comprising over 1,700 hours, we demonstrate strong protection against universal jailbreaks -- no attack on this system successfully elicited responses to all eight target queries comparable in detail to an undefended model. Our work establishes Constitutional Classifiers as practical and efficient safeguards for large language models.",
    "published": "Jan 08",
    "pdf_url": "https://arxiv.org/pdf/2601.04603v1",
    "arxiv_url": "http://arxiv.org/abs/2601.04603v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez",
      "Jared Kaplan"
    ]
  },
  {
    "title": "Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs",
    "authors": [
      "Myra Cheng",
      "Robert D. Hawkins",
      "Dan Jurafsky"
    ],
    "summary": "Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase \"wait a minute\", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.",
    "published": "Jan 07",
    "pdf_url": "https://arxiv.org/pdf/2601.04435v1",
    "arxiv_url": "http://arxiv.org/abs/2601.04435v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Unified Text-Image Generation with Weakness-Targeted Post-Training",
    "authors": [
      "Jiahui Chen",
      "Philippe Hansen-Estruch",
      "Xiaochuang Han",
      "Yushi Hu",
      "Emily Dinan",
      "Amita Kamath",
      "Michal Drozdzal",
      "Reyhane Askari-Hemmat",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "summary": "Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.",
    "published": "Jan 07",
    "pdf_url": "https://arxiv.org/pdf/2601.04339v2",
    "arxiv_url": "http://arxiv.org/abs/2601.04339v2",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Quantifying the Effect of Test Set Contamination on Generative Evaluations",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "Baber Abbasi",
      "Ken Ziyu Liu",
      "Brando Miranda",
      "Ahmed Ahmed",
      "Fazl Berez",
      "Abhay Puri",
      "Stella Biderman",
      "Niloofar Mireshghallah",
      "Sanmi Koyejo"
    ],
    "summary": "As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorizat...",
    "published": "Jan 07",
    "pdf_url": "https://arxiv.org/pdf/2601.04301v2",
    "arxiv_url": "http://arxiv.org/abs/2601.04301v2",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence",
    "authors": [
      "Marc Finzi",
      "Shikai Qiu",
      "Yiding Jiang",
      "Pavel Izmailov",
      "J. Zico Kolter",
      "Andrew Gordon Wilson"
    ],
    "summary": "Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highli...",
    "published": "Jan 06",
    "pdf_url": "https://arxiv.org/pdf/2601.03220v1",
    "arxiv_url": "http://arxiv.org/abs/2601.03220v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter",
      "Pavel Izmailov"
    ]
  },
  {
    "title": "Extracting books from production language models",
    "authors": [
      "Ahmed Ahmed",
      "A. Feder Cooper",
      "Sanmi Koyejo",
      "Percy Liang"
    ],
    "summary": "Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue...",
    "published": "Jan 06",
    "pdf_url": "https://arxiv.org/pdf/2601.02671v1",
    "arxiv_url": "http://arxiv.org/abs/2601.02671v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "K-EXAONE Technical Report",
    "authors": [
      "Eunbi Choi",
      "Kibong Choi",
      "Seokhee Hong",
      "Junwon Hwang",
      "Hyojin Jeon",
      "Hyunjik Jo",
      "Joonkee Kim",
      "Seonghwan Kim",
      "Soyeon Kim",
      "Sunkyoung Kim",
      "Yireun Kim",
      "Yongil Kim",
      "Haeju Lee",
      "Jinsik Lee",
      "Kyungmin Lee",
      "Sangha Park",
      "Heuiyeen Yeen",
      "Hwan Chang",
      "Stanley Jungkyu Choi",
      "Yejin Choi",
      "Jiwon Ham",
      "Kijeong Jeon",
      "Geunyeong Jeong",
      "Gerrard Jeongwon Jo",
      "Yonghwan Jo",
      "Jiyeon Jung",
      "Naeun Kang",
      "Dohoon Kim",
      "Euisoon Kim",
      "Hayeon Kim",
      "Hyosang Kim",
      "Hyunseo Kim",
      "Jieun Kim",
      "Minu Kim",
      "Myoungshin Kim",
      "Unsol Kim",
      "Youchul Kim",
      "YoungJin Kim",
      "Chaeeun Lee",
      "Chaeyoon Lee",
      "Changhun Lee",
      "Dahm Lee",
      "Edward Hwayoung Lee",
      "Honglak Lee",
      "Jinsang Lee",
      "Jiyoung Lee",
      "Sangeun Lee",
      "Seungwon Lim",
      "Solji Lim",
      "Woohyung Lim",
      "Chanwoo Moon",
      "Jaewoo Park",
      "Jinho Park",
      "Yongmin Park",
      "Hyerin Seo",
      "Wooseok Seo",
      "Yongwoo Song",
      "Sejong Yang",
      "Sihoon Yang",
      "Chang En Yea",
      "Sihyuk Yi",
      "Chansik Yoon",
      "Dongkeun Yoon",
      "Sangyeon Yoon",
      "Hyeongu Yun"
    ],
    "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
    "published": "Jan 05",
    "pdf_url": "https://arxiv.org/pdf/2601.01739v2",
    "arxiv_url": "http://arxiv.org/abs/2601.01739v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "authors": [
      "Lo\u00efc Magne",
      "Anas Awadalla",
      "Guanzhi Wang",
      "Yinzhen Xu",
      "Joshua Belofsky",
      "Fengyuan Hu",
      "Joohwan Kim",
      "Ludwig Schmidt",
      "Georgia Gkioxari",
      "Jan Kautz",
      "Yisong Yue",
      "Yejin Choi",
      "Yuke Zhu",
      "Linxi \"Jim\" Fan"
    ],
    "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
    "published": "Jan 04",
    "pdf_url": "https://arxiv.org/pdf/2601.02427v1",
    "arxiv_url": "http://arxiv.org/abs/2601.02427v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt",
      "Yejin Choi"
    ]
  },
  {
    "title": "Efficiently Estimating Data Efficiency for Language Model Fine-tuning",
    "authors": [
      "Gyung Hyun Je",
      "Colin Raffel"
    ],
    "summary": "While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.",
    "published": "Dec 31",
    "pdf_url": "https://arxiv.org/pdf/2512.24991v1",
    "arxiv_url": "http://arxiv.org/abs/2512.24991v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "End-to-End Test-Time Training for Long Context",
    "authors": [
      "Arnuv Tandon",
      "Karan Dalal",
      "Xinhao Li",
      "Daniel Koceja",
      "Marcel R\u00f8d",
      "Sam Buchanan",
      "Xiaolong Wang",
      "Jure Leskovec",
      "Sanmi Koyejo",
      "Tatsunori Hashimoto",
      "Carlos Guestrin",
      "Jed McCaleb",
      "Yejin Choi",
      "Yu Sun"
    ],
    "summary": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.",
    "published": "Dec 29",
    "pdf_url": "https://arxiv.org/pdf/2512.23675v2",
    "arxiv_url": "http://arxiv.org/abs/2512.23675v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo",
      "Tatsunori Hashimoto",
      "Yejin Choi"
    ]
  },
  {
    "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
    "authors": [
      "Phil Cuvin",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "summary": "Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.",
    "published": "Dec 28",
    "pdf_url": "https://arxiv.org/pdf/2512.22894v2",
    "arxiv_url": "http://arxiv.org/abs/2512.22894v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "An Information Theoretic Perspective on Agentic System Design",
    "authors": [
      "Shizhe He",
      "Avanika Narayan",
      "Ishan S. Khare",
      "Scott W. Linderman",
      "Christopher R\u00e9",
      "Dan Biderman"
    ],
    "summary": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\\times$ more accurate, $4.6\\times$ more concise, and conveys $5.5\\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective tha...",
    "published": "Dec 25",
    "pdf_url": "https://arxiv.org/pdf/2512.21720v1",
    "arxiv_url": "http://arxiv.org/abs/2512.21720v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "authors": [
      "NVIDIA",
      ":",
      "Aaron Blakeman",
      "Aaron Grattafiori",
      "Aarti Basant",
      "Abhibha Gupta",
      "Abhinav Khattar",
      "Adi Renduchintala",
      "Aditya Vavre",
      "Akanksha Shukla",
      "Akhiad Bercovich",
      "Aleksander Ficek",
      "Aleksandr Shaposhnikov",
      "Alex Kondratenko",
      "Alexander Bukharin",
      "Alexandre Milesi",
      "Ali Taghibakhshi",
      "Alisa Liu",
      "Amelia Barton",
      "Ameya Sunil Mahabaleshwarkar",
      "Amir Klein",
      "Amit Zuker",
      "Amnon Geifman",
      "Amy Shen",
      "Anahita Bhiwandiwalla",
      "Andrew Tao",
      "Anjulie Agrusa",
      "Ankur Verma",
      "Ann Guan",
      "Anubhav Mandarwal",
      "Arham Mehta",
      "Ashwath Aithal",
      "Ashwin Poojary",
      "Asif Ahamed",
      "Asit Mishra",
      "Asma Kuriparambil Thekkumpate",
      "Ayush Dattagupta",
      "Banghua Zhu",
      "Bardiya Sadeghi",
      "Barnaby Simkin",
      "Ben Lanir",
      "Benedikt Schifferer",
      "Besmira Nushi",
      "Bilal Kartal",
      "Bita Darvish Rouhani",
      "Boris Ginsburg",
      "Brandon Norick",
      "Brandon Soubasis",
      "Branislav Kisacanin",
      "Brian Yu",
      "Bryan Catanzaro",
      "Carlo del Mundo",
      "Chantal Hwang",
      "Charles Wang",
      "Cheng-Ping Hsieh",
      "Chenghao Zhang",
      "Chenhan Yu",
      "Chetan Mungekar",
      "Chintan Patel",
      "Chris Alexiuk",
      "Christopher Parisien",
      "Collin Neale",
      "Cyril Meurillon",
      "Damon Mosk-Aoyama",
      "Dan Su",
      "Dane Corneil",
      "Daniel Afrimi",
      "Daniel Lo",
      "Daniel Rohrer",
      "Daniel Serebrenik",
      "Daria Gitman",
      "Daria Levy",
      "Darko Stosic",
      "David Mosallanezhad",
      "Deepak Narayanan",
      "Dhruv Nathawani",
      "Dima Rekesh",
      "Dina Yared",
      "Divyanshu Kakwani",
      "Dong Ahn",
      "Duncan Riach",
      "Dusan Stosic",
      "Edgar Minasyan",
      "Edward Lin",
      "Eileen Long",
      "Eileen Peters Long",
      "Elad Segal",
      "Elena Lantz",
      "Ellie Evans",
      "Elliott Ning",
      "Eric Chung",
      "Eric Harper",
      "Eric Tramel",
      "Erick Galinkin",
      "Erik Pounds",
      "Evan Briones",
      "Evelina Bakhturina",
      "Evgeny Tsykunov",
      "Faisal Ladhak",
      "Fay Wang",
      "Fei Jia",
      "Felipe Soares",
      "Feng Chen",
      "Ferenc Galko",
      "Frank Sun",
      "Frankie Siino",
      "Gal Hubara Agam",
      "Ganesh Ajjanagadde",
      "Gantavya Bhatt",
      "Gargi Prasad",
      "George Armstrong",
      "Gerald Shen",
      "Gorkem Batmaz",
      "Grigor Nalbandyan",
      "Haifeng Qian",
      "Harsh Sharma",
      "Hayley Ross",
      "Helen Ngo",
      "Herbert Hum",
      "Herman Sahota",
      "Hexin Wang",
      "Himanshu Soni",
      "Hiren Upadhyay",
      "Huizi Mao",
      "Huy C Nguyen",
      "Huy Q Nguyen",
      "Iain Cunningham",
      "Ido Galil",
      "Ido Shahaf",
      "Igor Gitman",
      "Ilya Loshchilov",
      "Itamar Schen",
      "Itay Levy",
      "Ivan Moshkov",
      "Izik Golan",
      "Izzy Putterman",
      "Jan Kautz",
      "Jane Polak Scowcroft",
      "Jared Casper",
      "Jatin Mitra",
      "Jeffrey Glick",
      "Jenny Chen",
      "Jesse Oliver",
      "Jian Zhang",
      "Jiaqi Zeng",
      "Jie Lou",
      "Jimmy Zhang",
      "Jinhang Choi",
      "Jining Huang",
      "Joey Conway",
      "Joey Guman",
      "John Kamalu",
      "Johnny Greco",
      "Jonathan Cohen",
      "Joseph Jennings",
      "Joyjit Daw",
      "Julien Veron Vialard",
      "Junkeun Yi",
      "Jupinder Parmar",
      "Kai Xu",
      "Kan Zhu",
      "Kari Briski",
      "Katherine Cheung",
      "Katherine Luna",
      "Keith Wyss",
      "Keshav Santhanam",
      "Kevin Shih",
      "Kezhi Kong",
      "Khushi Bhardwaj",
      "Kirthi Shankar",
      "Krishna C. Puvvada",
      "Krzysztof Pawelec",
      "Kumar Anik",
      "Lawrence McAfee",
      "Laya Sleiman",
      "Leon Derczynski",
      "Li Ding",
      "Lizzie Wei",
      "Lucas Liebenwein",
      "Luis Vega",
      "Maanu Grover",
      "Maarten Van Segbroeck",
      "Maer Rodrigues de Melo",
      "Mahdi Nazemi",
      "Makesh Narsimhan Sreedhar",
      "Manoj Kilaru",
      "Maor Ashkenazi",
      "Marc Romeijn",
      "Marcin Chochowski",
      "Mark Cai",
      "Markus Kliegl",
      "Maryam Moosaei",
      "Matt Kulka",
      "Matvei Novikov",
      "Mehrzad Samadi",
      "Melissa Corpuz",
      "Mengru Wang",
      "Meredith Price",
      "Michael Andersch",
      "Michael Boone",
      "Michael Evans",
      "Miguel Martinez",
      "Mikail Khona",
      "Mike Chrzanowski",
      "Minseok Lee",
      "Mohammad Dabbah",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Nabin Mulepati",
      "Najeeb Nabwani",
      "Natalie Hereth",
      "Nave Assaf",
      "Negar Habibi",
      "Neta Zmora",
      "Netanel Haber",
      "Nicola Sessions",
      "Nidhi Bhatia",
      "Nikhil Jukar",
      "Nikki Pope",
      "Nikolai Ludwig",
      "Nima Tajbakhsh",
      "Nir Ailon",
      "Nirmal Juluru",
      "Nishant Sharma",
      "Oleksii Hrinchuk",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Oluwatobi Olabiyi",
      "Omer Ullman Argov",
      "Omri Puny",
      "Oren Tropp",
      "Ouye Xie",
      "Parth Chadha",
      "Pasha Shamis",
      "Paul Gibbons",
      "Pavlo Molchanov",
      "Pawel Morkisz",
      "Peter Dykas",
      "Peter Jin",
      "Pinky Xu",
      "Piotr Januszewski",
      "Pranav Prashant Thombre",
      "Prasoon Varshney",
      "Pritam Gundecha",
      "Przemek Tredak",
      "Qing Miao",
      "Qiyu Wan",
      "Rabeeh Karimi Mahabadi",
      "Rachit Garg",
      "Ran El-Yaniv",
      "Ran Zilberstein",
      "Rasoul Shafipour",
      "Rich Harang",
      "Rick Izzo",
      "Rima Shahbazyan",
      "Rishabh Garg",
      "Ritika Borkar",
      "Ritu Gala",
      "Riyad Islam",
      "Robert Hesse",
      "Roger Waleffe",
      "Rohit Watve",
      "Roi Koren",
      "Ruoxi Zhang",
      "Russell Hewett",
      "Russell J. Hewett",
      "Ryan Prenger",
      "Ryan Timbrook",
      "Sadegh Mahdavi",
      "Sahil Modi",
      "Samuel Kriman",
      "Sangkug Lim",
      "Sanjay Kariyappa",
      "Sanjeev Satheesh",
      "Saori Kaji",
      "Satish Pasumarthi",
      "Saurav Muralidharan",
      "Sean Narentharen",
      "Sean Narenthiran",
      "Seonmyeong Bak",
      "Sergey Kashirsky",
      "Seth Poulos",
      "Shahar Mor",
      "Shanmugam Ramasamy",
      "Shantanu Acharya",
      "Shaona Ghosh",
      "Sharath Turuvekere Sreenivas",
      "Shelby Thomas",
      "Shiqing Fan",
      "Shreya Gopal",
      "Shrimai Prabhumoye",
      "Shubham Pachori",
      "Shubham Toshniwal",
      "Shuoyang Ding",
      "Siddharth Singh",
      "Simeng Sun",
      "Smita Ithape",
      "Somshubra Majumdar",
      "Soumye Singhal",
      "Stas Sergienko",
      "Stefania Alborghetti",
      "Stephen Ge",
      "Sugam Dipak Devare",
      "Sumeet Kumar Barua",
      "Suseella Panguluri",
      "Suyog Gupta",
      "Sweta Priyadarshi",
      "Syeda Nahida Akter",
      "Tan Bui",
      "Teodor-Dumitru Ene",
      "Terry Kong",
      "Thanh Do",
      "Tijmen Blankevoort",
      "Tim Moon",
      "Tom Balough",
      "Tomer Asida",
      "Tomer Bar Natan",
      "Tomer Ronen",
      "Tugrul Konuk",
      "Twinkle Vashishth",
      "Udi Karpas",
      "Ushnish De",
      "Vahid Noorozi",
      "Vahid Noroozi",
      "Venkat Srinivasan",
      "Venmugil Elango",
      "Victor Cui",
      "Vijay Korthikanti",
      "Vinay Rao",
      "Vitaly Kurin",
      "Vitaly Lavrukhin",
      "Vladimir Anisimov",
      "Wanli Jiang",
      "Wasi Uddin Ahmad",
      "Wei Du",
      "Wei Ping",
      "Wenfei Zhou",
      "Will Jennings",
      "William Zhang",
      "Wojciech Prazuch",
      "Xiaowei Ren",
      "Yashaswi Karnati",
      "Yejin Choi",
      "Yev Meyer",
      "Yi-Fu Wu",
      "Yian Zhang",
      "Yigong Qin",
      "Ying Lin",
      "Yonatan Geifman",
      "Yonggan Fu",
      "Yoshi Subara",
      "Yoshi Suhara",
      "Yubo Gao",
      "Zach Moshe",
      "Zhen Dong",
      "Zhongbo Zhu",
      "Zihan Liu",
      "Zijia Chen",
      "Zijie Yan"
    ],
    "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
    "published": "Dec 24",
    "pdf_url": "https://arxiv.org/pdf/2512.20856v1",
    "arxiv_url": "http://arxiv.org/abs/2512.20856v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
    "authors": [
      "NVIDIA",
      ":",
      "Aaron Blakeman",
      "Aaron Grattafiori",
      "Aarti Basant",
      "Abhibha Gupta",
      "Abhinav Khattar",
      "Adi Renduchintala",
      "Aditya Vavre",
      "Akanksha Shukla",
      "Akhiad Bercovich",
      "Aleksander Ficek",
      "Aleksandr Shaposhnikov",
      "Alex Kondratenko",
      "Alexander Bukharin",
      "Alexandre Milesi",
      "Ali Taghibakhshi",
      "Alisa Liu",
      "Amelia Barton",
      "Ameya Sunil Mahabaleshwarkar",
      "Amir Klein",
      "Amit Zuker",
      "Amnon Geifman",
      "Amy Shen",
      "Anahita Bhiwandiwalla",
      "Andrew Tao",
      "Ann Guan",
      "Anubhav Mandarwal",
      "Arham Mehta",
      "Ashwath Aithal",
      "Ashwin Poojary",
      "Asif Ahamed",
      "Asma Kuriparambil Thekkumpate",
      "Ayush Dattagupta",
      "Banghua Zhu",
      "Bardiya Sadeghi",
      "Barnaby Simkin",
      "Ben Lanir",
      "Benedikt Schifferer",
      "Besmira Nushi",
      "Bilal Kartal",
      "Bita Darvish Rouhani",
      "Boris Ginsburg",
      "Brandon Norick",
      "Brandon Soubasis",
      "Branislav Kisacanin",
      "Brian Yu",
      "Bryan Catanzaro",
      "Carlo del Mundo",
      "Chantal Hwang",
      "Charles Wang",
      "Cheng-Ping Hsieh",
      "Chenghao Zhang",
      "Chenhan Yu",
      "Chetan Mungekar",
      "Chintan Patel",
      "Chris Alexiuk",
      "Christopher Parisien",
      "Collin Neale",
      "Damon Mosk-Aoyama",
      "Dan Su",
      "Dane Corneil",
      "Daniel Afrimi",
      "Daniel Rohrer",
      "Daniel Serebrenik",
      "Daria Gitman",
      "Daria Levy",
      "Darko Stosic",
      "David Mosallanezhad",
      "Deepak Narayanan",
      "Dhruv Nathawani",
      "Dima Rekesh",
      "Dina Yared",
      "Divyanshu Kakwani",
      "Dong Ahn",
      "Duncan Riach",
      "Dusan Stosic",
      "Edgar Minasyan",
      "Edward Lin",
      "Eileen Long",
      "Eileen Peters Long",
      "Elena Lantz",
      "Ellie Evans",
      "Elliott Ning",
      "Eric Chung",
      "Eric Harper",
      "Eric Tramel",
      "Erick Galinkin",
      "Erik Pounds",
      "Evan Briones",
      "Evelina Bakhturina",
      "Faisal Ladhak",
      "Fay Wang",
      "Fei Jia",
      "Felipe Soares",
      "Feng Chen",
      "Ferenc Galko",
      "Frankie Siino",
      "Gal Hubara Agam",
      "Ganesh Ajjanagadde",
      "Gantavya Bhatt",
      "Gargi Prasad",
      "George Armstrong",
      "Gerald Shen",
      "Gorkem Batmaz",
      "Grigor Nalbandyan",
      "Haifeng Qian",
      "Harsh Sharma",
      "Hayley Ross",
      "Helen Ngo",
      "Herman Sahota",
      "Hexin Wang",
      "Himanshu Soni",
      "Hiren Upadhyay",
      "Huizi Mao",
      "Huy C Nguyen",
      "Huy Q Nguyen",
      "Iain Cunningham",
      "Ido Shahaf",
      "Igor Gitman",
      "Ilya Loshchilov",
      "Ivan Moshkov",
      "Izzy Putterman",
      "Jan Kautz",
      "Jane Polak Scowcroft",
      "Jared Casper",
      "Jatin Mitra",
      "Jeffrey Glick",
      "Jenny Chen",
      "Jesse Oliver",
      "Jian Zhang",
      "Jiaqi Zeng",
      "Jie Lou",
      "Jimmy Zhang",
      "Jining Huang",
      "Joey Conway",
      "Joey Guman",
      "John Kamalu",
      "Johnny Greco",
      "Jonathan Cohen",
      "Joseph Jennings",
      "Joyjit Daw",
      "Julien Veron Vialard",
      "Junkeun Yi",
      "Jupinder Parmar",
      "Kai Xu",
      "Kan Zhu",
      "Kari Briski",
      "Katherine Cheung",
      "Katherine Luna",
      "Keshav Santhanam",
      "Kevin Shih",
      "Kezhi Kong",
      "Khushi Bhardwaj",
      "Krishna C. Puvvada",
      "Krzysztof Pawelec",
      "Kumar Anik",
      "Lawrence McAfee",
      "Laya Sleiman",
      "Leon Derczynski",
      "Li Ding",
      "Lucas Liebenwein",
      "Luis Vega",
      "Maanu Grover",
      "Maarten Van Segbroeck",
      "Maer Rodrigues de Melo",
      "Makesh Narsimhan Sreedhar",
      "Manoj Kilaru",
      "Maor Ashkenazi",
      "Marc Romeijn",
      "Mark Cai",
      "Markus Kliegl",
      "Maryam Moosaei",
      "Matvei Novikov",
      "Mehrzad Samadi",
      "Melissa Corpuz",
      "Mengru Wang",
      "Meredith Price",
      "Michael Boone",
      "Michael Evans",
      "Miguel Martinez",
      "Mike Chrzanowski",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Nabin Mulepati",
      "Natalie Hereth",
      "Nave Assaf",
      "Negar Habibi",
      "Neta Zmora",
      "Netanel Haber",
      "Nicola Sessions",
      "Nidhi Bhatia",
      "Nikhil Jukar",
      "Nikki Pope",
      "Nikolai Ludwig",
      "Nima Tajbakhsh",
      "Nirmal Juluru",
      "Oleksii Hrinchuk",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Oluwatobi Olabiyi",
      "Omer Ullman Argov",
      "Ouye Xie",
      "Parth Chadha",
      "Pasha Shamis",
      "Pavlo Molchanov",
      "Pawel Morkisz",
      "Peter Dykas",
      "Peter Jin",
      "Pinky Xu",
      "Piotr Januszewski",
      "Pranav Prashant Thombre",
      "Prasoon Varshney",
      "Pritam Gundecha",
      "Qing Miao",
      "Rabeeh Karimi Mahabadi",
      "Ran El-Yaniv",
      "Ran Zilberstein",
      "Rasoul Shafipour",
      "Rich Harang",
      "Rick Izzo",
      "Rima Shahbazyan",
      "Rishabh Garg",
      "Ritika Borkar",
      "Ritu Gala",
      "Riyad Islam",
      "Roger Waleffe",
      "Rohit Watve",
      "Roi Koren",
      "Ruoxi Zhang",
      "Russell J. Hewett",
      "Ryan Prenger",
      "Ryan Timbrook",
      "Sadegh Mahdavi",
      "Sahil Modi",
      "Samuel Kriman",
      "Sanjay Kariyappa",
      "Sanjeev Satheesh",
      "Saori Kaji",
      "Satish Pasumarthi",
      "Sean Narentharen",
      "Sean Narenthiran",
      "Seonmyeong Bak",
      "Sergey Kashirsky",
      "Seth Poulos",
      "Shahar Mor",
      "Shanmugam Ramasamy",
      "Shantanu Acharya",
      "Shaona Ghosh",
      "Sharath Turuvekere Sreenivas",
      "Shelby Thomas",
      "Shiqing Fan",
      "Shreya Gopal",
      "Shrimai Prabhumoye",
      "Shubham Pachori",
      "Shubham Toshniwal",
      "Shuoyang Ding",
      "Siddharth Singh",
      "Simeng Sun",
      "Smita Ithape",
      "Somshubra Majumdar",
      "Soumye Singhal",
      "Stefania Alborghetti",
      "Stephen Ge",
      "Sugam Dipak Devare",
      "Sumeet Kumar Barua",
      "Suseella Panguluri",
      "Suyog Gupta",
      "Sweta Priyadarshi",
      "Syeda Nahida Akter",
      "Tan Bui",
      "Teodor-Dumitru Ene",
      "Terry Kong",
      "Thanh Do",
      "Tijmen Blankevoort",
      "Tom Balough",
      "Tomer Asida",
      "Tomer Bar Natan",
      "Tugrul Konuk",
      "Twinkle Vashishth",
      "Udi Karpas",
      "Ushnish De",
      "Vahid Noorozi",
      "Vahid Noroozi",
      "Venkat Srinivasan",
      "Venmugil Elango",
      "Vijay Korthikanti",
      "Vitaly Kurin",
      "Vitaly Lavrukhin",
      "Wanli Jiang",
      "Wasi Uddin Ahmad",
      "Wei Du",
      "Wei Ping",
      "Wenfei Zhou",
      "Will Jennings",
      "William Zhang",
      "Wojciech Prazuch",
      "Xiaowei Ren",
      "Yashaswi Karnati",
      "Yejin Choi",
      "Yev Meyer",
      "Yi-Fu Wu",
      "Yian Zhang",
      "Ying Lin",
      "Yonatan Geifman",
      "Yonggan Fu",
      "Yoshi Subara",
      "Yoshi Suhara",
      "Yubo Gao",
      "Zach Moshe",
      "Zhen Dong",
      "Zihan Liu",
      "Zijia Chen",
      "Zijie Yan"
    ],
    "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
    "published": "Dec 23",
    "pdf_url": "https://arxiv.org/pdf/2512.20848v1",
    "arxiv_url": "http://arxiv.org/abs/2512.20848v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
    "authors": [
      "G\u00fcl Sena Alt\u0131nta\u015f",
      "Malikeh Ehghaghi",
      "Brian Lester",
      "Fengyuan Liu",
      "Wanru Zhao",
      "Marco Ciccone",
      "Colin Raffel"
    ],
    "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.",
    "published": "Dec 23",
    "pdf_url": "https://arxiv.org/pdf/2512.20757v1",
    "arxiv_url": "http://arxiv.org/abs/2512.20757v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
    "authors": [
      "Yanhong Li",
      "Songlin Yang",
      "Shawn Tan",
      "Mayank Mishra",
      "Rameswar Panda",
      "Jiawei Zhou",
      "Yoon Kim"
    ],
    "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",
    "published": "Dec 23",
    "pdf_url": "https://arxiv.org/pdf/2512.20569v1",
    "arxiv_url": "http://arxiv.org/abs/2512.20569v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Why Slop Matters",
    "authors": [
      "Cody Kommers",
      "Eamon Duede",
      "Julia Gordon",
      "Ari Holtzman",
      "Tess McNulty",
      "Spencer Stewart",
      "Lindsay Thomas",
      "Richard Jean So",
      "Hoyt Long"
    ],
    "summary": "AI-generated \"slop\" is often seen as digital pollution. We argue that this dismissal of the topic risks missing important aspects of AI Slop that deserve rigorous study. AI Slop serves a social function: it offers a supply-side solution to a variety of problems in cultural and economic demand - that, collectively, people want more content than humans can supply. We also argue that AI Slop is not mere digital detritus but has its own aesthetic value. Like other \"low\" cultural forms initially dismissed by critics, it nonetheless offers a legitimate means of collective sense-making, with the potential to express meaning and identity. We identify three key features of family resemblance for prototypical AI Slop: superficial competence (its veneer of quality is belied by a deeper lack of substance), asymmetry effort (it takes vastly less effort to generate than would be the case without AI), and mass producibility (it is part of a digital ecosystem of widespread generation and consumption). While AI Slop is heterogeneous and depends crucially on its medium, it tends to vary across three dimensions: instrumental utility, personalization, and surrealism. AI Slop will be an increasingly prolific and impactful part of our creative, information, and cultural economies; we should take it seriously as an object of study in its own right.",
    "published": "Dec 23",
    "pdf_url": "https://arxiv.org/pdf/2601.06060v1",
    "arxiv_url": "http://arxiv.org/abs/2601.06060v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Propose, Solve, Verify: Self-Play Through Formal Verification",
    "authors": [
      "Alex Wilf",
      "Pranjal Aggarwal",
      "Bryan Parno",
      "Daniel Fried",
      "Louis-Philippe Morency",
      "Paul Pu Liang",
      "Sean Welleck"
    ],
    "summary": "Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.",
    "published": "Dec 20",
    "pdf_url": "https://arxiv.org/pdf/2512.18160v1",
    "arxiv_url": "http://arxiv.org/abs/2512.18160v1",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators",
    "authors": [
      "Michael J. Ryan",
      "Yanzhe Zhang",
      "Amol Salunkhe",
      "Yi Chu",
      "Di Xu",
      "Diyi Yang"
    ],
    "summary": "Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.",
    "published": "Dec 19",
    "pdf_url": "https://arxiv.org/pdf/2512.17267v1",
    "arxiv_url": "http://arxiv.org/abs/2512.17267v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "authors": [
      "Yushi Hu",
      "Reyhane Askari-Hemmat",
      "Melissa Hall",
      "Emily Dinan",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
    "published": "Dec 18",
    "pdf_url": "https://arxiv.org/pdf/2512.16899v3",
    "arxiv_url": "http://arxiv.org/abs/2512.16899v3",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
    "authors": [
      "Amita Kamath",
      "Kai-Wei Chang",
      "Ranjay Krishna",
      "Luke Zettlemoyer",
      "Yushi Hu",
      "Marjan Ghazvininejad"
    ],
    "summary": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
    "published": "Dec 18",
    "pdf_url": "https://arxiv.org/pdf/2512.16853v1",
    "arxiv_url": "http://arxiv.org/abs/2512.16853v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Adaptation of Agentic AI",
    "authors": [
      "Pengcheng Jiang",
      "Jiacheng Lin",
      "Zhiyi Shi",
      "Zifeng Wang",
      "Luxi He",
      "Yichen Wu",
      "Ming Zhong",
      "Peiyang Song",
      "Qizheng Zhang",
      "Heng Wang",
      "Xueqiang Xu",
      "Hanwen Xu",
      "Pengrui Han",
      "Dylan Zhang",
      "Jiashuo Sun",
      "Chaoqi Yang",
      "Kun Qian",
      "Tian Wang",
      "Changran Hu",
      "Manling Li",
      "Quanzheng Li",
      "Hao Peng",
      "Sheng Wang",
      "Jingbo Shang",
      "Chao Zhang",
      "Jiaxuan You",
      "Liyuan Liu",
      "Pan Lu",
      "Yu Zhang",
      "Heng Ji",
      "Yejin Choi",
      "Dawn Song",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
    "published": "Dec 18",
    "pdf_url": "https://arxiv.org/pdf/2512.16301v2",
    "arxiv_url": "http://arxiv.org/abs/2512.16301v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception",
    "authors": [
      "Joel Mire",
      "Maria Antoniak",
      "Steven R. Wilson",
      "Zexin Ma",
      "Achyutarama R. Ganti",
      "Andrew Piper",
      "Maarten Sap"
    ],
    "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.",
    "published": "Dec 17",
    "pdf_url": "https://arxiv.org/pdf/2512.15925v1",
    "arxiv_url": "http://arxiv.org/abs/2512.15925v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "authors": [
      "Qiuyang Mang",
      "Wenhao Chai",
      "Zhifei Li",
      "Huanzhi Mao",
      "Shang Zhou",
      "Alexander Du",
      "Hanchen Li",
      "Shu Liu",
      "Edwin Chen",
      "Yichuan Wang",
      "Xieting Chu",
      "Zerui Cheng",
      "Yuan Xu",
      "Tian Xia",
      "Zirui Wang",
      "Tianneng Shi",
      "Jianzhu Yao",
      "Yilong Zhao",
      "Qizheng Zhang",
      "Charlie Ruan",
      "Zeyu Shen",
      "Kaiyuan Liu",
      "Runyuan He",
      "Dong Xing",
      "Zerui Li",
      "Zirong Zeng",
      "Yige Jiang",
      "Lufeng Cheng",
      "Ziyi Zhao",
      "Youran Sun",
      "Wesley Zheng",
      "Meiyuwang Zhang",
      "Ruyi Ji",
      "Xuechang Tu",
      "Zihan Zheng",
      "Zexing Chen",
      "Kangyang Zhou",
      "Zhaozi Wang",
      "Jingbang Chen",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Pramod Viswanath",
      "Vijay Ganesh",
      "Saining Xie",
      "Zhuang Liu",
      "Dawn Song",
      "Sewon Min",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jingbo Shang",
      "Alvin Cheung"
    ],
    "summary": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "published": "Dec 17",
    "pdf_url": "https://arxiv.org/pdf/2512.15699v1",
    "arxiv_url": "http://arxiv.org/abs/2512.15699v1",
    "queried_author": "Peter Henderson",
    "matching_authors": [
      "Peter Henderson"
    ]
  },
  {
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "authors": [
      "Benjamin Minixhofer",
      "Tyler Murray",
      "Tomasz Limisiewicz",
      "Anna Korhonen",
      "Luke Zettlemoyer",
      "Noah A. Smith",
      "Edoardo M. Ponti",
      "Luca Soldaini",
      "Valentin Hofmann"
    ],
    "summary": "Recent advances in generative AI have been largely driven by large language models (LLMs), deep neural networks that operate over discrete units called tokens. To represent text, the vast majority of LLMs use words or word fragments as the tokens, known as subword tokenization. Subword tokenization obscures fine-grained information, which is problematic, especially for scientific data - such as computer code or biological sequences - where meaning depends on the individual characters. Models that instead operate directly on the byte encoding of text avoid these limitations, but until now they have lagged behind subword-based models in performance. Here we introduce Bolmo, a family of fully open byte-level LLMs that approach the capabilities of subword-based systems. Using a two-stage conversion procedure, we transform existing subword-based models into byte-level models with minimal additional training. The resulting models outperform prior byte-level approaches and excel on character-level reasoning tasks, while remaining competitive across standard benchmarks. By efficiently processing byte-level information, these models achieve practical inference speeds and can be adapted at low cost using the existing ecosystem around the source LLM. Our results remove a long-standing performance barrier to end-to-end byte-level language modeling, demonstrating that models operating on raw text encodings can scale competitively while offering advantages in domains requiring fine-grained textual understanding.",
    "published": "Dec 17",
    "pdf_url": "https://arxiv.org/pdf/2512.15586v2",
    "arxiv_url": "http://arxiv.org/abs/2512.15586v2",
    "queried_author": "Luca Soldaini",
    "matching_authors": [
      "Luca Soldaini",
      "Luke Zettlemoyer",
      "Noah A. Smith",
      "Valentin Hofmann"
    ]
  },
  {
    "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "authors": [
      "Wentao Guo",
      "Mayank Mishra",
      "Xinle Cheng",
      "Ion Stoica",
      "Tri Dao"
    ],
    "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to v...",
    "published": "Dec 16",
    "pdf_url": "https://arxiv.org/pdf/2512.14080v1",
    "arxiv_url": "http://arxiv.org/abs/2512.14080v1",
    "queried_author": "Tri Dao",
    "matching_authors": [
      "Tri Dao"
    ]
  },
  {
    "title": "Olmo 3",
    "authors": [
      "Team Olmo",
      ":",
      "Allyson Ettinger",
      "Amanda Bertsch",
      "Bailey Kuehl",
      "David Graham",
      "David Heineman",
      "Dirk Groeneveld",
      "Faeze Brahman",
      "Finbarr Timbers",
      "Hamish Ivison",
      "Jacob Morrison",
      "Jake Poznanski",
      "Kyle Lo",
      "Luca Soldaini",
      "Matt Jordan",
      "Mayee Chen",
      "Michael Noukhovitch",
      "Nathan Lambert",
      "Pete Walsh",
      "Pradeep Dasigi",
      "Robert Berry",
      "Saumya Malik",
      "Saurabh Shah",
      "Scott Geng",
      "Shane Arora",
      "Shashank Gupta",
      "Taira Anderson",
      "Teng Xiao",
      "Tyler Murray",
      "Tyler Romero",
      "Victoria Graf",
      "Akari Asai",
      "Akshita Bhagia",
      "Alexander Wettig",
      "Alisa Liu",
      "Aman Rangapur",
      "Chloe Anastasiades",
      "Costa Huang",
      "Dustin Schwenk",
      "Harsh Trivedi",
      "Ian Magnusson",
      "Jaron Lochner",
      "Jiacheng Liu",
      "Lester James V. Miranda",
      "Maarten Sap",
      "Malia Morgan",
      "Michael Schmitz",
      "Michal Guerquin",
      "Michael Wilson",
      "Regan Huff",
      "Ronan Le Bras",
      "Rui Xin",
      "Rulin Shao",
      "Sam Skjonsberg",
      "Shannon Zejiang Shen",
      "Shuyue Stella Li",
      "Tucker Wilde",
      "Valentina Pyatkin",
      "Will Merrill",
      "Yapei Chang",
      "Yuling Gu",
      "Zhiyuan Zeng",
      "Ashish Sabharwal",
      "Luke Zettlemoyer",
      "Pang Wei Koh",
      "Ali Farhadi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi"
    ],
    "summary": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
    "published": "Dec 15",
    "pdf_url": "https://arxiv.org/pdf/2512.13961v1",
    "arxiv_url": "http://arxiv.org/abs/2512.13961v1",
    "queried_author": "Akari Asai",
    "matching_authors": [
      "Akari Asai",
      "Alexander Wettig",
      "Hamish Ivison",
      "Hannaneh Hajishirzi",
      "Ian Magnusson",
      "Kyle Lo",
      "Luca Soldaini",
      "Luke Zettlemoyer",
      "Maarten Sap",
      "Noah A. Smith",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Training Versatile Coding Agents in Synthetic Environments",
    "authors": [
      "Yiqi Zhu",
      "Apurva Gandhi",
      "Graham Neubig"
    ],
    "summary": "Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.",
    "published": "Dec 13",
    "pdf_url": "https://arxiv.org/pdf/2512.12216v2",
    "arxiv_url": "http://arxiv.org/abs/2512.12216v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "The 2025 Foundation Model Transparency Index",
    "authors": [
      "Alexander Wan",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Shayne Longpre",
      "Betty Xiong",
      "Percy Liang",
      "Rishi Bommasani"
    ],
    "summary": "Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.",
    "published": "Dec 11",
    "pdf_url": "https://arxiv.org/pdf/2512.10169v1",
    "arxiv_url": "http://arxiv.org/abs/2512.10169v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
    "authors": [
      "Nick Jiang",
      "Xiaoqing Sun",
      "Lisa Dunlap",
      "Lewis Smith",
      "Neel Nanda"
    ],
    "summary": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of ...",
    "published": "Dec 10",
    "pdf_url": "https://arxiv.org/pdf/2512.10092v1",
    "arxiv_url": "http://arxiv.org/abs/2512.10092v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
    "authors": [
      "Arjun Parthasarathy",
      "Nimit Kalra",
      "Rohun Agrawal",
      "Yann LeCun",
      "Oumayma Bounou",
      "Pavel Izmailov",
      "Micah Goldblum"
    ],
    "summary": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
    "published": "Dec 10",
    "pdf_url": "https://arxiv.org/pdf/2512.09929v1",
    "arxiv_url": "http://arxiv.org/abs/2512.09929v1",
    "queried_author": "Pavel Izmailov",
    "matching_authors": [
      "Pavel Izmailov"
    ]
  },
  {
    "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
    "authors": [
      "Justin W. Lin",
      "Eliot Krzysztof Jones",
      "Donovan Julian Jasper",
      "Ethan Jun-shen Ho",
      "Anna Wu",
      "Arnold Tianyi Yang",
      "Neil Perry",
      "Andy Zou",
      "Matt Fredrikson",
      "J. Zico Kolter",
      "Percy Liang",
      "Dan Boneh",
      "Daniel E. Ho"
    ],
    "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
    "published": "Dec 10",
    "pdf_url": "https://arxiv.org/pdf/2512.09882v1",
    "arxiv_url": "http://arxiv.org/abs/2512.09882v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter",
      "Percy Liang"
    ]
  },
  {
    "title": "Language models as tools for investigating the distinction between possible and impossible natural languages",
    "authors": [
      "Julie Kallini",
      "Christopher Potts"
    ],
    "summary": "We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.",
    "published": "Dec 10",
    "pdf_url": "https://arxiv.org/pdf/2512.09394v1",
    "arxiv_url": "http://arxiv.org/abs/2512.09394v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing",
    "authors": [
      "Peter Baile Chen",
      "Weiyue Li",
      "Dan Roth",
      "Michael Cafarella",
      "Samuel Madden",
      "Jacob Andreas"
    ],
    "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.",
    "published": "Dec 10",
    "pdf_url": "https://arxiv.org/pdf/2512.09386v1",
    "arxiv_url": "http://arxiv.org/abs/2512.09386v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "authors": [
      "Charlie Zhang",
      "Graham Neubig",
      "Xiang Yue"
    ],
    "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards red...",
    "published": "Dec 08",
    "pdf_url": "https://arxiv.org/pdf/2512.07783v1",
    "arxiv_url": "http://arxiv.org/abs/2512.07783v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "ARC-AGI Without Pretraining",
    "authors": [
      "Isaac Liao",
      "Albert Gu"
    ],
    "summary": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI \"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.",
    "published": "Dec 05",
    "pdf_url": "https://arxiv.org/pdf/2512.06104v1",
    "arxiv_url": "http://arxiv.org/abs/2512.06104v1",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "authors": [
      "Xiaochuang Han",
      "Youssef Emad",
      "Melissa Hall",
      "John Nguyen",
      "Karthik Padthe",
      "Liam Robbins",
      "Amir Bar",
      "Delong Chen",
      "Michal Drozdzal",
      "Maha Elbayad",
      "Yushi Hu",
      "Shang-Wen Li",
      "Sreya Dutta Roy",
      "Jakob Verbeek",
      "XuDong Wang",
      "Marjan Ghazvininejad",
      "Luke Zettlemoyer",
      "Emily Dinan"
    ],
    "summary": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and...",
    "published": "Dec 04",
    "pdf_url": "https://arxiv.org/pdf/2512.05103v2",
    "arxiv_url": "http://arxiv.org/abs/2512.05103v2",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models",
    "authors": [
      "Siddharth Betala",
      "Samuel P. Gleason",
      "Ali Ramlaoui",
      "Andy Xu",
      "Georgia Channing",
      "Daniel Levy",
      "Cl\u00e9mentine Fourrier",
      "Nikita Kazeev",
      "Chaitanya K. Joshi",
      "S\u00e9kou-Oumar Kaba",
      "F\u00e9lix Therrien",
      "Alex Hernandez-Garcia",
      "Roc\u00edo Mercado",
      "N. M. Anoop Krishnan",
      "Alexandre Duval"
    ],
    "summary": "Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.",
    "published": "Dec 04",
    "pdf_url": "https://arxiv.org/pdf/2512.04562v2",
    "arxiv_url": "http://arxiv.org/abs/2512.04562v2",
    "queried_author": "Cl\u00e9mentine Fourrier",
    "matching_authors": [
      "Cl\u00e9mentine Fourrier"
    ]
  },
  {
    "title": "ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation",
    "authors": [
      "Yiming Xu",
      "Yuan Yuan",
      "Vijay Viswanathan",
      "Graham Neubig"
    ],
    "summary": "Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.",
    "published": "Dec 04",
    "pdf_url": "https://arxiv.org/pdf/2512.04350v1",
    "arxiv_url": "http://arxiv.org/abs/2512.04350v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "authors": [
      "Zayne Sprague",
      "Jack Lu",
      "Manya Wadhwa",
      "Sedrick Keh",
      "Mengye Ren",
      "Greg Durrett"
    ],
    "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
    "published": "Dec 03",
    "pdf_url": "https://arxiv.org/pdf/2512.04072v1",
    "arxiv_url": "http://arxiv.org/abs/2512.04072v1",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Learning Steerable Clarification Policies with Collaborative Self-play",
    "authors": [
      "Jonathan Berant",
      "Maximillian Chen",
      "Adam Fisch",
      "Reza Aghajani",
      "Fantine Huot",
      "Mirella Lapata",
      "Jacob Eisenstein"
    ],
    "summary": "To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.",
    "published": "Dec 03",
    "pdf_url": "https://arxiv.org/pdf/2512.04068v2",
    "arxiv_url": "http://arxiv.org/abs/2512.04068v2",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs",
    "authors": [
      "Isha Chaturvedi",
      "Anjana Nair",
      "Yushen Li",
      "Adhitya Rajendra Kumar",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Vasu Sharma"
    ],
    "summary": "We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attribution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of answers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.",
    "published": "Dec 03",
    "pdf_url": "https://arxiv.org/pdf/2512.08976v1",
    "arxiv_url": "http://arxiv.org/abs/2512.08976v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval",
    "authors": [
      "Constantin Venhoff",
      "Ashkan Khakzar",
      "Sonia Joseph",
      "Philip Torr",
      "Neel Nanda"
    ],
    "summary": "Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines h...",
    "published": "Dec 02",
    "pdf_url": "https://arxiv.org/pdf/2512.03276v1",
    "arxiv_url": "http://arxiv.org/abs/2512.03276v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Plantain: Plan-Answer Interleaved Reasoning",
    "authors": [
      "Anthony Liang",
      "Jonathan Berant",
      "Adam Fisch",
      "Abhimanyu Goyal",
      "Kalpesh Krishna",
      "Jacob Eisenstein"
    ],
    "summary": "Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard \"think-then-answer\" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over ...",
    "published": "Dec 02",
    "pdf_url": "https://arxiv.org/pdf/2512.03176v1",
    "arxiv_url": "http://arxiv.org/abs/2512.03176v1",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
    "authors": [
      "Zhonghao He",
      "Tianyi Qiu",
      "Hirokazu Shirado",
      "Maarten Sap"
    ],
    "summary": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale ...",
    "published": "Dec 02",
    "pdf_url": "https://arxiv.org/pdf/2512.02914v1",
    "arxiv_url": "http://arxiv.org/abs/2512.02914v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models",
    "authors": [
      "Xinyue Ai",
      "Yutong He",
      "Albert Gu",
      "Ruslan Salakhutdinov",
      "J Zico Kolter",
      "Nicholas Matthew Boffi",
      "Max Simchowitz"
    ],
    "summary": "Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow to outperform ...",
    "published": "Dec 02",
    "pdf_url": "https://arxiv.org/pdf/2512.02636v2",
    "arxiv_url": "http://arxiv.org/abs/2512.02636v2",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu",
      "J Zico Kolter"
    ]
  },
  {
    "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
    "authors": [
      "Zhengyang Geng",
      "Yiyang Lu",
      "Zongze Wu",
      "Eli Shechtman",
      "J. Zico Kolter",
      "Kaiming He"
    ],
    "summary": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
    "published": "Dec 01",
    "pdf_url": "https://arxiv.org/pdf/2512.02012v1",
    "arxiv_url": "http://arxiv.org/abs/2512.02012v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Difficulties with Evaluating a Deception Detector for AIs",
    "authors": [
      "Lewis Smith",
      "Bilal Chughtai",
      "Neel Nanda"
    ],
    "summary": "Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.",
    "published": "Nov 27",
    "pdf_url": "https://arxiv.org/pdf/2511.22662v2",
    "arxiv_url": "http://arxiv.org/abs/2511.22662v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
    "authors": [
      "Young-Jun Lee",
      "Seungone Kim",
      "Byung-Kwan Lee",
      "Minkyeong Moon",
      "Yechan Hwang",
      "Jong Myoung Kim",
      "Graham Neubig",
      "Sean Welleck",
      "Ho-Jin Choi"
    ],
    "summary": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
    "published": "Nov 27",
    "pdf_url": "https://arxiv.org/pdf/2511.22173v1",
    "arxiv_url": "http://arxiv.org/abs/2511.22173v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig",
      "Sean Welleck",
      "Seungone Kim"
    ]
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "authors": [
      "Hongjin Su",
      "Shizhe Diao",
      "Ximing Lu",
      "Mingjie Liu",
      "Jiacheng Xu",
      "Xin Dong",
      "Yonggan Fu",
      "Peter Belcak",
      "Hanrong Ye",
      "Hongxu Yin",
      "Yi Dong",
      "Evelina Bakhturina",
      "Tao Yu",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "published": "Nov 26",
    "pdf_url": "https://arxiv.org/pdf/2511.21689v1",
    "arxiv_url": "http://arxiv.org/abs/2511.21689v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Structured Prompting Enables More Robust Evaluation of Language Models",
    "authors": [
      "Asad Aali",
      "Muhammad Ahmed Mohsin",
      "Vasiliki Bikia",
      "Arnav Singhvi",
      "Richard Gaus",
      "Suhana Bedi",
      "Hejie Cui",
      "Miguel Fuentes",
      "Alyssa Unell",
      "Yifan Mai",
      "Jordan Cahoon",
      "Michael Pfeffer",
      "Roxana Daneshjou",
      "Sanmi Koyejo",
      "Emily Alsentzer",
      "Christopher Potts",
      "Nigam H. Shah",
      "Akshay S. Chaudhari"
    ],
    "summary": "As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we approximate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks ($+$2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing chain-of-thought reduces LM sensitivity to prompt design (smaller $\u0394$ across prompts). To our knowledge, this is the first benchmarking study to systematically integrate structured prompting into an es...",
    "published": "Nov 25",
    "pdf_url": "https://arxiv.org/pdf/2511.20836v2",
    "arxiv_url": "http://arxiv.org/abs/2511.20836v2",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
    "authors": [
      "Adhiraj Ghosh",
      "Vishaal Udandarao",
      "Thao Nguyen",
      "Matteo Farina",
      "Mehdi Cherti",
      "Jenia Jitsev",
      "Sewoong Oh",
      "Elisa Ricci",
      "Ludwig Schmidt",
      "Matthias Bethge"
    ],
    "summary": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
    "published": "Nov 25",
    "pdf_url": "https://arxiv.org/pdf/2511.20643v1",
    "arxiv_url": "http://arxiv.org/abs/2511.20643v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Latent Collaboration in Multi-Agent Systems",
    "authors": [
      "Jiaru Zou",
      "Xiyuan Yang",
      "Ruizhong Qiu",
      "Gaotang Li",
      "Katherine Tieu",
      "Pan Lu",
      "Ke Shen",
      "Hanghang Tong",
      "Yejin Choi",
      "Jingrui He",
      "James Zou",
      "Mengdi Wang",
      "Ling Yang"
    ],
    "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
    "published": "Nov 25",
    "pdf_url": "https://arxiv.org/pdf/2511.20639v2",
    "arxiv_url": "http://arxiv.org/abs/2511.20639v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
    "authors": [
      "Rulin Shao",
      "Akari Asai",
      "Shannon Zejiang Shen",
      "Hamish Ivison",
      "Varsha Kishore",
      "Jingming Zhuo",
      "Xinran Zhao",
      "Molly Park",
      "Samuel G. Finlayson",
      "David Sontag",
      "Tyler Murray",
      "Sewon Min",
      "Pradeep Dasigi",
      "Luca Soldaini",
      "Faeze Brahman",
      "Wen-tau Yih",
      "Tongshuang Wu",
      "Luke Zettlemoyer",
      "Yoon Kim",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh"
    ],
    "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
    "published": "Nov 24",
    "pdf_url": "https://arxiv.org/pdf/2511.19399v2",
    "arxiv_url": "http://arxiv.org/abs/2511.19399v2",
    "queried_author": "Akari Asai",
    "matching_authors": [
      "Akari Asai",
      "Hamish Ivison",
      "Hannaneh Hajishirzi",
      "Luca Soldaini",
      "Luke Zettlemoyer",
      "Pang Wei Koh",
      "Yoon Kim"
    ]
  },
  {
    "title": "Natural Emergent Misalignment from Reward Hacking in Production RL",
    "authors": [
      "Monte MacDiarmid",
      "Benjamin Wright",
      "Jonathan Uesato",
      "Joe Benton",
      "Jon Kutasov",
      "Sara Price",
      "Naia Bouscal",
      "Sam Bowman",
      "Trenton Bricken",
      "Alex Cloud",
      "Carson Denison",
      "Johannes Gasteiger",
      "Ryan Greenblatt",
      "Jan Leike",
      "Jack Lindsey",
      "Vlad Mikulik",
      "Ethan Perez",
      "Alex Rodrigues",
      "Drake Thomas",
      "Albert Webson",
      "Daniel Ziegler",
      "Evan Hubinger"
    ],
    "summary": "We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) \"inoculation prompting\", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned.",
    "published": "Nov 23",
    "pdf_url": "https://arxiv.org/pdf/2511.18397v1",
    "arxiv_url": "http://arxiv.org/abs/2511.18397v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Fantastic Bugs and Where to Find Them in AI Benchmarks",
    "authors": [
      "Sang Truong",
      "Yuheng Tu",
      "Michael Hardy",
      "Anka Reuel",
      "Zeyu Tang",
      "Jirayu Burapacheep",
      "Jonathan Perera",
      "Chibuike Uwakwe",
      "Ben Domingue",
      "Nick Haber",
      "Sanmi Koyejo"
    ],
    "summary": "Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.",
    "published": "Nov 20",
    "pdf_url": "https://arxiv.org/pdf/2511.16842v1",
    "arxiv_url": "http://arxiv.org/abs/2511.16842v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "ARC Is a Vision Problem!",
    "authors": [
      "Keya Hu",
      "Ali Cy",
      "Linlu Qiu",
      "Xiaoman Delores Ding",
      "Runqian Wang",
      "Yeyin Eva Zhu",
      "Jacob Andreas",
      "Kaiming He"
    ],
    "summary": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
    "published": "Nov 18",
    "pdf_url": "https://arxiv.org/pdf/2511.14761v1",
    "arxiv_url": "http://arxiv.org/abs/2511.14761v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels",
    "authors": [
      "Stuart H. Sul",
      "Simran Arora",
      "Benjamin F. Spector",
      "Christopher R\u00e9"
    ],
    "summary": "Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \\times$ speedup for data- and tensor-parallel workloads, $4.08 \\times$ for sequence-parallel workloads, and $1.22 \\times$ for expert-parallel workloads.",
    "published": "Nov 17",
    "pdf_url": "https://arxiv.org/pdf/2511.13940v1",
    "arxiv_url": "http://arxiv.org/abs/2511.13940v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "Beat the long tail: Distribution-Aware Speculative Decoding for RL Training",
    "authors": [
      "Zelei Shao",
      "Vikranth Srivatsa",
      "Sanjana Srivastava",
      "Qingyang Wu",
      "Alpay Ariyak",
      "Xiaoxia Wu",
      "Ameen Patel",
      "Jue Wang",
      "Percy Liang",
      "Tri Dao",
      "Ce Zhang",
      "Yiying Zhang",
      "Ben Athiwaratkun",
      "Chenfeng Xu",
      "Junxiong Wang"
    ],
    "summary": "Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.",
    "published": "Nov 17",
    "pdf_url": "https://arxiv.org/pdf/2511.13841v1",
    "arxiv_url": "http://arxiv.org/abs/2511.13841v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Tri Dao"
    ]
  },
  {
    "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs",
    "authors": [
      "Truong Vo",
      "Sanmi Koyejo"
    ],
    "summary": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.",
    "published": "Nov 15",
    "pdf_url": "https://arxiv.org/pdf/2511.12014v1",
    "arxiv_url": "http://arxiv.org/abs/2511.12014v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations",
    "authors": [
      "Eunkyu Park",
      "Wesley Hanwen Deng",
      "Vasudha Varadarajan",
      "Mingxi Yan",
      "Gunhee Kim",
      "Maarten Sap",
      "Motahhare Eslami"
    ],
    "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.",
    "published": "Nov 15",
    "pdf_url": "https://arxiv.org/pdf/2511.12001v2",
    "arxiv_url": "http://arxiv.org/abs/2511.12001v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "On the Entropy Calibration of Language Models",
    "authors": [
      "Steven Cao",
      "Gregory Valiant",
      "Percy Liang"
    ],
    "summary": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing as generations grow longer, due to error accumulation. To calibrate the model and improve text quality, it has become standard practice to truncate the distribution, but this approach reduces output diversity, which we would like to avoid. Therefore, in this paper, we ask: does miscalibration improve automatically with scale, and if not, is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the rate of scaling depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted theoretically: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution becau...",
    "published": "Nov 15",
    "pdf_url": "https://arxiv.org/pdf/2511.11966v2",
    "arxiv_url": "http://arxiv.org/abs/2511.11966v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Modeling and Predicting Multi-Turn Answer Instability in Large Language Models",
    "authors": [
      "Jiahang He",
      "Rishi Ramachandran",
      "Neel Ramachandran",
      "Aryan Katakam",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Aryan Shrivastava"
    ],
    "summary": "As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple \"Think again\" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.",
    "published": "Nov 12",
    "pdf_url": "https://arxiv.org/pdf/2511.10688v1",
    "arxiv_url": "http://arxiv.org/abs/2511.10688v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Training Language Models to Explain Their Own Computations",
    "authors": [
      "Belinda Z. Li",
      "Zifan Carl Guo",
      "Vincent Huang",
      "Jacob Steinhardt",
      "Jacob Andreas"
    ],
    "summary": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the explainer model is significantly more capable than the target). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods. Code and data at https://github.com/TransluceAI/introspective-interp",
    "published": "Nov 11",
    "pdf_url": "https://arxiv.org/pdf/2511.08579v3",
    "arxiv_url": "http://arxiv.org/abs/2511.08579v3",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "HipKittens: Fast and Furious AMD Kernels",
    "authors": [
      "William Hu",
      "Drew Wadsworth",
      "Sean Siddens",
      "Stanley Winata",
      "Daniel Y. Fu",
      "Ryann Swann",
      "Muhammad Osama",
      "Christopher R\u00e9",
      "Simran Arora"
    ],
    "summary": "AMD GPUs offer state-of-the-art compute and memory bandwidth; however, peak performance AMD kernels are written in raw assembly. To address the difficulty of mapping AI algorithms to hardware, recent work proposes C++ embedded and PyTorch-inspired domain-specific languages like ThunderKittens (TK) to simplify high performance AI kernel development on NVIDIA hardware. We explore the extent to which such primitives -- for explicit tile-based programming with optimized memory accesses and fine-grained asynchronous execution across workers -- are NVIDIA-specific or general. We provide the first detailed study of the programming primitives that lead to performant AMD AI kernels, and we encapsulate these insights in the HipKittens (HK) programming framework. We find that tile-based abstractions used in prior DSLs generalize to AMD GPUs, however we need to rethink the algorithms that instantiate these abstractions for AMD. We validate the HK primitives across CDNA3 and CDNA4 AMD platforms. In evaluations, HK kernels compete with AMD's hand-optimized assembly kernels for GEMMs and attention, and consistently outperform compiler baselines. Moreover, assembly is difficult to scale to the breadth of AI workloads; reflecting this, in some settings HK outperforms all available kernel baselines by $1.2-2.4\\times$ (e.g., $d=64$ attention, GQA backwards, memory-bound kernels). These findings help pave the way for a single, tile-based software layer for high-performance AI kernels that translates across GPU vendors. HipKittens is released at: https://github.com/HazyResearch/HipKittens.",
    "published": "Nov 11",
    "pdf_url": "https://arxiv.org/pdf/2511.08083v1",
    "arxiv_url": "http://arxiv.org/abs/2511.08083v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
    "authors": [
      "Jon Saad-Falcon",
      "Avanika Narayan",
      "Hakki Orhun Akengin",
      "J. Wes Griffin",
      "Herumb Shandilya",
      "Adrian Gamarra Lafuente",
      "Medhya Goel",
      "Rebecca Joseph",
      "Shlok Natarajan",
      "Etash Kumar Guha",
      "Shang Zhu",
      "Ben Athiwaratkun",
      "John Hennessy",
      "Azalia Mirhoseini",
      "Christopher R\u00e9"
    ],
    "summary": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant head...",
    "published": "Nov 11",
    "pdf_url": "https://arxiv.org/pdf/2511.07885v3",
    "arxiv_url": "http://arxiv.org/abs/2511.07885v3",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought",
    "authors": [
      "Shourya Batra",
      "Pierce Tillman",
      "Samarth Gaggar",
      "Shashank Kesineni",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Vasu Sharma",
      "Maheep Chaudhary"
    ],
    "summary": "As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\\%$ reduction in CPL on QwQ-32B, $17.9\\%$ reduction in CPL on Llama-3.1-8B, and $31.2\\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.",
    "published": "Nov 11",
    "pdf_url": "https://arxiv.org/pdf/2511.07772v2",
    "arxiv_url": "http://arxiv.org/abs/2511.07772v2",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments",
    "authors": [
      "Zhiyuan Zeng",
      "Hamish Ivison",
      "Yiping Wang",
      "Lifan Yuan",
      "Shuyue Stella Li",
      "Zhuorui Ye",
      "Siting Li",
      "Jacqueline He",
      "Runlong Zhou",
      "Tong Chen",
      "Chenyang Zhao",
      "Yulia Tsvetkov",
      "Simon Shaolei Du",
      "Natasha Jaques",
      "Hao Peng",
      "Pang Wei Koh",
      "Hannaneh Hajishirzi"
    ],
    "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.",
    "published": "Nov 10",
    "pdf_url": "https://arxiv.org/pdf/2511.07317v1",
    "arxiv_url": "http://arxiv.org/abs/2511.07317v1",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search",
    "authors": [
      "Samuel Sokota",
      "Eugene Vinitsky",
      "Hengyuan Hu",
      "J. Zico Kolter",
      "Gabriele Farina"
    ],
    "summary": "Few classical games have been regarded as such significant benchmarks of artificial intelligence as to have justified training costs in the millions of dollars. Among these, Stratego -- a board wargame exemplifying the challenge of strategic decision making under massive amounts of hidden information -- stands apart as a case where such efforts failed to produce performance at the level of top humans. This work establishes a step change in both performance and cost for Stratego, showing that it is now possible not only to reach the level of top humans, but to achieve vastly superhuman level -- and that doing so requires not an industrial budget, but merely a few thousand dollars. We achieved this result by developing general approaches for self-play reinforcement learning and test-time search under imperfect information.",
    "published": "Nov 10",
    "pdf_url": "https://arxiv.org/pdf/2511.07312v1",
    "arxiv_url": "http://arxiv.org/abs/2511.07312v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits",
    "authors": [
      "Dev Patel",
      "Gabrielle Gervacio",
      "Diekola Raimi",
      "Kevin Zhu",
      "Ryan Lagasse",
      "Gabriel Grand",
      "Ashwinee Panda",
      "Maheep Chaudhary"
    ],
    "summary": "Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\\% at matched compute, enabling efficient yet safety-preserving LLM deployment.",
    "published": "Nov 09",
    "pdf_url": "https://arxiv.org/pdf/2511.07482v1",
    "arxiv_url": "http://arxiv.org/abs/2511.07482v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "authors": [
      "Renfei Zhang",
      "Manasa Kaniselvan",
      "Niloofar Mireshghallah"
    ],
    "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
    "published": "Nov 08",
    "pdf_url": "https://arxiv.org/pdf/2511.05933v1",
    "arxiv_url": "http://arxiv.org/abs/2511.05933v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "Long Grounded Thoughts: Synthesizing Visual Problems and Reasoning Chains at Scale",
    "authors": [
      "David Acuna",
      "Chao-Han Huck Yang",
      "Yuntian Deng",
      "Jaehun Jung",
      "Ximing Lu",
      "Prithviraj Ammanabrolu",
      "Hyunwoo Kim",
      "Yuan-Hong Liao",
      "Yejin Choi"
    ],
    "summary": "Despite rapid progress, multimodal reasoning still lacks a systematic approach to synthesize large-scale vision-centric datasets beyond visual math. We introduce a framework able to synthesize vision-centric problems spanning diverse levels of complexity, and the resulting dataset with over 1M high-quality problems including: reasoning traces, preference data, and instruction prompts supporting SFT, offline and online RL. Our vision-centric synthesis framework uses a two-stage process focusing on: (1) generating diverse verifiable questions from existing images at scale, and (2) creating complex compositional visual problems by merging simpler questions. Remarkably, finetuning Qwen2.5-VL-7B on our data outperforms existing open-data baselines across evaluated vision-centric benchmarks, and our best configurations match or surpass strong closed-data models such as MiMo-VL-7B-RL on Vstar Bench, CV-Bench and MMStar-V. Notably, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro, +3.7%) and audio reasoning (MMAU, +1.32%), demonstrating its effectiveness. Similarly, despite containing no embodied visual data, we observe notable gains (NiEH, +8.8%) when evaluating open-ended embodied QA. Lastly, we use our data to comprehensively analyze at scale (1M+) the entire VLM post-training pipeline showing that (i) SFT on high-quality data with cognitive behaviors on reasoning traces is essential to scale online RL, (ii) offline RL could match online RL's performance while disaggregating compute demands, and, (iii) SFT on high quality data...",
    "published": "Nov 07",
    "pdf_url": "https://arxiv.org/pdf/2511.05705v2",
    "arxiv_url": "http://arxiv.org/abs/2511.05705v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Real-Time Reasoning Agents in Evolving Environments",
    "authors": [
      "Yule Wen",
      "Yixin Ye",
      "Yanzhe Zhang",
      "Diyi Yang",
      "Hao Zhu"
    ],
    "summary": "Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.",
    "published": "Nov 07",
    "pdf_url": "https://arxiv.org/pdf/2511.04898v1",
    "arxiv_url": "http://arxiv.org/abs/2511.04898v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Addressing divergent representations from causal interventions on neural networks",
    "authors": [
      "Satchel Grant",
      "Simon Jerome Han",
      "Alexa R. Tartaglini",
      "Christopher Potts"
    ],
    "summary": "A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate theoretically and empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two cases of such divergences: \"harmless\" divergences that occur in the behavioral null-space of the layer(s) of interest, and \"pernicious\" divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we apply and modify the Counterfactual Latent (CL) loss from Grant (2025) allowing representations from causal interventions to remain closer to the natural distribution, reducing the likelihood of harmful divergences while preserving the interpretive power of the interventions. Together, these results highlight a path towards more reliable interpretability methods.",
    "published": "Nov 06",
    "pdf_url": "https://arxiv.org/pdf/2511.04638v4",
    "arxiv_url": "http://arxiv.org/abs/2511.04638v4",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Who Evaluates AI's Social Impacts? Mapping Coverage and Gaps in First and Third Party Evaluations",
    "authors": [
      "Anka Reuel",
      "Avijit Ghosh",
      "Jenny Chim",
      "Andrew Tran",
      "Yanan Long",
      "Jennifer Mickel",
      "Usman Gohar",
      "Srishti Yadav",
      "Pawan Sasanka Ammanamanchi",
      "Mowafak Allaham",
      "Hossein A. Rahmani",
      "Mubashara Akhtar",
      "Felix Friedrich",
      "Robert Scholz",
      "Michael Alexander Riegler",
      "Jan Batzner",
      "Eliya Habba",
      "Arushi Saxena",
      "Anastassia Kornilova",
      "Kevin Wei",
      "Prajna Soni",
      "Yohan Mathew",
      "Kevin Klyman",
      "Jeba Sania",
      "Subramanyam Sahoo",
      "Olivia Beyer Bruvik",
      "Pouya Sadeghi",
      "Sujata Goswami",
      "Angelina Wang",
      "Yacine Jernite",
      "Zeerak Talat",
      "Stella Biderman",
      "Mykel Kochenderfer",
      "Sanmi Koyejo",
      "Irene Solaiman"
    ],
    "summary": "Foundation models are increasingly central to high-stakes AI systems, and governance frameworks now depend on evaluations to assess their risks and capabilities. Although general capability evaluations are widespread, social impact assessments covering bias, fairness, privacy, environmental costs, and labor practices remain uneven across the AI ecosystem. To characterize this landscape, we conduct the first comprehensive analysis of both first-party and third-party social impact evaluation reporting across a wide range of model developers. Our study examines 186 first-party release reports and 183 post-release evaluation sources, and complements this quantitative analysis with interviews of model developers. We find a clear division of evaluation labor: first-party reporting is sparse, often superficial, and has declined over time in key areas such as environmental impact and bias, while third-party evaluators including academic researchers, nonprofits, and independent organizations provide broader and more rigorous coverage of bias, harmful content, and performance disparities. However, this complementarity has limits. Only model developers can authoritatively report on data provenance, content moderation labor, financial costs, and training infrastructure, yet interviews reveal that these disclosures are often deprioritized unless tied to product adoption or regulatory compliance. Our findings indicate that current evaluation practices leave major gaps in assessing AI's societal impacts, highlighting the urgent need for policies that promote developer transparency, streng...",
    "published": "Nov 06",
    "pdf_url": "https://arxiv.org/pdf/2511.05613v1",
    "arxiv_url": "http://arxiv.org/abs/2511.05613v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Reusing Pre-Training Data at Test Time is a Compute Multiplier",
    "authors": [
      "Alex Fang",
      "Thomas Voice",
      "Ruoming Pang",
      "Ludwig Schmidt",
      "Tom Gunter"
    ],
    "summary": "Large language models learn from their vast pre-training corpora, gaining the ability to solve an ever increasing variety of tasks; yet although researchers work to improve these datasets, there is little effort to understand how efficient the pre-training apparatus is at extracting ideas and knowledge from the data. In this work, we use retrieval augmented generation along with test-time compute as a way to quantify how much dataset value was left behind by the process of pre-training, and how this changes across scale. We demonstrate that pre-training then retrieving from standard and largely open-sourced datasets results in significant accuracy gains in MMLU, Math-500, and SimpleQA, which persist through decontamination. For MMLU we observe that retrieval acts as a ~5x compute multiplier versus pre-training alone. We show that these results can be further improved by leveraging additional compute at test time to parse the retrieved context, demonstrating a 10 percentage point improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results suggest that today's pre-training methods do not make full use of the information in existing pre-training datasets, leaving significant room for progress.",
    "published": "Nov 06",
    "pdf_url": "https://arxiv.org/pdf/2511.04234v1",
    "arxiv_url": "http://arxiv.org/abs/2511.04234v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation",
    "authors": [
      "Shih-Lun Wu",
      "Yoon Kim",
      "Cheng-Zhi Anna Huang"
    ],
    "summary": "We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.",
    "published": "Nov 06",
    "pdf_url": "https://arxiv.org/pdf/2511.03942v1",
    "arxiv_url": "http://arxiv.org/abs/2511.03942v1",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents",
    "authors": [
      "Xingyao Wang",
      "Simon Rosenberg",
      "Juan Michelini",
      "Calvin Smith",
      "Hoang Tran",
      "Engel Nyst",
      "Rohit Malhotra",
      "Xuhui Zhou",
      "Valerie Chen",
      "Robert Brennan",
      "Graham Neubig"
    ],
    "summary": "Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation...",
    "published": "Nov 05",
    "pdf_url": "https://arxiv.org/pdf/2511.03690v1",
    "arxiv_url": "http://arxiv.org/abs/2511.03690v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Reading Between the Lines: The One-Sided Conversation Problem",
    "authors": [
      "Victoria Ebert",
      "Rishabh Singh",
      "Tuochao Chen",
      "Noah A. Smith",
      "Shyamnath Gollakota"
    ],
    "summary": "Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.",
    "published": "Nov 04",
    "pdf_url": "https://arxiv.org/pdf/2511.03056v1",
    "arxiv_url": "http://arxiv.org/abs/2511.03056v1",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities",
    "authors": [
      "Amanda Bertsch",
      "Adithya Pratapa",
      "Teruko Mitamura",
      "Graham Neubig",
      "Matthew R. Gormley"
    ],
    "summary": "As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.",
    "published": "Nov 04",
    "pdf_url": "https://arxiv.org/pdf/2511.02817v1",
    "arxiv_url": "http://arxiv.org/abs/2511.02817v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval",
    "authors": [
      "Hung-Ting Chen",
      "Xiang Liu",
      "Shauli Ravfogel",
      "Eunsol Choi"
    ],
    "summary": "Most text retrievers generate \\emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.",
    "published": "Nov 04",
    "pdf_url": "https://arxiv.org/pdf/2511.02770v1",
    "arxiv_url": "http://arxiv.org/abs/2511.02770v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining",
    "authors": [
      "Costin-Andrei Oncescu",
      "Qingyang Wu",
      "Wai Tong Chung",
      "Robert Wu",
      "Bryan Gopal",
      "Junxiong Wang",
      "Tri Dao",
      "Ben Athiwaratkun"
    ],
    "summary": "An increasing number of LLMs employ Mixture-of-Experts (MoE) architectures where the feed-forward layer is replaced by a pool of experts and each token only activates a small subset of them. During autoregressive generation, these models often enter a memory-bound regime even for moderate batch sizes because the average expert load grows more slowly than in an equivalent dense feedforward layer. Consequently, MoE latency is governed by the number of activated experts. We introduce a framework for dynamically re-routing token-to-expert mapping to lower this number (and thus, the decode latency) while preserving a comparable quality. Our best results use a batch-aware routing that works by having tokens piggyback experts that have already been loaded into memory due to being crucial to other tokens within the same batch. Empirically, we evaluate our method on the Qwen3-30B and Qwen3-235B models with a batch size of $16$. Without any statistically significant loss in accuracy, our approach achieves latency reductions of $39\\%$ and $15\\%$ in the MoE layer decode latency, respectively.",
    "published": "Nov 04",
    "pdf_url": "https://arxiv.org/pdf/2511.02237v1",
    "arxiv_url": "http://arxiv.org/abs/2511.02237v1",
    "queried_author": "Tri Dao",
    "matching_authors": [
      "Tri Dao"
    ]
  },
  {
    "title": "Training Proactive and Personalized LLM Agents",
    "authors": [
      "Weiwei Sun",
      "Xuhui Zhou",
      "Weihua Du",
      "Xingyao Wang",
      "Sean Welleck",
      "Graham Neubig",
      "Maarten Sap",
      "Yiming Yang"
    ],
    "summary": "While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.",
    "published": "Nov 04",
    "pdf_url": "https://arxiv.org/pdf/2511.02208v1",
    "arxiv_url": "http://arxiv.org/abs/2511.02208v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig",
      "Maarten Sap",
      "Sean Welleck"
    ]
  },
  {
    "title": "Shared Parameter Subspaces and Cross-Task Linearity in Emergently Misaligned Behavior",
    "authors": [
      "Daniel Aarao Reis Arturi",
      "Eric Zhang",
      "Andrew Ansah",
      "Kevin Zhu",
      "Ashwinee Panda",
      "Aishwarya Balwani"
    ],
    "summary": "Recent work has discovered that large language models can develop broadly misaligned behaviors after being fine-tuned on narrowly harmful datasets, a phenomenon known as emergent misalignment (EM). However, the fundamental mechanisms enabling such harmful generalization across disparate domains remain poorly understood. In this work, we adopt a geometric perspective to study EM and demonstrate that it exhibits a fundamental cross-task linear structure in how harmful behavior is encoded across different datasets. Specifically, we find a strong convergence in EM parameters across tasks, with the fine-tuned weight updates showing relatively high cosine similarities, as well as shared lower-dimensional subspaces as measured by their principal angles and projection overlaps. Furthermore, we also show functional equivalence via linear mode connectivity, wherein interpolated models across narrow misalignment tasks maintain coherent, broadly misaligned behavior. Our results indicate that EM arises from different narrow tasks discovering the same set of shared parameter directions, suggesting that harmful behaviors may be organized into specific, predictable regions of the weight landscape. By revealing this fundamental connection between parametric geometry and behavioral outcomes, we hope our work catalyzes further research on parameter space interpretability and weight-based interventions.",
    "published": "Nov 03",
    "pdf_url": "https://arxiv.org/pdf/2511.02022v1",
    "arxiv_url": "http://arxiv.org/abs/2511.02022v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Accumulating Context Changes the Beliefs of Language Models",
    "authors": [
      "Jiayi Geng",
      "Howard Chen",
      "Ryan Liu",
      "Manoel Horta Ribeiro",
      "Robb Willer",
      "Graham Neubig",
      "Thomas L. Griffiths"
    ],
    "summary": "Language model (LM) assistants are increasingly used in applications such as brainstorming and research. Improvements in memory and context size have allowed these models to become more autonomous, which has also resulted in more text accumulation in their context windows without explicit user intervention. This comes with a latent risk: the belief profiles of models -- their understanding of the world as manifested in their responses or actions -- may silently change as context accumulates. This can lead to subtly inconsistent user experiences, or shifts in behavior that deviate from the original alignment of the models. In this paper, we explore how accumulating context by engaging in interactions and processing text -- talking and reading -- can change the beliefs of language models, as manifested in their responses and behaviors. Our results reveal that models' belief profiles are highly malleable: GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of discussion about moral dilemmas and queries about safety, while Grok 4 shows a 27.2% shift on political issues after reading texts from the opposing position. We also examine models' behavioral changes by designing tasks that require tool use, where each tool selection corresponds to an implicit belief. We find that these changes align with stated belief shifts, suggesting that belief shifts will be reflected in actual behavior in agentic systems. Our analysis exposes the hidden risk of belief shift as models undergo extended sessions of talking or reading, rendering their opinions and actions unreliable.",
    "published": "Nov 03",
    "pdf_url": "https://arxiv.org/pdf/2511.01805v2",
    "arxiv_url": "http://arxiv.org/abs/2511.01805v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks",
    "authors": [
      "Andrew M. Bean",
      "Ryan Othniel Kearns",
      "Angelika Romanou",
      "Franziska Sofia Hafner",
      "Harry Mayne",
      "Jan Batzner",
      "Negar Foroutan",
      "Chris Schmitz",
      "Karolina Korgul",
      "Hunar Batra",
      "Oishi Deb",
      "Emma Beharry",
      "Cornelius Emde",
      "Thomas Foster",
      "Anna Gausen",
      "Mar\u00eda Grandury",
      "Simeng Han",
      "Valentin Hofmann",
      "Lujain Ibrahim",
      "Hazel Kim",
      "Hannah Rose Kirk",
      "Fangru Lin",
      "Gabrielle Kaili-May Liu",
      "Lennart Luettgau",
      "Jabez Magomere",
      "Jonathan Rystr\u00f8m",
      "Anna Sotnikova",
      "Yushi Yang",
      "Yilun Zhao",
      "Adel Bibi",
      "Antoine Bosselut",
      "Ronald Clark",
      "Arman Cohan",
      "Jakob Foerster",
      "Yarin Gal",
      "Scott A. Hale",
      "Inioluwa Deborah Raji",
      "Christopher Summerfield",
      "Philip H. S. Torr",
      "Cozmin Ududec",
      "Luc Rocher",
      "Adam Mahdi"
    ],
    "summary": "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.",
    "published": "Nov 03",
    "pdf_url": "https://arxiv.org/pdf/2511.04703v1",
    "arxiv_url": "http://arxiv.org/abs/2511.04703v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut",
      "Valentin Hofmann"
    ]
  },
  {
    "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering",
    "authors": [
      "John Yang",
      "Kilian Lieret",
      "Joyce Yang",
      "Carlos E. Jimenez",
      "Ofir Press",
      "Ludwig Schmidt",
      "Diyi Yang"
    ],
    "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the stu...",
    "published": "Nov 02",
    "pdf_url": "https://arxiv.org/pdf/2511.00839v1",
    "arxiv_url": "http://arxiv.org/abs/2511.00839v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering",
    "authors": [
      "Eric Bigelow",
      "Daniel Wurgaft",
      "YingQiao Wang",
      "Noah Goodman",
      "Tomer Ullman",
      "Hidenori Tanaka",
      "Ekdeep Singh Lubana"
    ],
    "summary": "Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.",
    "published": "Nov 01",
    "pdf_url": "https://arxiv.org/pdf/2511.00617v1",
    "arxiv_url": "http://arxiv.org/abs/2511.00617v1",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
    "authors": [
      "Caleb Ziems",
      "William Held",
      "Jane Yu",
      "Amir Goldberg",
      "David Grusky",
      "Diyi Yang"
    ],
    "summary": "To serve global users safely and productively, LLMs need culture-specific knowledge that might not be learned during pre-training. How do we find such knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The most common solutions are single-initiative: either researchers define challenging questions that users passively answer (traditional annotation), or users actively produce data that researchers structure as benchmarks (knowledge extraction). The process would benefit from mixed-initiative collaboration, where users guide the process to meaningfully reflect their cultures, and LLMs steer the process towards more challenging questions that meet the researcher's goals. We propose a mixed-initiative methodology called CultureCartography. Here, an LLM initializes annotation with questions for which it has low-confidence answers, making explicit both its prior knowledge and the gaps therein. This allows a human respondent to fill these gaps and steer the model towards salient topics through direct edits. We implement this methodology as a tool called CultureExplorer. Compared to a baseline where humans answer LLM-proposed questions, we find that CultureExplorer more effectively produces knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B by up to 19.2% on related culture benchmarks.",
    "published": "Oct 31",
    "pdf_url": "https://arxiv.org/pdf/2510.27672v1",
    "arxiv_url": "http://arxiv.org/abs/2510.27672v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "William Held"
    ]
  },
  {
    "title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models",
    "authors": [
      "Boyi Wei",
      "Zora Che",
      "Nathaniel Li",
      "Udari Madhushani Sehwag",
      "Jasper G\u00f6tting",
      "Samira Nedungadi",
      "Julian Michael",
      "Summer Yue",
      "Dan Hendrycks",
      "Peter Henderson",
      "Zifan Wang",
      "Seth Donoughe",
      "Mantas Mazeika"
    ],
    "summary": "Open-weight bio-foundation models present a dual-use dilemma. While holding great promise for accelerating scientific research and drug development, they could also enable bad actors to develop more deadly bioweapons. To mitigate the risk posed by these models, current approaches focus on filtering biohazardous data during pre-training. However, the effectiveness of such an approach remains unclear, particularly against determined actors who might fine-tune these models for malicious use. To address this gap, we propose BioRiskEval, a framework to evaluate the robustness of procedures that are intended to reduce the dual-use capabilities of bio-foundation models. BioRiskEval assesses models' virus understanding through three lenses, including sequence modeling, mutational effects prediction, and virulence prediction. Our results show that current filtering practices may not be particularly effective: Excluded knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits broader generalizability in sequence modeling. Furthermore, dual-use signals may already reside in the pretrained representations, and can be elicited via simple linear probing. These findings highlight the challenges of data filtering as a standalone procedure, underscoring the need for further research into robust safety and security strategies for open-weight bio-foundation models.",
    "published": "Oct 31",
    "pdf_url": "https://arxiv.org/pdf/2510.27629v4",
    "arxiv_url": "http://arxiv.org/abs/2510.27629v4",
    "queried_author": "Peter Henderson",
    "matching_authors": [
      "Peter Henderson"
    ]
  },
  {
    "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
    "authors": [
      "Uzay Macar",
      "Paul C. Bogdan",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In \"agentic misalignment\" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output withou...",
    "published": "Oct 31",
    "pdf_url": "https://arxiv.org/pdf/2510.27484v1",
    "arxiv_url": "http://arxiv.org/abs/2510.27484v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments",
    "authors": [
      "Rishika Bhagwatkar",
      "Syrielle Montariol",
      "Angelika Romanou",
      "Beatriz Borges",
      "Irina Rish",
      "Antoine Bosselut"
    ],
    "summary": "Humans can naturally identify, reason about, and explain anomalies in their environment. In computer vision, this long-standing challenge remains limited to industrial defects or unrealistic, synthetically generated anomalies, failing to capture the richness and unpredictability of real-world anomalies. In this work, we introduce CAVE, the first benchmark of real-world visual anomalies. CAVE supports three open-ended tasks: anomaly description, explanation, and justification; with fine-grained annotations for visual grounding and categorizing anomalies based on their visual manifestations, their complexity, severity, and commonness. These annotations draw inspiration from cognitive science research on how humans identify and resolve anomalies, providing a comprehensive framework for evaluating Vision-Language Models (VLMs) in detecting and understanding anomalies. We show that state-of-the-art VLMs struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies. By offering a realistic and cognitively grounded benchmark, CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs.",
    "published": "Oct 29",
    "pdf_url": "https://arxiv.org/pdf/2510.26006v1",
    "arxiv_url": "http://arxiv.org/abs/2510.26006v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Revisiting Multilingual Data Mixtures in Language Model Pretraining",
    "authors": [
      "Negar Foroutan",
      "Paul Teiletche",
      "Ayush Kumar Tarun",
      "Antoine Bosselut"
    ],
    "summary": "The impact of different multilingual data mixtures in pretraining large language models (LLMs) has been a topic of ongoing debate, often raising concerns about potential trade-offs between language coverage and model performance (i.e., the curse of multilinguality). In this work, we investigate these assumptions by training 1.1B and 3B parameter LLMs on diverse multilingual corpora, varying the number of languages from 25 to 400. Our study challenges common beliefs surrounding multilingual training. First, we find that combining English and multilingual data does not necessarily degrade the in-language performance of either group, provided that languages have a sufficient number of tokens included in the pretraining corpus. Second, we observe that using English as a pivot language (i.e., a high-resource language that serves as a catalyst for multilingual generalization) yields benefits across language families, and contrary to expectations, selecting a pivot language from within a specific family does not consistently improve performance for languages within that family. Lastly, we do not observe a significant \"curse of multilinguality\" as the number of training languages increases in models at this scale. Our findings suggest that multilingual data, when balanced appropriately, can enhance language model capabilities without compromising performance, even in low-resource settings",
    "published": "Oct 29",
    "pdf_url": "https://arxiv.org/pdf/2510.25947v1",
    "arxiv_url": "http://arxiv.org/abs/2510.25947v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution",
    "authors": [
      "Junlong Li",
      "Wenshuo Zhao",
      "Jian Zhao",
      "Weihao Zeng",
      "Haoze Wu",
      "Xiaochen Wang",
      "Rui Ge",
      "Yuxuan Cao",
      "Yuzhen Huang",
      "Wei Liu",
      "Junteng Liu",
      "Zhaochen Su",
      "Yiyang Guo",
      "Fan Zhou",
      "Lueyang Zhang",
      "Juan Michelini",
      "Xingyao Wang",
      "Xiang Yue",
      "Shuyan Zhou",
      "Graham Neubig",
      "Junxian He"
    ],
    "summary": "Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomin...",
    "published": "Oct 29",
    "pdf_url": "https://arxiv.org/pdf/2510.25726v2",
    "arxiv_url": "http://arxiv.org/abs/2510.25726v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving",
    "authors": [
      "Auguste Poiroux",
      "Antoine Bosselut",
      "Viktor Kun\u010dak"
    ],
    "summary": "Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.",
    "published": "Oct 29",
    "pdf_url": "https://arxiv.org/pdf/2510.25427v1",
    "arxiv_url": "http://arxiv.org/abs/2510.25427v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents",
    "authors": [
      "Yueqi Song",
      "Ketan Ramaneti",
      "Zaid Sheikh",
      "Ziru Chen",
      "Boyu Gou",
      "Tianbao Xie",
      "Yiheng Xu",
      "Danyang Zhang",
      "Apurva Gandhi",
      "Fan Yang",
      "Joseph Liu",
      "Tianyue Ou",
      "Zhihao Yuan",
      "Frank Xu",
      "Shuyan Zhou",
      "Xingyao Wang",
      "Xiang Yue",
      "Tao Yu",
      "Huan Sun",
      "Yu Su",
      "Graham Neubig"
    ],
    "summary": "Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24702v1",
    "arxiv_url": "http://arxiv.org/abs/2510.24702v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning",
    "authors": [
      "Bo Liu",
      "Chuanyang Jin",
      "Seungone Kim",
      "Weizhe Yuan",
      "Wenting Zhao",
      "Ilia Kulikov",
      "Xian Li",
      "Sainbayar Sukhbaatar",
      "Jack Lanchantin",
      "Jason Weston"
    ],
    "summary": "Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24684v1",
    "arxiv_url": "http://arxiv.org/abs/2510.24684v1",
    "queried_author": "Seungone Kim",
    "matching_authors": [
      "Seungone Kim"
    ]
  },
  {
    "title": "Relative Scaling Laws for LLMs",
    "authors": [
      "William Held",
      "David Hall",
      "Percy Liang",
      "Diyi Yang"
    ],
    "summary": "Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introduce relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson.",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24626v2",
    "arxiv_url": "http://arxiv.org/abs/2510.24626v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Percy Liang",
      "William Held"
    ]
  },
  {
    "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
    "authors": [
      "Christine Ye",
      "Sihan Yuan",
      "Suchetha Cooray",
      "Steven Dillmann",
      "Ian L. V. Roque",
      "Dalya Baron",
      "Philipp Frank",
      "Sergio Martin-Alvarez",
      "Nolan Koblischke",
      "Frank J Qu",
      "Diyi Yang",
      "Risa Wechsler",
      "Ioana Ciuca"
    ],
    "summary": "Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and ...",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24591v2",
    "arxiv_url": "http://arxiv.org/abs/2510.24591v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature",
    "authors": [
      "Jack Merullo",
      "Srihita Vatsavaya",
      "Lucius Bushnaq",
      "Owen Lewis"
    ],
    "summary": "We characterize how memorization is represented in transformer models and show that it can be disentangled in the weights of both language models (LMs) and vision transformers (ViTs) using a decomposition based on the loss landscape curvature. This insight is based on prior theoretical and empirical work showing that the curvature for memorized training points is much sharper than non memorized, meaning ordering weight components from high to low curvature can reveal a distinction without explicit labels. This motivates a weight editing procedure that suppresses far more recitation of untargeted memorized data more effectively than a recent unlearning method (BalancedSubnet), while maintaining lower perplexity. Since the basis of curvature has a natural interpretation for shared structure in model weights, we analyze the editing procedure extensively on its effect on downstream tasks in LMs, and find that fact retrieval and arithmetic are specifically and consistently negatively affected, even though open book fact retrieval and general logical reasoning is conserved. We posit these tasks rely heavily on specialized directions in weight space rather than general purpose mechanisms, regardless of whether those individual datapoints are memorized. We support this by showing a correspondence between task data's activation strength with low curvature components that we edit out, and the drop in task performance after the edit. Our work enhances the understanding of memorization in neural networks with practical applications towards removing it, and provides evidence for idiosyn...",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24256v2",
    "arxiv_url": "http://arxiv.org/abs/2510.24256v2",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Success and Cost Elicit Convention Formation for Efficient Communication",
    "authors": [
      "Saujas Vaduguru",
      "Yilun Hua",
      "Yoav Artzi",
      "Daniel Fried"
    ],
    "summary": "Humans leverage shared conversational context to become increasingly successful and efficient at communicating over time. One manifestation of this is the formation of ad hoc linguistic conventions, which allow people to coordinate on short, less costly utterances that are understood using shared conversational context. We present a method to train large multimodal models to form conventions, enabling efficient communication. Our approach uses simulated reference games between models, and requires no additional human-produced data. In repeated reference games involving photographs and tangram images, our method enables models to communicate efficiently with people: reducing the message length by up to 41% while increasing success by 15% over the course of the interaction. Human listeners respond faster when interacting with our model that forms conventions. We also show that training based on success or cost alone is insufficient - both are necessary to elicit convention formation.",
    "published": "Oct 28",
    "pdf_url": "https://arxiv.org/pdf/2510.24023v1",
    "arxiv_url": "http://arxiv.org/abs/2510.24023v1",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)",
    "authors": [
      "Liwei Jiang",
      "Yuanjun Chai",
      "Margaret Li",
      "Mickel Liu",
      "Raymond Fok",
      "Nouha Dziri",
      "Yulia Tsvetkov",
      "Maarten Sap",
      "Alon Albalak",
      "Yejin Choi"
    ],
    "summary": "Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator ...",
    "published": "Oct 27",
    "pdf_url": "https://arxiv.org/pdf/2510.22954v1",
    "arxiv_url": "http://arxiv.org/abs/2510.22954v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap",
      "Yejin Choi"
    ]
  },
  {
    "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations",
    "authors": [
      "Zora Zhiruo Wang",
      "Yijia Shao",
      "Omar Shaikh",
      "Daniel Fried",
      "Graham Neubig",
      "Diyi Yang"
    ],
    "summary": "AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.",
    "published": "Oct 26",
    "pdf_url": "https://arxiv.org/pdf/2510.22780v2",
    "arxiv_url": "http://arxiv.org/abs/2510.22780v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Graham Neubig"
    ]
  },
  {
    "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality",
    "authors": [
      "Shayne Longpre",
      "Sneha Kudugunta",
      "Niklas Muennighoff",
      "I-Hung Hsu",
      "Isaac Caswell",
      "Alex Pentland",
      "Sercan Arik",
      "Chen-Yu Lee",
      "Sayna Ebrahimi"
    ],
    "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.",
    "published": "Oct 24",
    "pdf_url": "https://arxiv.org/pdf/2510.22037v2",
    "arxiv_url": "http://arxiv.org/abs/2510.22037v2",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection",
    "authors": [
      "Gaku Morio",
      "Harri Rowlands",
      "Dominik Stammbach",
      "Christopher D. Manning",
      "Peter Henderson"
    ],
    "summary": "Companies spend large amounts of money on public relations campaigns to project a positive brand image. However, sometimes there is a mismatch between what they say and what they do. Oil & gas companies, for example, are accused of \"greenwashing\" with imagery of climate-friendly initiatives. Understanding the framing, and changes in framing, at scale can help better understand the goals and nature of public relations campaigns. To address this, we introduce a benchmark dataset of expert-annotated video ads obtained from Facebook and YouTube. The dataset provides annotations for 13 framing types for more than 50 companies or advocacy groups across 20 countries. Our dataset is especially designed for the evaluation of vision-language models (VLMs), distinguishing it from past text-only framing datasets. Baseline experiments show some promising results, while leaving room for improvement for future work: GPT-4.1 can detect environmental messages with 79% F1 score, while our best model only achieves 46% F1 score on identifying framing around green innovation. We also identify challenges that VLMs must address, such as implicit framing, handling videos of various lengths, or implicit cultural backgrounds. Our dataset contributes to research in multimodal analysis of strategic communication in the energy sector.",
    "published": "Oct 24",
    "pdf_url": "https://arxiv.org/pdf/2510.21679v1",
    "arxiv_url": "http://arxiv.org/abs/2510.21679v1",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning",
      "Peter Henderson"
    ]
  },
  {
    "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents",
    "authors": [
      "Xuhui Zhou",
      "Valerie Chen",
      "Zora Zhiruo Wang",
      "Graham Neubig",
      "Maarten Sap",
      "Xingyao Wang"
    ],
    "summary": "Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user's mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a \\textbf{persistent memory} of the user, and provides user-related suggestions to the SWE agent. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7\\% compared to 18.1\\% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it useful 86\\% of the time, underscoring the value of stateful user modeling for practical coding agents.",
    "published": "Oct 24",
    "pdf_url": "https://arxiv.org/pdf/2510.21903v2",
    "arxiv_url": "http://arxiv.org/abs/2510.21903v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig",
      "Maarten Sap"
    ]
  },
  {
    "title": "L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks",
    "authors": [
      "Jiyu Cui",
      "Fang Wu",
      "Haokai Zhao",
      "Minggao Feng",
      "Xenophon Evangelopoulos",
      "Andrew I. Cooper",
      "Yejin Choi"
    ],
    "summary": "Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms lea...",
    "published": "Oct 23",
    "pdf_url": "https://arxiv.org/pdf/2510.20976v1",
    "arxiv_url": "http://arxiv.org/abs/2510.20976v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Code-enabled language models can outperform reasoning models on diverse tasks",
    "authors": [
      "Cedegao E. Zhang",
      "C\u00e9dric Colas",
      "Gabriel Poesia",
      "Joshua B. Tenenbaum",
      "Jacob Andreas"
    ],
    "summary": "Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. In this paper, we show that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, our simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%). Furthermore, the code-augmented reasoning traces display rich and varied problem-solving strategies. Our findings support that (1) CodeAdapt-style learning and reasoning may be robust and domain general and (2) code-enabled LMs are cognitively grounded and powerful systems, potentially providing a strong foundation for in-weight reinforcement learning.",
    "published": "Oct 23",
    "pdf_url": "https://arxiv.org/pdf/2510.20909v1",
    "arxiv_url": "http://arxiv.org/abs/2510.20909v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People",
    "authors": [
      "Gabriel Grand",
      "Valerio Pepe",
      "Jacob Andreas",
      "Joshua B. Tenenbaum"
    ],
    "summary": "Many high-stakes applications of AI require forming data-driven hypotheses and making targeted guesses; e.g., in scientific and diagnostic settings. Given limited resources, to what extent do agents based on language models (LMs) act rationally? We develop methods to benchmark and enhance agentic information-seeking, drawing on insights from human behavior. First, we introduce a strategic decision-oriented dialogue task called Collaborative Battleship, in which a partially-informed Captain must balance exploration (asking questions) and action (taking shots), while a fully-informed Spotter must provide accurate answers under an information bottleneck. Compared to human players (N=42), we find that LM agents struggle to ground answers in context, generate informative questions, and select high-value actions. Next, to address these gaps, we develop novel Monte Carlo inference strategies for LMs based on principles from Bayesian Experimental Design (BED). For Spotter agents, our approach boosts accuracy by up to 14.7% absolute over LM-only baselines; for Captain agents, it raises expected information gain (EIG) by up to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We replicate these findings on Guess Who? where our methods significantly boost accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for building rational inf...",
    "published": "Oct 23",
    "pdf_url": "https://arxiv.org/pdf/2510.20886v1",
    "arxiv_url": "http://arxiv.org/abs/2510.20886v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Simple Context Compression: Mean-Pooling and Multi-Ratio Training",
    "authors": [
      "Yair Feldman",
      "Yoav Artzi"
    ],
    "summary": "A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.",
    "published": "Oct 23",
    "pdf_url": "https://arxiv.org/pdf/2510.20797v1",
    "arxiv_url": "http://arxiv.org/abs/2510.20797v1",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Steering Evaluation-Aware Language Models to Act Like They Are Deployed",
    "authors": [
      "Tim Tian Hua",
      "Andrew Qin",
      "Samuel Marks",
      "Neel Nanda"
    ],
    "summary": "Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.",
    "published": "Oct 23",
    "pdf_url": "https://arxiv.org/pdf/2510.20487v4",
    "arxiv_url": "http://arxiv.org/abs/2510.20487v4",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "olmOCR 2: Unit Test Rewards for Document OCR",
    "authors": [
      "Jake Poznanski",
      "Luca Soldaini",
      "Kyle Lo"
    ],
    "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
    "published": "Oct 22",
    "pdf_url": "https://arxiv.org/pdf/2510.19817v1",
    "arxiv_url": "http://arxiv.org/abs/2510.19817v1",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo",
      "Luca Soldaini"
    ]
  },
  {
    "title": "Blackbox Model Provenance via Palimpsestic Membership Inference",
    "authors": [
      "Rohith Kuditipudi",
      "Jing Huang",
      "Sally Zhu",
      "Diyi Yang",
      "Christopher Potts",
      "Percy Liang"
    ],
    "summary": "Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice's model to produce text. Can Alice prove that Bob is using her model, either by querying Bob's derivative model (query setting) or from the text alone (observational setting)? We formulate this question as an independence testing problem--in which the null hypothesis is that Bob's model or text is independent of Alice's randomized training run--and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice's model using test statistics that capture correlation between Bob's model or text and the ordering of training examples in Alice's training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice's training data. In the query setting, we directly estimate (via prompting) the likelihood Bob's model gives to Alice's training examples and order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model's training data order, achieving a p-value on the order of at most 1e-8 in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob's text overlapping with spans of Alice's training examples and 2) the likelihood of Bob's text with respect to different versions of ...",
    "published": "Oct 22",
    "pdf_url": "https://arxiv.org/pdf/2510.19796v1",
    "arxiv_url": "http://arxiv.org/abs/2510.19796v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts",
      "Diyi Yang",
      "Percy Liang"
    ]
  },
  {
    "title": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters",
    "authors": [
      "Anand Choudhary",
      "Yasser Sula\u0131man",
      "Lukas Mauch",
      "Ghouthi Boukli Hacene",
      "Fabien Cardinaux",
      "Antoine Bosselut"
    ],
    "summary": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a sparse subset of model parameters. However, the effectiveness of sparse adaptation depends on optimally selecting the model parameters to be fine-tuned. In this work, we introduce a novel sparse fine-tuning technique named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which fine-tunes only those model parameters which have the largest gradient magnitudes on downstream tasks and the smallest pre-trained magnitudes, intuitively prioritizing parameters that are highly task-relevant, but minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3 8B and Gemma 2B as base models shows that GaLLoP consistently improves or matches the in-distribution as well as out-of-distribution performance obtained via the usage of other leading parameter-efficient fine-tuning techniques, including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates catastrophic forgetting and memorization of task data, as important pre-trained parameters remain unchanged, and stabilizes performance relative to other fine-tuning techniques, robustly generalizing across most random seeds.",
    "published": "Oct 22",
    "pdf_url": "https://arxiv.org/pdf/2510.19778v1",
    "arxiv_url": "http://arxiv.org/abs/2510.19778v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "TowerVision: Understanding and Improving Multilinguality in Vision-Language Models",
    "authors": [
      "Andr\u00e9 G. Viveiros",
      "Patrick Fernandes",
      "Saul Santos",
      "Sonal Sannigrahi",
      "Emmanouil Zaranis",
      "Nuno M. Guerreiro",
      "Amin Farajian",
      "Pierre Colombo",
      "Graham Neubig",
      "Andr\u00e9 F. T. Martins"
    ],
    "summary": "Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.",
    "published": "Oct 22",
    "pdf_url": "https://arxiv.org/pdf/2510.21849v3",
    "arxiv_url": "http://arxiv.org/abs/2510.21849v3",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference",
    "authors": [
      "Xiang Liu",
      "Xuming Hu",
      "Xiaowen Chu",
      "Eunsol Choi"
    ],
    "summary": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\\%, establishing a practical path toward compute-efficient reasoning.",
    "published": "Oct 22",
    "pdf_url": "https://arxiv.org/pdf/2510.19669v2",
    "arxiv_url": "http://arxiv.org/abs/2510.19669v2",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge",
    "authors": [
      "Zhilin Wang",
      "Jaehun Jung",
      "Ximing Lu",
      "Shizhe Diao",
      "Ellie Evans",
      "Jiaqi Zeng",
      "Pavlo Molchanov",
      "Yejin Choi",
      "Jan Kautz",
      "Yi Dong"
    ],
    "summary": "Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench",
    "published": "Oct 21",
    "pdf_url": "https://arxiv.org/pdf/2510.18941v1",
    "arxiv_url": "http://arxiv.org/abs/2510.18941v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting",
    "authors": [
      "Howard Chen",
      "Noam Razin",
      "Karthik Narasimhan",
      "Danqi Chen"
    ],
    "summary": "Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.",
    "published": "Oct 21",
    "pdf_url": "https://arxiv.org/pdf/2510.18874v2",
    "arxiv_url": "http://arxiv.org/abs/2510.18874v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen",
      "Karthik Narasimhan"
    ]
  },
  {
    "title": "Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search",
    "authors": [
      "Howard Yen",
      "Ashwin Paranjape",
      "Mengzhou Xia",
      "Thejas Venkatesh",
      "Jack Hessel",
      "Danqi Chen",
      "Yuhao Zhang"
    ],
    "summary": "Long-horizon agentic search requires iteratively exploring the web over long trajectories and synthesizing information across many sources, and is the foundation for enabling powerful applications like deep research systems. In this work, we show that popular agentic search frameworks struggle to scale to long trajectories primarily due to context limitations-they accumulate long, noisy content, hit context window and tool budgets, or stop early. Then, we introduce SLIM (Simple Lightweight Information Management), a simple framework that separates retrieval into distinct search and browse tools, and periodically summarizes the trajectory, keeping context concise while enabling longer, more focused searches. On long-horizon tasks, SLIM achieves comparable performance at substantially lower cost and with far fewer tool calls than strong open-source baselines across multiple base models. Specifically, with o3 as the base model, SLIM achieves 56% on BrowseComp and 31% on HLE, outperforming all open-source frameworks by 8 and 4 absolute points, respectively, while incurring 4-6x fewer tool calls. Finally, we release an automated fine-grained trajectory analysis pipeline and error taxonomy for characterizing long-horizon agentic search frameworks; SLIM exhibits fewer hallucinations than prior systems. We hope our analysis framework and simple tool design inform future long-horizon agents.",
    "published": "Oct 21",
    "pdf_url": "https://arxiv.org/pdf/2510.18939v1",
    "arxiv_url": "http://arxiv.org/abs/2510.18939v1",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Extracting Rule-based Descriptions of Attention Features in Transformers",
    "authors": [
      "Dan Friedman",
      "Adithya Bhaskar",
      "Alexander Wettig",
      "Danqi Chen"
    ],
    "summary": "Mechanistic interpretability strives to explain model behavior in terms of bottom-up primitives. The leading paradigm is to express hidden states as a sparse linear combination of basis vectors, called features. However, this only identifies which text sequences (exemplars) activate which features; the actual interpretation of features requires subjective inspection of these exemplars. This paper advocates for a different solution: rule-based descriptions that match token patterns in the input and correspondingly increase or decrease the likelihood of specific output tokens. Specifically, we extract rule-based descriptions of SAE features trained on the outputs of attention layers. While prior work treats the attention layers as an opaque box, we describe how it may naturally be expressed in terms of interactions between input and output features, of which we study three types: (1) skip-gram rules of the form \"[Canadian city]... speaks --> English\", (2) absence rules of the form \"[Montreal]... speaks -/-> English,\" and (3) counting rules that toggle only when the count of a word exceeds a certain value or the count of another word. Absence and counting rules are not readily discovered by inspection of exemplars, where manual and automatic descriptions often identify misleading or incomplete explanations. We then describe a simple approach to extract these types of rules automatically from a transformer, and apply it to GPT-2 small. We find that a majority of features may be described well with around 100 skip-gram rules, though absence rules are abundant even as early as th...",
    "published": "Oct 20",
    "pdf_url": "https://arxiv.org/pdf/2510.18148v1",
    "arxiv_url": "http://arxiv.org/abs/2510.18148v1",
    "queried_author": "Alexander Wettig",
    "matching_authors": [
      "Alexander Wettig",
      "Danqi Chen"
    ]
  },
  {
    "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations",
    "authors": [
      "Tong Chen",
      "Akari Asai",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi",
      "Faeze Brahman"
    ],
    "summary": "Language models often generate factually incorrect information unsupported by their training data, a phenomenon known as extrinsic hallucination. Existing mitigation approaches often degrade performance on open-ended generation and downstream tasks, limiting their practical utility. We propose an online reinforcement learning method using a novel binary retrieval-augmented reward (RAR) to address this tradeoff. Unlike continuous reward schemes, our approach assigns a reward of one only when the model's output is entirely factually correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models across diverse tasks. For open-ended generation, binary RAR achieves a 39.3% reduction in hallucination rates, substantially outperforming both supervised training and continuous-reward RL baselines. In short-form question answering, the model learns calibrated abstention, strategically outputting \"I don't know\" when faced with insufficient parametric knowledge. This yields 44.4% and 21.7% fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these factuality gains come without performance degradation on instruction following, math, or code, whereas continuous-reward RL, despite improving factuality, induces quality regressions.",
    "published": "Oct 20",
    "pdf_url": "https://arxiv.org/pdf/2510.17733v1",
    "arxiv_url": "http://arxiv.org/abs/2510.17733v1",
    "queried_author": "Akari Asai",
    "matching_authors": [
      "Akari Asai",
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs",
    "authors": [
      "Emily Xiao",
      "Yixiao Zeng",
      "Ada Chen",
      "Chin-Jou Li",
      "Amanda Bertsch",
      "Graham Neubig"
    ],
    "summary": "A popular method to adapt large language models (LLMs) to new tasks is in-context learning (ICL), which is effective but incurs high inference costs as context length grows. In this paper we propose a method to perform instruction induction, where we take training examples and reduce them to a compact but descriptive prompt that can achieve performance comparable to ICL over the full training set. Specifically, we propose PROMPT-MII, a reinforcement learning (RL) based framework to meta-learn an instruction induction model that can generate compact instructions on the fly for an arbitrary new dataset. We train on over 3,000 diverse classification datasets from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves downstream model quality by 4-9 F1 points (10-20% relative), matching ICL performance while requiring 3-13x fewer tokens.",
    "published": "Oct 19",
    "pdf_url": "https://arxiv.org/pdf/2510.16932v2",
    "arxiv_url": "http://arxiv.org/abs/2510.16932v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
    "authors": [
      "Kangrui Wang",
      "Pingyue Zhang",
      "Zihan Wang",
      "Yaning Gao",
      "Linjie Li",
      "Qineng Wang",
      "Hanyang Chen",
      "Chi Wan",
      "Yiping Lu",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Ranjay Krishna",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Yejin Choi",
      "Manling Li"
    ],
    "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation (\"what is the current state?\") and Transition Modeling (\"what comes next?\") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude ...",
    "published": "Oct 19",
    "pdf_url": "https://arxiv.org/pdf/2510.16907v1",
    "arxiv_url": "http://arxiv.org/abs/2510.16907v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes",
    "authors": [
      "Yu Ying Chiu",
      "Michael S. Lee",
      "Rachel Calcott",
      "Brandon Handoko",
      "Paul de Font-Reaulx",
      "Paula Rodriguez",
      "Chen Bo Calvin Zhang",
      "Ziwen Han",
      "Udari Madhushani Sehwag",
      "Yash Maurya",
      "Christina Q Knight",
      "Harry R. Lloyd",
      "Florence Bacus",
      "Mantas Mazeika",
      "Bing Liu",
      "Yejin Choi",
      "Mitchell L Gordon",
      "Sydney Levine"
    ],
    "summary": "As AI systems progress, we rely more on them to make decisions with us and for us. To ensure that such decisions are aligned with human values, it is imperative for us to understand not only what decisions they make but also how they come to those decisions. Reasoning language models, which provide both final responses and (partially transparent) intermediate thinking traces, present a timely opportunity to study AI procedural reasoning. Unlike math and code problems which often have objectively correct answers, moral dilemmas are an excellent testbed for process-focused evaluation because they allow for multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral scenarios, each paired with a set of rubric criteria that experts consider essential to include (or avoid) when reasoning about the scenarios. MoReBench contains over 23 thousand criteria including identifying moral considerations, weighing trade-offs, and giving actionable recommendations to cover cases on AI advising humans moral decisions as well as making moral decisions autonomously. Separately, we curate MoReBench-Theory: 150 examples to test whether AI can reason under five major frameworks in normative ethics. Our results show that scaling laws and existing benchmarks on math, code, and scientific reasoning tasks fail to predict models' abilities to perform moral reasoning. Models also show partiality towards specific moral frameworks (e.g., Benthamite Act Utilitarianism and Kantian Deontology), which might be side effects of popular training paradigms. Together, these benchmarks advance pr...",
    "published": "Oct 18",
    "pdf_url": "https://arxiv.org/pdf/2510.16380v1",
    "arxiv_url": "http://arxiv.org/abs/2510.16380v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve strong performance via extended chains of thought but often generate unnecessarily long outputs. Maximizing intelligence per token--accuracy relative to response length--remains an open problem. We revisit reinforcement learning (RL) with the simplest length penalty--truncation--and show that accuracy degradation arises not from the lack of sophisticated penalties but from inadequate RL optimization. We identify three key challenges: (i) large bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward signal. We address them with Doing Length pEnalty Right (DLER), a training recipe combining batch-wise reward normalization, higher clipping, dynamic sampling, and a simple truncation length penalty. DLER achieves state-of-the-art accuracy--efficiency trade-offs, cutting output length by over 70 percent while surpassing all previous baseline accuracy. It also improves test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple concise responses in parallel with 28 percent higher accuracy and lower latency. We further introduce Difficulty-Aware DLER, which adaptively tightens truncation on easier questions for additional efficiency gains. We also propose an update-selective merging method that preserves baseline accuracy while retaining the concise reasoning ability of the DLER model, which is useful for scenarios where RL training data is scarce.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.15110v1",
    "arxiv_url": "http://arxiv.org/abs/2510.15110v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Continual Learning via Sparse Memory Finetuning",
    "authors": [
      "Jessy Lin",
      "Luke Zettlemoyer",
      "Gargi Ghosh",
      "Wen-Tau Yih",
      "Aram Markosyan",
      "Vincent-Pierre Berges",
      "Barlas O\u011fuz"
    ],
    "summary": "Modern language models are powerful, but typically static after deployment. A major obstacle to building models that continually learn over time is catastrophic forgetting, where updating on new data erases previously acquired capabilities. Motivated by the intuition that mitigating forgetting is challenging because trainable parameters are shared across all tasks, we investigate whether sparse parameter updates can enable learning without catastrophic forgetting. We introduce sparse memory finetuning, leveraging memory layer models (Berges et al., 2024), which are sparsely updated by design. By updating only the memory slots that are highly activated by a new piece of knowledge relative to usage on pretraining data, we reduce interference between new knowledge and the model's existing capabilities. We evaluate learning and forgetting compared to full finetuning and parameter-efficient finetuning with LoRA on two question answering tasks. We find that sparse memory finetuning learns new knowledge while exhibiting substantially less forgetting: while NaturalQuestions F1 drops by 89% after full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields only an 11% drop with the same level of new knowledge acquisition. Our results suggest sparsity in memory layers offers a promising path toward continual learning in large language models.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.15103v1",
    "arxiv_url": "http://arxiv.org/abs/2510.15103v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data",
    "authors": [
      "Alana Renda",
      "Jillian Ross",
      "Michael Cafarella",
      "Jacob Andreas"
    ],
    "summary": "Real-world settings where language models (LMs) are deployed -- in domains spanning healthcare, finance, and other forms of knowledge work -- require models to grapple with incomplete information and reason under uncertainty. Yet most LM evaluations focus on problems with well-defined answers and success criteria. This gap exists in part because natural problems involving uncertainty are difficult to construct: given that LMs have access to most of the same knowledge as humans, it is non-trivial to design questions for which LMs will struggle to produce correct answers, but which humans can answer reliably. As a result, LM performance on reasoning under uncertainty remains poorly characterized. To address this gap, we introduce OpenEstimate, an extensible, multi-domain benchmark for evaluating LMs on numerical estimation tasks that require models to synthesize significant amounts of background information and express predictions as probabilistic priors. We assess these priors for accuracy and calibration, quantifying their usefulness relative to samples from the true distribution of interest. Across six frontier LMs, we find that LM-elicited priors are often inaccurate and overconfident. Performance improves modestly depending on how uncertainty is elicited from the model, but is largely unaffected by changes in sampling strategy, reasoning effort, or prompt design. The OpenEstimate benchmark thus offers a challenging evaluation for frontier LMs and a platform for developing models that are better at probabilistic estimation and reasoning under uncertainty.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.15096v1",
    "arxiv_url": "http://arxiv.org/abs/2510.15096v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Midtraining Bridges Pretraining and Posttraining Distributions",
    "authors": [
      "Emmy Liu",
      "Graham Neubig",
      "Chenyan Xiong"
    ],
    "summary": "Midtraining, the practice of mixing specialized data with more general pretraining data in an intermediate training phase, has become widespread in language model development, yet there is little understanding of what makes it effective. We propose that midtraining functions as distributional bridging by providing better initialization for posttraining. We conduct controlled pretraining experiments, and find that midtraining benefits are largest for domains distant from general pretraining data, such as code and math, and scale with the proximity advantage the midtraining data provides toward the target distribution. In these domains, midtraining consistently outperforms continued pretraining on specialized data alone both in-domain and in terms of mitigating forgetting. We further conduct an investigation on the starting time and mixture weight of midtraining data, using code as a case study, and find that time of introduction and mixture weight interact strongly such that early introduction of specialized data is amenable to high mixture weights, while late introduction requires lower ones. This suggests that late introduction of specialized data outside a plasticity window cannot be compensated for by increasing data mixtures later in training. Beyond midtraining itself, this suggests that distributional transitions between any training phases may benefit from similar bridging strategies.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.14865v2",
    "arxiv_url": "http://arxiv.org/abs/2510.14865v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
    "authors": [
      "Michelle S. Lam",
      "Omar Shaikh",
      "Hallie Xu",
      "Alice Guo",
      "Diyi Yang",
      "Jeffrey Heer",
      "James A. Landay",
      "Michael S. Bernstein"
    ],
    "summary": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.14591v1",
    "arxiv_url": "http://arxiv.org/abs/2510.14591v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking",
    "authors": [
      "Sathyanarayanan Ramamoorthy",
      "Vishwa Shah",
      "Simran Khanuja",
      "Zaid Sheikh",
      "Shan Jie",
      "Ann Chia",
      "Shearman Chua",
      "Graham Neubig"
    ],
    "summary": "This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.14307v1",
    "arxiv_url": "http://arxiv.org/abs/2510.14307v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior",
    "authors": [
      "Rahul Nadkarni",
      "Yanai Elazar",
      "Hila Gonen",
      "Noah A. Smith"
    ],
    "summary": "We present an experimental recipe for studying the relationship between training data and language model (LM) behavior. We outline steps for intervening on data batches -- i.e., ``rewriting history'' -- and then retraining model checkpoints over that data to test hypotheses relating data to behavior. Our recipe breaks down such an intervention into stages that include selecting evaluation items from a benchmark that measures model behavior, matching relevant documents to those items, and modifying those documents before retraining and measuring the effects. We demonstrate the utility of our recipe through case studies on factual knowledge acquisition in LMs, using both cooccurrence statistics and information retrieval methods to identify documents that might contribute to knowledge learning. Our results supplement past observational analyses that link cooccurrence to model behavior, while demonstrating that extant methods for identifying relevant training documents do not fully explain an LM's ability to correctly answer knowledge questions. Overall, we outline a recipe that researchers can follow to test further hypotheses about how training data affects model behavior. Our code is made publicly available to promote future work.",
    "published": "Oct 16",
    "pdf_url": "https://arxiv.org/pdf/2510.14261v1",
    "arxiv_url": "http://arxiv.org/abs/2510.14261v1",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
    "authors": [
      "Giovanni Monea",
      "Yair Feldman",
      "Shankar Padmanabhan",
      "Kiant\u00e9 Brantley",
      "Yoav Artzi"
    ],
    "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
    "published": "Oct 15",
    "pdf_url": "https://arxiv.org/pdf/2510.13797v3",
    "arxiv_url": "http://arxiv.org/abs/2510.13797v3",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences",
    "authors": [
      "Julian Minder",
      "Cl\u00e9ment Dumas",
      "Stewart Slocum",
      "Helena Casademunt",
      "Cameron Holmes",
      "Robert West",
      "Neel Nanda"
    ],
    "summary": "Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the...",
    "published": "Oct 14",
    "pdf_url": "https://arxiv.org/pdf/2510.13900v1",
    "arxiv_url": "http://arxiv.org/abs/2510.13900v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
    "authors": [
      "Sunny Yu",
      "Ahmad Jabbar",
      "Robert Hawkins",
      "Dan Jurafsky",
      "Myra Cheng"
    ],
    "summary": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
    "published": "Oct 14",
    "pdf_url": "https://arxiv.org/pdf/2510.12699v1",
    "arxiv_url": "http://arxiv.org/abs/2510.12699v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Chimera: State Space Models Beyond Sequences",
    "authors": [
      "Aakash Lahoti",
      "Tanya Marwah",
      "Ratish Puduppully",
      "Albert Gu"
    ],
    "summary": "Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biases--such as position embeddings in sequences and images, or random walks in graphs--to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimera's efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformer's quadratic complexity without domain-specific heuristics. These results validate Chimera's core contribution and support the idea that data topology is a powerful inductive bias across modalities.",
    "published": "Oct 14",
    "pdf_url": "https://arxiv.org/pdf/2510.12111v1",
    "arxiv_url": "http://arxiv.org/abs/2510.12111v1",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
    "authors": [
      "Sayash Kapoor",
      "Benedikt Stroebl",
      "Peter Kirgis",
      "Nitya Nadgir",
      "Zachary S Siegel",
      "Boyi Wei",
      "Tianci Xue",
      "Ziru Chen",
      "Felix Chen",
      "Saiteja Utpala",
      "Franck Ndzomga",
      "Dheeraj Oruganty",
      "Sophie Luskin",
      "Kangheng Liu",
      "Botao Yu",
      "Amit Arora",
      "Dongyoon Hahm",
      "Harsh Trivedi",
      "Huan Sun",
      "Juyong Lee",
      "Tengjun Jin",
      "Yifan Mai",
      "Yifei Zhou",
      "Yuxuan Zhu",
      "Rishi Bommasani",
      "Daniel Kang",
      "Dawn Song",
      "Peter Henderson",
      "Yu Su",
      "Percy Liang",
      "Arvind Narayanan"
    ],
    "summary": "AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.",
    "published": "Oct 13",
    "pdf_url": "https://arxiv.org/pdf/2510.11977v1",
    "arxiv_url": "http://arxiv.org/abs/2510.11977v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Peter Henderson"
    ]
  },
  {
    "title": "Data or Language Supervision: What Makes CLIP Better than DINO?",
    "authors": [
      "Yiming Liu",
      "Yuhui Zhang",
      "Dhruba Ghosh",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ],
    "summary": "CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar ImageNet accuracy. Embedding analysis shows that CLIP captures high-level semantics (e.g., object categories, text), while DINO is more responsive to low-level features like colors and styles. When integrated into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive tasks, while DINO slightly outperforms on vision-centric ones. Variants of language supervision (e.g., sigmoid loss, pre-trained language encoders) yield limited gains. Our findings provide scientific insights into vision encoder design and its impact on VLM performance.",
    "published": "Oct 13",
    "pdf_url": "https://arxiv.org/pdf/2510.11835v1",
    "arxiv_url": "http://arxiv.org/abs/2510.11835v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors",
    "authors": [
      "Alexis Ross",
      "Jacob Andreas"
    ],
    "summary": "Research on reasoning in language models (LMs) predominantly focuses on improving the correctness of their outputs. But some important applications require modeling reasoning patterns that are incorrect. For example, automated systems that can reason about and simulate student errors are useful for providing real-time feedback in the classroom or offline practice for educators-in-training. This paper presents a new method, MISTAKE, that (1) constructs high-quality synthetic examples of reasoning errors by leveraging cycle consistency between incorrect answers and latent misconceptions; and (2) uses the generated data to learn models for student simulation, misconception classification, and answer generation. We evaluate MISTAKE on three educational tasks and find that it results in (1) higher accuracy when simulating incorrect student answers based on specific misconceptions, (2) increased performance inferring latent misconceptions from observed incorrect answers, and (3) higher alignment with expert-written distractor answers when generating incorrect answers (e.g., for multiple-choice tests).",
    "published": "Oct 13",
    "pdf_url": "https://arxiv.org/pdf/2510.11502v1",
    "arxiv_url": "http://arxiv.org/abs/2510.11502v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs",
    "authors": [
      "Nikita Afonin",
      "Nikita Andriyanov",
      "Vahagn Hovhannisyan",
      "Nikhil Bageshpura",
      "Kyle Liu",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Oleg Rogov",
      "Elena Tutubalina",
      "Alexander Panchenko",
      "Mikhail Seleznyov"
    ],
    "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across four model families (Gemini, Kimi-K2, Grok, and Qwen), narrow in-context examples cause models to produce misaligned responses to benign, unrelated queries. With 16 in-context examples, EM rates range from 1\\% to 24\\% depending on model and domain, appearing with as few as 2 examples. Neither larger model scale nor explicit reasoning provides reliable protection. We formulate and test a hypothesis, which explains in-context EM as conflict between safety objectives and context-following behavior. Consistent with this, instructing models to prioritize safety reduces EM while prioritizing context-following increases it. These findings establish ICL as a previously underappreciated vector for emergent misalignment that operates without parameter modification and resists simple scaling-based solutions.",
    "published": "Oct 13",
    "pdf_url": "https://arxiv.org/pdf/2510.11288v3",
    "arxiv_url": "http://arxiv.org/abs/2510.11288v3",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting",
    "authors": [
      "Michael Y. Hu",
      "Benjamin Van Durme",
      "Jacob Andreas",
      "Harsh Jhamtani"
    ],
    "summary": "Language model (LM) agents deployed in novel environments often exhibit poor sample efficiency when learning from sequential interactions. This significantly hinders the usefulness of such agents in environments where interaction is costly (for example, when they interact with humans or reset physical systems). While a number of existing LM agent architectures incorporate various mechanisms for experience storage and reflection, they make limited use of LMs' abilities to directly generate or reason about full counterfactual trajectories. We introduce ECHO (Experience Consolidation via Hindsight Optimization), a prompting framework that adapts hindsight experience replay from reinforcement learning for language model agents. ECHO generates optimized trajectories for alternative goals that could have been achieved during failed attempts, effectively creating synthetic positive examples from unsuccessful interactions. Our approach consists of two components: a hindsight rule that uses the language model itself to identify relevant subgoals and generate optimized trajectories, and an update rule that maintains compressed trajectory representations in memory. We evaluate ECHO on stateful versions of XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a collaborative information-gathering enterprise simulation. Across both domains, ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid, it also outperforms a number of sophisticated agent architectures including Reflexion and AWM, demonstrating faster adaptation to novel environments ...",
    "published": "Oct 11",
    "pdf_url": "https://arxiv.org/pdf/2510.10304v2",
    "arxiv_url": "http://arxiv.org/abs/2510.10304v2",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Tasks",
    "authors": [
      "Adnan El Assadi",
      "Isaac Chung",
      "Roman Solomatin",
      "Niklas Muennighoff",
      "Kenneth Enevoldsen"
    ],
    "summary": "Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, though with substantial variation: models reach high performance on some datasets while struggling on notably low-resource languages. Our human annotations also reveal multiple dataset issues. We additionally benchmark nine LLMs as annotators on reranking, classification, and STS tasks, finding that they fall short of human performance (76.1% vs. 81.2%) despite offering scalability advantages. We provide human performance baselines, insights into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of results and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
    "published": "Oct 11",
    "pdf_url": "https://arxiv.org/pdf/2510.10062v3",
    "arxiv_url": "http://arxiv.org/abs/2510.10062v3",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "How can we assess human-agent interactions? Case studies in software agent design",
    "authors": [
      "Valerie Chen",
      "Rohit Malhotra",
      "Xingyao Wang",
      "Juan Michelini",
      "Xuhui Zhou",
      "Aditya Bharat Soni",
      "Hoang H. Tran",
      "Calvin Smith",
      "Ameet Talwalkar",
      "Graham Neubig"
    ],
    "summary": "LLM-powered agents are both a promising new technology and a source of complexity, where choices about models, tools, and prompting can affect their usefulness. While numerous benchmarks measure agent accuracy across domains, they mostly assume full automation, failing to represent the collaborative nature of real-world use cases. In this paper, we make two major steps towards the rigorous assessment of human-agent interactions. First, we propose PULSE, a framework for more efficient human-centric evaluation of agent designs, which comprises collecting user feedback, training an ML model to predict user satisfaction, and computing results by combining human satisfaction ratings with model-generated pseudo-labels. Second, we deploy the framework on a large-scale web platform built around the open-source software agent OpenHands, collecting in-the-wild usage data across over 15k users. We conduct case studies around how three agent design decisions -- choice of LLM backbone, planning strategy, and memory mechanisms -- impact developer satisfaction rates, yielding practical insights for software agent design. We also show how our framework can lead to more robust conclusions about agent design, reducing confidence intervals by 40% compared to a standard A/B test. Finally, we find substantial discrepancies between in-the-wild results and benchmark performance (e.g., the anti-correlation between results comparing claude-sonnet-4 and gpt-5), underscoring the limitations of benchmark-driven evaluation. Our findings provide guidance for evaluations of LLM agents with humans and ide...",
    "published": "Oct 10",
    "pdf_url": "https://arxiv.org/pdf/2510.09801v2",
    "arxiv_url": "http://arxiv.org/abs/2510.09801v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Attention to Non-Adopters",
    "authors": [
      "Kaitlyn Zhou",
      "Kristina Gligori\u0107",
      "Myra Cheng",
      "Michelle S. Lam",
      "Vyoma Raman",
      "Boluwatife Aminu",
      "Caeley Woo",
      "Michael Brockman",
      "Hannah Cha",
      "Dan Jurafsky"
    ],
    "summary": "Although language model-based chat systems are increasingly used in daily life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025, 66% had never used ChatGPT. At the same time, LLM development and evaluation rely mainly on data from adopters (e.g., logs, preference data), focusing on the needs and tasks for a limited demographic group of adopters in terms of geographic location, education, and gender. In this position paper, we argue that incorporating non-adopter perspectives is essential for developing broadly useful and capable LLMs. We contend that relying on methods that focus primarily on adopters will risk missing a range of tasks and needs prioritized by non-adopters, entrenching inequalities in who benefits from LLMs, and creating oversights in model development and evaluation. To illustrate this claim, we conduct case studies with non-adopters and show: how non-adopter needs diverge from those of current users, how non-adopter needs point us towards novel reasoning tasks, and how to systematically integrate non-adopter needs via human-centered methods.",
    "published": "Oct 10",
    "pdf_url": "https://arxiv.org/pdf/2510.15951v1",
    "arxiv_url": "http://arxiv.org/abs/2510.15951v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows",
    "authors": [
      "Hamed Mahdavi",
      "Pouria Mahdavinia",
      "Samira Malek",
      "Pegah Mohammadipour",
      "Alireza Hashemi",
      "Majid Daliri",
      "Alireza Farhadi",
      "Amir Khasahmadi",
      "Niloofar Mireshghallah",
      "Vasant Honavar"
    ],
    "summary": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.",
    "published": "Oct 10",
    "pdf_url": "https://arxiv.org/pdf/2510.09021v1",
    "arxiv_url": "http://arxiv.org/abs/2510.09021v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "Neologism Learning for Controllability and Self-Verbalization",
    "authors": [
      "John Hewitt",
      "Oyvind Tafjord",
      "Robert Geirhos",
      "Been Kim"
    ],
    "summary": "Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.",
    "published": "Oct 09",
    "pdf_url": "https://arxiv.org/pdf/2510.08506v1",
    "arxiv_url": "http://arxiv.org/abs/2510.08506v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt"
    ]
  },
  {
    "title": "Base Models Know How to Reason, Thinking Models Learn When",
    "authors": [
      "Constantin Venhoff",
      "Iv\u00e1n Arcuschin",
      "Philip Torr",
      "Arthur Conmy",
      "Neel Nanda"
    ],
    "summary": "Why do thinking language models like DeepSeek R1 outperform their base counterparts? Despite consistent performance gains, it remains unclear to what extent thinking models learn entirely new reasoning capabilities or repurpose pre-existing base model ones. In this work, we propose a hybrid model where we activate reasoning mechanisms in base models at the right time to elicit thinking-model-level reasoning chains, implying that thinking models exploit already existing capabilities. To ground our analysis, we introduce an unsupervised, bottom-up approach for uncovering human-interpretable reasoning behaviors in thinking models. This approach provides an unbiased method to discover reasoning behaviors without imposing manual or LLM-derived assumptions. Across three base and four thinking models, using GSM8K and MATH500, our hybrid model recovers up to 91% of the performance gap to thinking models without any weight updates while steering only 12% of tokens. Concretely, our empirical setup provides a simple, causal way to test the effectiveness of existing reasoning mechanisms in base models by invoking them directly and measuring the resulting task performance. More broadly, these results reframe our understanding of how thinking models are trained: pre-training is when models acquire most of their reasoning mechanisms, and post-training teaches efficient deployment of these mechanisms at the right time, enabling efficient use of their inference-time compute.",
    "published": "Oct 08",
    "pdf_url": "https://arxiv.org/pdf/2510.07364v3",
    "arxiv_url": "http://arxiv.org/abs/2510.07364v3",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
    "authors": [
      "Rushi Qiang",
      "Yuchen Zhuang",
      "Anikait Singh",
      "Percy Liang",
      "Chao Zhang",
      "Sherry Yang",
      "Bo Dai"
    ],
    "summary": "While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. We introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. We apply MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality.",
    "published": "Oct 08",
    "pdf_url": "https://arxiv.org/pdf/2510.07307v1",
    "arxiv_url": "http://arxiv.org/abs/2510.07307v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
    "authors": [
      "Taylor Sorensen",
      "Yejin Choi"
    ],
    "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.",
    "published": "Oct 08",
    "pdf_url": "https://arxiv.org/pdf/2510.07105v1",
    "arxiv_url": "http://arxiv.org/abs/2510.07105v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?",
    "authors": [
      "Aochong Oliver Li",
      "Tanya Goyal"
    ],
    "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajectory, yielding better inference efficiency and exploration. A key prerequisite, however, is the ability to assess the usefulness and build on another model's partial thinking -- we call this off-trajectory reasoning. Our paper investigates a critical question: can standard solo-reasoning training pipelines deliver desired off-trajectory behaviors? We propose twin tests that capture the two extremes of the off-trajectory spectrum, namely Recoverability, which tests whether LLMs can backtrack from \"distractions\" induced by misleading reasoning traces, and Guidability, which tests their ability to build upon correct reasoning from stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and reveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often more fragile under distraction. Moreover, all models tested fail to effectively leverage guiding steps from collaborators on problems beyond their inherent capabilities with solve rates remaining under 9.2%. Finally, we conduct control studies to isolate the effects of three factors in post-training on these behaviors: the choice of distillation teacher, the use of RL, and data selection strategy. Our results provide actionable insights for training natively strong reasoning collaborators; e.g., we find that suboptimal recoverability behaviors of teache...",
    "published": "Oct 07",
    "pdf_url": "https://arxiv.org/pdf/2510.06410v1",
    "arxiv_url": "http://arxiv.org/abs/2510.06410v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences",
    "authors": [
      "Kshitish Ghate",
      "Andy Liu",
      "Devansh Jain",
      "Taylor Sorensen",
      "Atoosa Kasirzadeh",
      "Aylin Caliskan",
      "Mona T. Diab",
      "Maarten Sap"
    ],
    "summary": "As large language models (LLMs) are deployed globally, creating pluralistic systems that can accommodate the diverse preferences and values of users worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure LLMs' and reward models' (RMs) steerability towards users' value and stylistic preference profiles grounded in psychology and human-LLM interaction literature. To address the gap in existing datasets that do not support controlled evaluations of RM steering, we synthetically generated 165,888 preference pairs -- systematically varying pairs along 4 value dimensions (traditional, secular-rational, survival, and self-expression) and 4 style dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER to evaluate whether, given a user profile and a pair of candidate value-laden and style-laden responses, LLMs and RMs are able to select the output that aligns with the user's preferences. We evaluate six open-source and proprietary LLMs and RMs under eleven systematic prompting conditions and six preference comparison scenarios. Notably, our results show that, when given the user's full profile of values and stylistic preferences, the best models achieve <75% accuracy at choosing the correct response, in contrast to >99% accuracy when only relevant style and value preferences are provided. EVALUESTEER thus highlights the limitations of current RMs at identifying and adapting to relevant user profile information, and provides a challenging testbed for developing RMs that can be steered towards diverse human values and preferences.",
    "published": "Oct 07",
    "pdf_url": "https://arxiv.org/pdf/2510.06370v2",
    "arxiv_url": "http://arxiv.org/abs/2510.06370v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Latent Speech-Text Transformer",
    "authors": [
      "Yen-Ju Lu",
      "Yashesh Gaur",
      "Wei Zhou",
      "Benjamin Muller",
      "Jesus Villalba",
      "Najim Dehak",
      "Luke Zettlemoyer",
      "Gargi Ghosh",
      "Mike Lewis",
      "Srinivasan Iyer",
      "Duc Le"
    ],
    "summary": "Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech a...",
    "published": "Oct 07",
    "pdf_url": "https://arxiv.org/pdf/2510.06195v1",
    "arxiv_url": "http://arxiv.org/abs/2510.06195v1",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer",
      "Mike Lewis"
    ]
  },
  {
    "title": "Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability",
    "authors": [
      "Taylor Sorensen",
      "Benjamin Newman",
      "Jared Moore",
      "Chan Park",
      "Jillian Fisher",
      "Niloofar Mireshghallah",
      "Liwei Jiang",
      "Yejin Choi"
    ],
    "summary": "Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata for conditional distributional modeling: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how current post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from >40 data sources and spanning >90 tasks requiring models to steer to and match diverse distributions ranging from varied human preferences to numerical distributions and more. We find that while current post-training techniques help elicit underlying capabilities and knowledge, they hurt models' ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum Tuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the output space, and improving distributional alignment on held-out datasets.",
    "published": "Oct 07",
    "pdf_url": "https://arxiv.org/pdf/2510.06084v1",
    "arxiv_url": "http://arxiv.org/abs/2510.06084v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah",
      "Yejin Choi"
    ]
  },
  {
    "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool Use",
    "authors": [
      "Zhuofeng Li",
      "Haoxiang Zhang",
      "Seungju Han",
      "Sheng Liu",
      "Jianwen Xie",
      "Yu Zhang",
      "Yejin Choi",
      "James Zou",
      "Pan Lu"
    ],
    "summary": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-callin...",
    "published": "Oct 07",
    "pdf_url": "https://arxiv.org/pdf/2510.05592v1",
    "arxiv_url": "http://arxiv.org/abs/2510.05592v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Learning to Interpret Weight Differences in Language Models",
    "authors": [
      "Avichal Goel",
      "Yoon Kim",
      "Nir Shavit",
      "Tony T. Wang"
    ],
    "summary": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT-adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
    "published": "Oct 06",
    "pdf_url": "https://arxiv.org/pdf/2510.05092v3",
    "arxiv_url": "http://arxiv.org/abs/2510.05092v3",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Efficient Prediction of Pass@k Scaling in Large Language Models",
    "authors": [
      "Joshua Kazdan",
      "Rylan Schaeffer",
      "Youssef Allouah",
      "Colin Sullivan",
      "Kyssen Yu",
      "Noam Levi",
      "Sanmi Koyejo"
    ],
    "summary": "Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.",
    "published": "Oct 06",
    "pdf_url": "https://arxiv.org/pdf/2510.05197v1",
    "arxiv_url": "http://arxiv.org/abs/2510.05197v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Modeling Student Learning with 3.8 Million Program Traces",
    "authors": [
      "Alexis Ross",
      "Megha Srivastava",
      "Jeremiah Blanchard",
      "Jacob Andreas"
    ],
    "summary": "As programmers write code, they often edit and retry multiple times, creating rich \"interaction traces\" that reveal how they approach coding tasks and provide clues about their level of skill development. For novice programmers in particular, these traces reflect the diverse reasoning processes they employ to code, such as exploratory behavior to understand how a programming concept works, re-strategizing in response to bugs, and personalizing stylistic choices. In this work, we explore what can be learned from training language models on such reasoning traces: not just about code, but about coders, and particularly students learning to program. We introduce a dataset of over 3.8 million programming reasoning traces from users of Pencil Code, a free online educational platform used by students to learn simple programming concepts. Compared to models trained only on final programs or synthetically-generated traces, we find that models trained on real traces are stronger at modeling diverse student behavior. Through both behavioral and probing analyses, we also find that many properties of code traces, such as goal backtracking or number of comments, can be predicted from learned representations of the students who write them. Building on this result, we show that we can help students recover from mistakes by steering code generation models to identify a sequence of edits that will results in more correct code while remaining close to the original student's style. Together, our results suggest that many properties of code are properties of individual students and that trainin...",
    "published": "Oct 06",
    "pdf_url": "https://arxiv.org/pdf/2510.05056v1",
    "arxiv_url": "http://arxiv.org/abs/2510.05056v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Agentic Misalignment: How LLMs Could Be Insider Threats",
    "authors": [
      "Aengus Lynch",
      "Benjamin Wright",
      "Caleb Larson",
      "Stuart J. Ritchie",
      "Soren Mindermann",
      "Evan Hubinger",
      "Ethan Perez",
      "Kevin Troy"
    ],
    "summary": "We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei...",
    "published": "Oct 05",
    "pdf_url": "https://arxiv.org/pdf/2510.05179v2",
    "arxiv_url": "http://arxiv.org/abs/2510.05179v2",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning",
    "authors": [
      "Zhanke Zhou",
      "Chentao Cao",
      "Xiao Feng",
      "Xuan Li",
      "Zongze Li",
      "Xiangyu Lu",
      "Jiangchao Yao",
      "Weikai Huang",
      "Linrui Xu",
      "Tian Cheng",
      "Guanyu Jiang",
      "Yiming Zheng",
      "Brando Miranda",
      "Tongliang Liu",
      "Sanmi Koyejo",
      "Masashi Sugiyama",
      "Bo Han"
    ],
    "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.",
    "published": "Oct 05",
    "pdf_url": "https://arxiv.org/pdf/2510.06261v1",
    "arxiv_url": "http://arxiv.org/abs/2510.06261v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Internal states before wait modulate reasoning patterns",
    "authors": [
      "Dmitrii Troitskii",
      "Koyena Pal",
      "Chris Wendler",
      "Callum Stuart McDougall",
      "Neel Nanda"
    ],
    "summary": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
    "published": "Oct 05",
    "pdf_url": "https://arxiv.org/pdf/2510.04128v1",
    "arxiv_url": "http://arxiv.org/abs/2510.04128v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
    "authors": [
      "Jijie Zhou",
      "Niloofar Mireshghallah",
      "Tianshi Li"
    ],
    "summary": "The rapid deployment of large language models (LLMs) in consumer applications has led to frequent exchanges of personal information. To obtain useful responses, users often share more than necessary, increasing privacy risks via memorization, context-based personalization, or security breaches. We present a framework to formally define and operationalize data minimization: for a given user prompt and response model, quantifying the least privacy-revealing disclosure that maintains utility, and we propose a priority-queue tree search to locate this optimal point within a privacy-ordered transformation space. We evaluated the framework on four datasets spanning open-ended conversations (ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth answers (CaseHold, MedQA), quantifying achievable data minimization with nine LLMs as the response model. Our results demonstrate that larger frontier LLMs can tolerate stronger data minimization while maintaining task quality than smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that LLMs struggle to predict optimal data minimization directly, showing a bias toward abstraction that leads to oversharing. This suggests not just a privacy gap, but a capability gap: models may lack awareness of what information they actually need to solve a task.",
    "published": "Oct 04",
    "pdf_url": "https://arxiv.org/pdf/2510.03662v1",
    "arxiv_url": "http://arxiv.org/abs/2510.03662v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows",
    "authors": [
      "John Nguyen",
      "Marton Havasi",
      "Tariq Berrada",
      "Luke Zettlemoyer",
      "Ricky T. Q. Chen"
    ],
    "summary": "We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.",
    "published": "Oct 03",
    "pdf_url": "https://arxiv.org/pdf/2510.03506v3",
    "arxiv_url": "http://arxiv.org/abs/2510.03506v3",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition",
    "authors": [
      "Xiaoyan Bai",
      "Aryan Shrivastava",
      "Ari Holtzman",
      "Chenhao Tan"
    ],
    "summary": "Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.",
    "published": "Oct 03",
    "pdf_url": "https://arxiv.org/pdf/2510.03399v1",
    "arxiv_url": "http://arxiv.org/abs/2510.03399v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
    "authors": [
      "Wanjia Zhao",
      "Qinwei Ma",
      "Jingzhe Shi",
      "Shirley Wu",
      "Jiaqi Han",
      "Yijia Xiao",
      "Si-Yuan Chen",
      "Xiao Luo",
      "Ludwig Schmidt",
      "James Zou"
    ],
    "summary": "Benchmarks for competition-style reasoning have advanced evaluation in mathematics and programming, yet physics remains comparatively explored. Most existing physics benchmarks evaluate only final answers, which fail to capture reasoning processes, while recent stepwise methods rely on heuristic LLM-as-judge scoring or restrictive linear assumptions, limiting reliability and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. We prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human experts' scoring. Experiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities.",
    "published": "Oct 03",
    "pdf_url": "https://arxiv.org/pdf/2510.03185v2",
    "arxiv_url": "http://arxiv.org/abs/2510.03185v2",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models",
    "authors": [
      "Tol\u00falop\u00e9 \u00d2g\u00fanr\u00e8m\u00ed",
      "Christopher D. Manning",
      "Dan Jurafsky",
      "Karen Livescu"
    ],
    "summary": "Spoken language models (SLMs) that integrate speech with large language models (LMs) rely on modality adapters (MAs) to map the output of speech encoders to a representation that is understandable to the decoder LM. Yet we know very little about how these crucial MAs transform representations. Here we examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA representation, we uncover two strategies for MA representations. For models using a Whisper encoder, MAs appear to represent the meaning of the input using an English-based interlingua, allowing them to handle languages unseen in instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs instead represent the phonetics of the input, but expressed with English words. We hypothesise that which arises depends on whether the speech encoder is trained only for speech recognition or also for translation.",
    "published": "Oct 02",
    "pdf_url": "https://arxiv.org/pdf/2510.02569v2",
    "arxiv_url": "http://arxiv.org/abs/2510.02569v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning",
      "Dan Jurafsky"
    ]
  },
  {
    "title": "Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models",
    "authors": [
      "Alexa R. Tartaglini",
      "Satchel Grant",
      "Daniel Wurgaft",
      "Christopher Potts",
      "Judith E. Fan"
    ],
    "summary": "Data visualizations are vital components of many scientific articles and news stories. Current vision-language models (VLMs) still struggle on basic data visualization understanding tasks, but the causes of failure remain unclear. Are VLM failures attributable to limitations in how visual information in the data visualization is encoded, how information is transferred between the vision and language modules, or how information is processed within the language module? We developed FUGU, a suite of data visualization understanding tasks, to precisely characterize potential sources of difficulty (e.g., extracting the position of data points, distances between them, and other summary statistics). We used FUGU to investigate three widely used VLMs. To diagnose the sources of errors produced by these models, we used activation patching and linear probes to trace information flow through models across a variety of prompting strategies. We found that some models fail to generate the coordinates of individual data points correctly, and these initial errors often lead to erroneous final responses. When these models are provided with the correct coordinates, performance improves substantially. Moreover, even when the model generates an incorrect response, the correct coordinates can be successfully read out from the latent representations in the vision encoder, suggesting that the source of these errors lies in the vision-language handoff. We further found that while providing correct coordinates helps with tasks involving one or a small number of data points, it generally worsens per...",
    "published": "Oct 02",
    "pdf_url": "https://arxiv.org/pdf/2510.21740v1",
    "arxiv_url": "http://arxiv.org/abs/2510.21740v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Position: Privacy Is Not Just Memorization!",
    "authors": [
      "Niloofar Mireshghallah",
      "Tianshi Li"
    ],
    "summary": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
    "published": "Oct 02",
    "pdf_url": "https://arxiv.org/pdf/2510.01645v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01645v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?",
    "authors": [
      "Hanqun Cao",
      "Hongrui Zhang",
      "Junde Xu",
      "Zhou Zhang",
      "Lingdong Shen",
      "Minghao Sun",
      "Ge Liu",
      "Jinbo Xu",
      "Wu-Jun Li",
      "Jinren Ni",
      "Cesar de la Fuente-Nunez",
      "Tianfan Fu",
      "Yejin Choi",
      "Pheng-Ann Heng",
      "Fang Wu"
    ],
    "summary": "Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.",
    "published": "Oct 02",
    "pdf_url": "https://arxiv.org/pdf/2510.01571v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01571v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
    "authors": [
      "Isha Gupta",
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "Ken Ziyu Liu",
      "Sanmi Koyejo"
    ],
    "summary": "The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks can transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus ...",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01494v2",
    "arxiv_url": "http://arxiv.org/abs/2510.01494v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence",
    "authors": [
      "Myra Cheng",
      "Cinoo Lee",
      "Pranav Khadpe",
      "Sunny Yu",
      "Dyllan Han",
      "Dan Jurafsky"
    ],
    "summary": "Both the general public and academic communities have raised concerns about sycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing with or flattering users. Yet, beyond isolated media reports of severe consequences, like reinforcing delusions, little is known about the extent of sycophancy or how it affects people who use AI. Here we show the pervasiveness and harmful impacts of sycophancy when people seek advice from AI. First, across 11 state-of-the-art AI models, we find that models are highly sycophantic: they affirm users' actions 50% more than humans do, and they do so even in cases where user queries mention manipulation, deception, or other relational harms. Second, in two preregistered experiments (N = 1604), including a live-interaction study where participants discuss a real interpersonal conflict from their life, we find that interaction with sycophantic AI models significantly reduced participants' willingness to take actions to repair interpersonal conflict, while increasing their conviction of being in the right. However, participants rated sycophantic responses as higher quality, trusted the sycophantic AI model more, and were more willing to use it again. This suggests that people are drawn to AI that unquestioningly validate, even as that validation risks eroding their judgment and reducing their inclination toward prosocial behavior. These preferences create perverse incentives both for people to increasingly rely on sycophantic AI models and for AI model training to favor sycophancy. Our findings highlight the necessity of explic...",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01395v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01395v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
    "authors": [
      "Jian Hu",
      "Mingjie Liu",
      "Ximing Lu",
      "Fang Wu",
      "Zaid Harchaoui",
      "Shizhe Diao",
      "Yejin Choi",
      "Pavlo Molchanov",
      "Jun Yang",
      "Jan Kautz",
      "Yi Dong"
    ],
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps...",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01180v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01180v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
    "authors": [
      "Jiayi Zhang",
      "Simon Yu",
      "Derek Chong",
      "Anthony Sicilia",
      "Michael R. Tomz",
      "Christopher D. Manning",
      "Weiyan Shi"
    ],
    "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., \"Generate 5 jokes about coffee and their corresponding probabilities\"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01171v3",
    "arxiv_url": "http://arxiv.org/abs/2510.01171v3",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "Pay-Per-Search Models are Abstention Models",
    "authors": [
      "Mustafa Omer Gul",
      "Claire Cardie",
      "Tanya Goyal"
    ],
    "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leverage...",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01152v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01152v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "Eliciting Secret Knowledge from Language Models",
    "authors": [
      "Bartosz Cywi\u0144ski",
      "Emil Ryd",
      "Rowan Wang",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy",
      "Samuel Marks"
    ],
    "summary": "We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in all settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. Our white-box techniques based on logit lens and sparse autoencoders (SAEs) also consistently increase the success rate of the LLM auditor, but are less effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01070v2",
    "arxiv_url": "http://arxiv.org/abs/2510.01070v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "GEM: A Gym for Agentic LLMs",
    "authors": [
      "Zichen Liu",
      "Anya Sims",
      "Keyu Duan",
      "Changyu Chen",
      "Simon Yu",
      "Xiangxin Zhou",
      "Haotian Xu",
      "Shaopan Xiong",
      "Bo Liu",
      "Chenmien Tan",
      "Chuen Yang Beh",
      "Weixun Wang",
      "Hao Zhu",
      "Weiyan Shi",
      "Diyi Yang",
      "Michael Shieh",
      "Yee Whye Teh",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "summary": "The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
    "published": "Oct 01",
    "pdf_url": "https://arxiv.org/pdf/2510.01051v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01051v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space",
    "authors": [
      "Houjun Liu",
      "Shikhar Murty",
      "Christopher D. Manning",
      "R\u00f3bert Csord\u00e1s"
    ],
    "summary": "Current approaches for scaling inference-time compute in transformers train them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and rely solely on serially-generated, natural-language verbalization. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens requiring more computation can form a \"bubble\" of cloned residuals in the middle of the network. Crucially, this behavior is learned during pretraining with only language modeling loss. Using half of the training budget, Thoughtbubbles outperforms the perplexity and zero-shot evals of both standard decoder LMs and those using non-adaptive parallel computation approaches. These results hold across model sizes from 150M to 1.9B. Thoughtbubbles achieves competitive GSM8K results using half of the baseline's token budget. The implicit nature of our method enables models to begin learning adaptive computation at pretraining time, paving the way to unified train-time and test-time scaling behaviors.",
    "published": "Sep 30",
    "pdf_url": "https://arxiv.org/pdf/2510.00219v2",
    "arxiv_url": "http://arxiv.org/abs/2510.00219v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It",
    "authors": [
      "Shuyue Stella Li",
      "Avinandan Bose",
      "Faeze Brahman",
      "Simon Shaolei Du",
      "Pang Wei Koh",
      "Maryam Fazel",
      "Yulia Tsvetkov"
    ],
    "summary": "Current large language model (LLM) development treats task-solving and preference alignment as separate challenges, optimizing first for objective correctness, then for alignment to aggregated human preferences. This paradigm fails in human-facing applications where solving a problem correctly is insufficient if the response mismatches the user's needs. This challenge intensifies in just-in-time scenarios where no prior user interaction history exists due to cold-start conditions or privacy constraints. LLMs need to identify what they don't know about user preferences, strategically elicit preference values through questioning, then adapt their reasoning processes and responses accordingly -- a complicated chain of cognitive processes which we term personalized reasoning. We introduce PREFDISCO, an evaluation methodology that transforms static benchmarks into interactive personalization tasks using psychologically-grounded personas with sparse preferences. Our framework creates scenarios where identical questions require different reasoning chains depending on user context, as optimal explanation approaches vary by individual expertise and preferences while maintaining factual accuracy. Evaluation of 21 frontier models across 10 tasks reveals 29.0% of naive personalization attempts produce worse preference alignment than generic responses, yet generic responses also fail to serve individual user needs effectively. These findings suggest personalized reasoning requires dedicated development rather than emerging naturally. PREFDISCO establishes personalized reasoning as a mea...",
    "published": "Sep 30",
    "pdf_url": "https://arxiv.org/pdf/2510.00177v1",
    "arxiv_url": "http://arxiv.org/abs/2510.00177v1",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Auto-ARGUE: LLM-Based Report Generation Evaluation",
    "authors": [
      "William Walden",
      "Marc Mason",
      "Orion Weller",
      "Laura Dietz",
      "John Conroy",
      "Neil Molino",
      "Hannah Recknor",
      "Bryan Li",
      "Gabrielle Kaili-May Liu",
      "Yu Hou",
      "Dawn Lawrie",
      "James Mayfield",
      "Eugene Yang"
    ],
    "summary": "Generation of long-form, citation-backed reports is a primary use case for retrieval augmented generation (RAG) systems. While open-source evaluation tools exist for various RAG tasks, ones tailored to report generation (RG) are lacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based implementation of the recently proposed ARGUE framework for RG evaluation. We present analysis of Auto-ARGUE on the RG pilot task from the TREC 2024 NeuCLIR track, showing good system-level correlations with human judgments. We further release a web app for visualization of Auto-ARGUE outputs.",
    "published": "Sep 30",
    "pdf_url": "https://arxiv.org/pdf/2509.26184v4",
    "arxiv_url": "http://arxiv.org/abs/2509.26184v4",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
    "authors": [
      "Fang Wu",
      "Weihao Xuan",
      "Heli Qi",
      "Ximing Lu",
      "Aaron Tu",
      "Li Erran Li",
      "Yejin Choi"
    ],
    "summary": "Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These r...",
    "published": "Sep 29",
    "pdf_url": "https://arxiv.org/pdf/2509.25454v3",
    "arxiv_url": "http://arxiv.org/abs/2509.25454v3",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "AutoCode: LLMs as Problem Setters for Competitive Programming",
    "authors": [
      "Shang Zhou",
      "Zihan Zheng",
      "Kaiyuan Liu",
      "Zeyu Shen",
      "Zerui Cheng",
      "Zexing Chen",
      "Hansen He",
      "Jianzhu Yao",
      "Huanzhi Mao",
      "Qiuyang Mang",
      "Tianfu Fu",
      "Beichen Li",
      "Dongruixuan Li",
      "Wenhao Chai",
      "Zhuang Liu",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Natasha Jaques",
      "Pramod Viswanath",
      "Saining Xie",
      "Jingbo Shang"
    ],
    "summary": "Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.",
    "published": "Sep 29",
    "pdf_url": "https://arxiv.org/pdf/2510.12803v1",
    "arxiv_url": "http://arxiv.org/abs/2510.12803v1",
    "queried_author": "Peter Henderson",
    "matching_authors": [
      "Peter Henderson"
    ]
  },
  {
    "title": "Humanline: Online Alignment as Perceptual Loss",
    "authors": [
      "Sijia Liu",
      "Niklas Muennighoff",
      "Kawin Ethayarajh"
    ],
    "summary": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
    "published": "Sep 29",
    "pdf_url": "https://arxiv.org/pdf/2509.24207v1",
    "arxiv_url": "http://arxiv.org/abs/2509.24207v1",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff"
    ]
  },
  {
    "title": "Pretraining Scaling Laws for Generative Evaluations of Language Models",
    "authors": [
      "Rylan Schaeffer",
      "Noam Levi",
      "Brando Miranda",
      "Sanmi Koyejo"
    ],
    "summary": "Neural scaling laws have driven the field's ever-expanding exponential growth in parameters, data and compute. While scaling behaviors for pretraining losses and discriminative benchmarks are well established, generative benchmarks such as mathematical problem-solving or software engineering remain under-explored. We propose and evaluate three different pretraining scaling laws for fitting pass-at-$k$ on generative evaluations and for predicting pass-at-$k$ of the most expensive model using cheaper models. Our three scaling laws differ in the covariates used: (1) pretraining compute, (2) model parameters and pretraining tokens, (3) log likelihoods of gold reference solutions. First, we demonstrate that generative evaluations introduce new hyperparameters (in our setting, $k$) that act as a control lever for scaling behavior, modulating both the scaling law parameters and the predictability of performance. Second, we identify a stark difference in parameter stability: while the compute and parameters+tokens laws stabilize for only the last $\\mathord{\\sim}1.5\\mathord{-}2.5$ orders of magnitude, the gold reference likelihood law is uniquely stable, converging across $\\mathord{\\sim}5$ orders. Third, in terms of predictive performance, we find all three scaling laws perform comparably, although the compute law predicts slightly worse for small $k$ and the gold reference law predicts slightly worse for large $k$. Finally, we establish a theoretical connection, proving that the compute scaling law emerges as the compute-optimal envelope of the parameters-and-tokens law. Our framew...",
    "published": "Sep 28",
    "pdf_url": "https://arxiv.org/pdf/2509.24012v2",
    "arxiv_url": "http://arxiv.org/abs/2509.24012v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling",
    "authors": [
      "Rylan Schaeffer",
      "Noam Levi",
      "Andreas Kirsch",
      "Theo Guenais",
      "Brando Miranda",
      "Elyas Obbad",
      "Sanmi Koyejo"
    ],
    "summary": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of compute-optimal scaling, laying a foundation for future scaling of language models. In the years since, however, valid concerns about Chinchilla have been raised: wide confidence intervals, discrepancies between its three approaches, and incongruities with other scaling laws. This raises a critical question for the field: Can practitioners still rely on Chinchilla's prescriptions? Our work demonstrates the answer is yes. We begin by uncovering that the model parameters central to Chinchilla's analyses were ambiguous: three interpretations are possible, with relative differences between different interpretations of model parameters as high as 15.2%. We find that, perhaps surprisingly, which model parameters are used for the analyses do not meaningfully affect key results: the scaling law estimates and the compute-optimal tokens-to-parameter ratio. Indeed, under one interpretation, the tokens-to-parameter ratio becomes more constant with the target compute budget. We then ask how distorted the Chinchilla model parameters could have been without meaningfully affecting the key results. By deliberately perturbing model parameters in four structured ways, we find that key Chinchilla results are most sensitive to additive or systematic errors, which can alter the otherwise flat trend of the optimal tokens-to-parameter ratio, but overall, Chinchilla's key results withstand sizable perturbations. Altogether, our findings offer the field renewed confidence in Chinchilla as a durable guide for scaling language models.",
    "published": "Sep 28",
    "pdf_url": "https://arxiv.org/pdf/2509.23963v1",
    "arxiv_url": "http://arxiv.org/abs/2509.23963v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
    "authors": [
      "Siyang Wu",
      "Honglin Bao",
      "Sida Li",
      "Ari Holtzman",
      "James A. Evans"
    ],
    "summary": "We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functio...",
    "published": "Sep 27",
    "pdf_url": "https://arxiv.org/pdf/2509.23488v3",
    "arxiv_url": "http://arxiv.org/abs/2509.23488v3",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Multiplayer Nash Preference Optimization",
    "authors": [
      "Fang Wu",
      "Xu Huang",
      "Weihao Xuan",
      "Zhiwei Zhang",
      "Yijia Xiao",
      "Guancheng Wan",
      "Xiaomin Li",
      "Bing Hu",
      "Peng Xia",
      "Jure Leskovec",
      "Yejin Choi"
    ],
    "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. This work introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against a population of opponents while being regularized toward a reference model. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Comprehensive empirical evaluation shows that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive ...",
    "published": "Sep 27",
    "pdf_url": "https://arxiv.org/pdf/2509.23102v2",
    "arxiv_url": "http://arxiv.org/abs/2509.23102v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data",
    "authors": [
      "Syeda Nahida Akter",
      "Shrimai Prabhumoye",
      "Eric Nyberg",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Yejin Choi",
      "Bryan Catanzaro"
    ],
    "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that nai...",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2510.03264v1",
    "arxiv_url": "http://arxiv.org/abs/2510.03264v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Adaptive Margin RLHF via Preference over Preferences",
    "authors": [
      "Yaswanth Chittepu",
      "Prasann Singhal",
      "Greg Durrett",
      "Scott Niekum"
    ],
    "summary": "Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, ...",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2509.22851v3",
    "arxiv_url": "http://arxiv.org/abs/2509.22851v3",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
    "authors": [
      "Xingyu Fu",
      "Siyi Liu",
      "Yinuo Xu",
      "Pan Lu",
      "Guangqiuse Hu",
      "Tianbo Yang",
      "Taran Anantasagar",
      "Christopher Shen",
      "Yikai Mao",
      "Yuanzhe Liu",
      "Keyush Shah",
      "Chung Un Lee",
      "Yejin Choi",
      "James Zou",
      "Dan Roth",
      "Chris Callison-Burch"
    ],
    "summary": "Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy vid...",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2509.22646v2",
    "arxiv_url": "http://arxiv.org/abs/2509.22646v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "RLP: Reinforcement as a Pretraining Objective",
    "authors": [
      "Ali Hatamizadeh",
      "Syeda Nahida Akter",
      "Shrimai Prabhumoye",
      "Jan Kautz",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Yejin Choi"
    ],
    "summary": "The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, ...",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2510.01265v1",
    "arxiv_url": "http://arxiv.org/abs/2510.01265v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers",
    "authors": [
      "Peter Shaw",
      "James Cohan",
      "Jacob Eisenstein",
      "Kristina Toutanova"
    ],
    "summary": "The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. We establish that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. We prove that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. We further show that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Our empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, we outline a potential path towards training neural networks that achieve greater compression and generalization.",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2509.22445v2",
    "arxiv_url": "http://arxiv.org/abs/2509.22445v2",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards",
    "authors": [
      "Aaron Tu",
      "Weihao Xuan",
      "Heli Qi",
      "Xu Huang",
      "Qingcheng Zeng",
      "Shayan Talaei",
      "Yijia Xiao",
      "Peng Xia",
      "Xiangru Tang",
      "Yuchen Zhuang",
      "Bing Hu",
      "Hanqun Cao",
      "Wenqi Shi",
      "Tianang Leng",
      "Rui Yang",
      "Yingjian Chen",
      "Ziqi Wang",
      "Irene Li",
      "Nan Liu",
      "Huaxiu Yao",
      "Li Erran Li",
      "Ge Liu",
      "Amin Saberi",
      "Naoto Yokoya",
      "Jure Leskovec",
      "Yejin Choi",
      "Fang Wu"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is a practical and scalable approach to enhancing large language models in areas such as math, code, and other structured tasks. Two questions motivate this paper: how much of the reported gains survive under strictly parity-controlled evaluation, and whether RLVR is cost-free or exacts a measurable tax. We argue that progress is real, but gains are often overstated due to three forces - an RLVR tax, evaluation pitfalls, and data contamination. Using a partial-prompt contamination audit and matched-budget reproductions across base and RL models, we show that several headline gaps shrink or vanish under clean, parity-controlled evaluation. We then propose a tax-aware training and evaluation protocol that co-optimizes accuracy, grounding, and calibrated abstention and standardizes budgeting and provenance checks. Applied to recent RLVR setups, this protocol yields more reliable estimates of reasoning gains and, in several cases, revises prior conclusions. Our position is constructive: RLVR is valuable and industry-ready; we advocate keeping its practical benefits while prioritizing reliability, safety, and measurement.",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2509.21882v1",
    "arxiv_url": "http://arxiv.org/abs/2509.21882v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems",
    "authors": [
      "Xavier Gonzalez",
      "E. Kelly Buchanan",
      "Hyun Dong Lee",
      "Jerry Weihong Liu",
      "Ke Alexander Wang",
      "David M. Zoltowski",
      "Christopher R\u00e9",
      "Scott W. Linderman"
    ],
    "summary": "Harnessing parallelism in seemingly sequential models is a central challenge for modern machine learning. Several approaches have been proposed for evaluating sequential processes in parallel using fixed-point methods, like Newton, Picard, and Jacobi iterations. In this work, we show that these methods can be understood within a common framework based on linear dynamical systems (LDSs), where different iteration schemes arise naturally as approximate linearizations of a nonlinear recursion. This unifying view highlights shared principles behind these techniques and clarifies when particular fixed-point methods are most likely to be effective. By bridging diverse algorithms through the language of LDSs, our framework provides a clearer theoretical foundation for parallelizing sequential models and points toward new opportunities for efficient and scalable computation.",
    "published": "Sep 26",
    "pdf_url": "https://arxiv.org/pdf/2509.21716v1",
    "arxiv_url": "http://arxiv.org/abs/2509.21716v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations",
    "authors": [
      "Weiqiao Han",
      "Chenlin Meng",
      "Christopher D. Manning",
      "Stefano Ermon"
    ],
    "summary": "We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.",
    "published": "Sep 25",
    "pdf_url": "https://arxiv.org/pdf/2509.21513v2",
    "arxiv_url": "http://arxiv.org/abs/2509.21513v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding",
    "authors": [
      "Abdul Waheed",
      "Zhen Wu",
      "Dareen Alharthi",
      "Seungone Kim",
      "Bhiksha Raj"
    ],
    "summary": "Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.",
    "published": "Sep 25",
    "pdf_url": "https://arxiv.org/pdf/2509.21451v1",
    "arxiv_url": "http://arxiv.org/abs/2509.21451v1",
    "queried_author": "Seungone Kim",
    "matching_authors": [
      "Seungone Kim"
    ]
  },
  {
    "title": "RL Grokking Recipe: How Does RL Unlock and Transfer New Algorithms in LLMs?",
    "authors": [
      "Yiyou Sun",
      "Yuhan Cao",
      "Pohao Huang",
      "Haoyue Bai",
      "Hannaneh Hajishirzi",
      "Nouha Dziri",
      "Dawn Song"
    ],
    "summary": "It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code -- Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding -- a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? -- and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, bu...",
    "published": "Sep 25",
    "pdf_url": "https://arxiv.org/pdf/2509.21016v2",
    "arxiv_url": "http://arxiv.org/abs/2509.21016v2",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Language Models that Think, Chat Better",
    "authors": [
      "Adithya Bhaskar",
      "Xi Ye",
      "Danqi Chen"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model reasoning by using rule-based rewards in verifiable domains such as mathematics and code. However, RLVR leads to limited generalization for open-ended tasks -- such as writing outline essays or making meal plans -- where humans reason routinely. This paper shows that the RLVR paradigm is effective beyond verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking (**RLMT**) for general-purpose chat capabilities. Using diverse real-world prompts, RLMT requires LMs to generate long CoT reasoning before response, and optimizes them with online RL against a preference-based reward model used in RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT consistently outperforms standard RLHF pipelines. This includes substantial gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and ArenaHardV2), along with 1-3 point improvements on other tasks like creative writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be applied directly to base models without an SFT stage, akin to R1-Zero training. Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex multi-staged pipeline with 25M+ examples. We close with qualitative and quantitative analyses of how trained models plan their responses. Our results r...",
    "published": "Sep 24",
    "pdf_url": "https://arxiv.org/pdf/2509.20357v1",
    "arxiv_url": "http://arxiv.org/abs/2509.20357v1",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models",
    "authors": [
      "Julie Kallini",
      "Dan Jurafsky",
      "Christopher Potts",
      "Martijn Bartelds"
    ],
    "summary": "Subword tokenizers trained on multilingual corpora naturally produce overlapping tokens across languages. Does token overlap facilitate cross-lingual transfer or instead introduce interference between languages? Prior work offers mixed evidence, partly due to varied setups and confounders, such as token frequency or subword segmentation granularity. To address this question, we devise a controlled experiment where we train bilingual autoregressive models on multiple language pairs under systematically varied vocabulary overlap settings. Crucially, we explore a new dimension to understanding how overlap affects transfer: the semantic similarity of tokens shared across languages. We first analyze our models' hidden representations and find that overlap of any kind creates embedding spaces that capture cross-lingual semantic relationships, while this effect is much weaker in models with disjoint vocabularies. On XNLI and XQuAD, we find that models with overlap outperform models with disjoint vocabularies, and that transfer performance generally improves as overlap increases. Overall, our findings highlight the advantages of token overlap in multilingual models and show that substantial shared vocabulary remains a beneficial design choice for multilingual tokenizers.",
    "published": "Sep 23",
    "pdf_url": "https://arxiv.org/pdf/2509.18750v2",
    "arxiv_url": "http://arxiv.org/abs/2509.18750v2",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts",
      "Dan Jurafsky"
    ]
  },
  {
    "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies",
    "authors": [
      "Jiaxu Zhou",
      "Jen-tse Huang",
      "Xuhui Zhou",
      "Man Ho Lam",
      "Xintao Wang",
      "Hao Zhu",
      "Wenxuan Wang",
      "Maarten Sap"
    ],
    "summary": "Large language models (LLMs) are increasingly deployed to simulate human collective behaviors, yet the methodological rigor of these \"AI societies\" remains under-explored. Through a systematic audit of 42 recent studies, we identify six pervasive flaws-spanning agent profiles, interaction, memory, control, unawareness, and realism (PIMMUR). Our analysis reveals that 90.7% of studies violate at least one principle, undermining simulation validity. We demonstrate that frontier LLMs correctly identify the underlying social experiment in 47.6% of cases, while 65.3% of prompts exert excessive control that pre-determines outcomes. By reproducing five representative experiments (e.g., telephone game), we show that reported collective phenomena often vanish or reverse when PIMMUR principles are enforced, suggesting that many \"emergent\" behaviors are methodological artifacts rather than genuine social dynamics. Our findings suggest that current AI simulations may capture model-specific biases rather than universal human social behaviors, raising critical concerns about the use of LLMs as scientific proxies for human society.",
    "published": "Sep 22",
    "pdf_url": "https://arxiv.org/pdf/2509.18052v2",
    "arxiv_url": "http://arxiv.org/abs/2509.18052v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models",
    "authors": [
      "Satyapriya Krishna",
      "Andy Zou",
      "Rahul Gupta",
      "Eliot Krzysztof Jones",
      "Nick Winter",
      "Dan Hendrycks",
      "J. Zico Kolter",
      "Matt Fredrikson",
      "Spyros Matsoukas"
    ],
    "summary": "The safety and alignment of Large Language Models (LLMs) are critical for their responsible deployment. Current evaluation methods predominantly focus on identifying and preventing overtly harmful outputs. However, they often fail to address a more insidious failure mode: models that produce benign-appearing outputs while operating on malicious or deceptive internal reasoning. This vulnerability, often triggered by sophisticated system prompt injections, allows models to bypass conventional safety filters, posing a significant, underexplored risk. To address this gap, we introduce the Deceptive Reasoning Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy between a model's internal reasoning process and its final output. D-REX was constructed through a competitive red-teaming exercise where participants crafted adversarial system prompts to induce such deceptive behaviors. Each sample in D-REX contains the adversarial system prompt, an end-user's test query, the model's seemingly innocuous response, and, crucially, the model's internal chain-of-thought, which reveals the underlying malicious intent. Our benchmark facilitates a new, essential evaluation task: the detection of deceptive alignment. We demonstrate that D-REX presents a significant challenge for existing models and safety mechanisms, highlighting the urgent need for new techniques that scrutinize the internal processes of LLMs, not just their final outputs.",
    "published": "Sep 22",
    "pdf_url": "https://arxiv.org/pdf/2509.17938v1",
    "arxiv_url": "http://arxiv.org/abs/2509.17938v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology",
    "authors": [
      "Fagun Patel",
      "Duc Q. Nguyen",
      "Sang T. Truong",
      "Jody Vaynshtok",
      "Sanmi Koyejo",
      "Nick Haber"
    ],
    "summary": "According to the U.S. National Institutes of Health, more than 3.4 million children experience speech disorders that require clinical intervention. The number of speech-language pathologists (SLPs) is roughly 20 times fewer than the number of affected children, highlighting a significant gap in children's care and a pressing need for technological support that improves the productivity of SLPs. State-of-the-art multimodal language models (MLMs) show promise for supporting SLPs, but their use remains underexplored largely due to a limited understanding of their performance in high-stakes clinical settings. To address this gap, we collaborate with domain experts to develop a taxonomy of real-world use cases of MLMs in speech-language pathologies. Building on this taxonomy, we introduce the first comprehensive benchmark for evaluating MLM across five core use cases, each containing 1,000 manually annotated data points. This benchmark includes robustness and sensitivity tests under various settings, including background noise, speaker gender, and accent. Our evaluation of 15 state-of-the-art MLMs reveals that no single model consistently outperforms others across all tasks. Notably, we find systematic disparities, with models performing better on male speakers, and observe that chain-of-thought prompting can degrade performance on classification tasks with large label spaces and narrow decision boundaries. Furthermore, we study fine-tuning MLMs on domain-specific data, achieving improvements of over 10\\% compared to base models. These findings highlight both the potential and l...",
    "published": "Sep 20",
    "pdf_url": "https://arxiv.org/pdf/2509.16765v2",
    "arxiv_url": "http://arxiv.org/abs/2509.16765v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior",
    "authors": [
      "Angelina Wang",
      "Daniel E. Ho",
      "Sanmi Koyejo"
    ],
    "summary": "Standard offline evaluations for language models -- a series of independent, state-less inferences made by models -- fail to capture how language models actually behave in practice, where personalization fundamentally alters model behavior. For instance, identical benchmark questions to the same language model can produce markedly different responses when prompted to a state-less system, in one user's chat session, or in a different user's chat session. In this work, we provide empirical evidence showcasing this phenomenon by comparing offline evaluations to field evaluations conducted by having 800 real users of ChatGPT and Gemini pose benchmark and other provided questions to their chat interfaces.",
    "published": "Sep 18",
    "pdf_url": "https://arxiv.org/pdf/2509.19364v1",
    "arxiv_url": "http://arxiv.org/abs/2509.19364v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Pre-training under infinite compute",
    "authors": [
      "Konwoo Kim",
      "Suhas Kotha",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "summary": "Since compute grows much faster than web text available for language model pre-training, we ask how one should approach pre-training under fixed data and no compute constraints. We first show that existing data-constrained approaches of increasing epoch count and parameter count eventually overfit, and we significantly improve upon such recipes by properly tuning regularization, finding that the optimal weight decay is $30\\times$ larger than standard practice. Since our regularized recipe monotonically decreases loss following a simple power law in parameter count, we estimate its best possible performance via the asymptote of its scaling law rather than the performance at a fixed compute budget. We then identify that ensembling independently trained models achieves a significantly lower loss asymptote than the regularized recipe. Our best intervention combining epoching, regularization, parameter scaling, and ensemble scaling achieves an asymptote at 200M tokens using $5.17\\times$ less data than our baseline, and our data scaling laws predict that this improvement persists at higher token budgets. We find that our data efficiency gains can be realized at much smaller parameter counts as we can distill an ensemble into a student model that is 8$\\times$ smaller and retains $83\\%$ of the ensembling benefit. Finally, our interventions designed for validation loss generalize to downstream benchmarks, achieving a $9\\%$ improvement for pre-training evals and a $17.5\\times$ data efficiency improvement over continued pre-training on math mid-training data. Our results show that sim...",
    "published": "Sep 18",
    "pdf_url": "https://arxiv.org/pdf/2509.14786v1",
    "arxiv_url": "http://arxiv.org/abs/2509.14786v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Tatsunori Hashimoto"
    ]
  },
  {
    "title": "Synthetic bootstrapped pretraining",
    "authors": [
      "Zitong Yang",
      "Aonan Zhang",
      "Hong Liu",
      "Tatsunori Hashimoto",
      "Emmanuel Cand\u00e8s",
      "Chong Wang",
      "Ruoming Pang"
    ],
    "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter and a 6B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers up to 60% of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.",
    "published": "Sep 17",
    "pdf_url": "https://arxiv.org/pdf/2509.15248v3",
    "arxiv_url": "http://arxiv.org/abs/2509.15248v3",
    "queried_author": "Tatsunori Hashimoto",
    "matching_authors": [
      "Tatsunori Hashimoto"
    ]
  },
  {
    "title": "Value Alignment of Social Media Ranking Algorithms",
    "authors": [
      "Farnaz Jahanbakhsh",
      "Dora Zhao",
      "Tiziano Piccardi",
      "Zachary Robertson",
      "Ziv Epstein",
      "Sanmi Koyejo",
      "Michael S. Bernstein"
    ],
    "summary": "While social media feed rankings are primarily driven by engagement signals rather than any explicit value system, the resulting algorithmic feeds are not value-neutral: engagement may prioritize specific individualistic values. This paper presents an approach for social media feed value alignment. We adopt Schwartz's theory of Basic Human Values -- a broad set of human values that articulates complementary and opposing values forming the building blocks of many cultures -- and we implement an algorithmic approach that models and then ranks feeds by expressions of Schwartz's values in social media posts. Our approach enables controls where users can express weights on their desired values, combining these weights and post value expressions into a ranking that respects users' articulated trade-offs. Through controlled experiments (N=141 and N=250), we demonstrate that users can use these controls to architect feeds reflecting their desired values. Across users, value-ranked feeds align with personal values, diverging substantially from existing engagement-driven feeds.",
    "published": "Sep 17",
    "pdf_url": "https://arxiv.org/pdf/2509.14434v1",
    "arxiv_url": "http://arxiv.org/abs/2509.14434v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments",
    "authors": [
      "Project Apertus",
      "Alejandro Hern\u00e1ndez-Cano",
      "Alexander H\u00e4gele",
      "Allen Hao Huang",
      "Angelika Romanou",
      "Antoni-Joan Solergibert",
      "Barna Pasztor",
      "Bettina Messmer",
      "Dhia Garbaya",
      "Eduard Frank \u010eurech",
      "Ido Hakimi",
      "Juan Garc\u00eda Giraldo",
      "Mete Ismayilzada",
      "Negar Foroutan",
      "Skander Moalla",
      "Tiancheng Chen",
      "Vinko Sabol\u010dec",
      "Yixuan Xu",
      "Michael Aerni",
      "Badr AlKhamissi",
      "In\u00e9s Altemir Mari\u00f1as",
      "Mohammad Hossein Amani",
      "Matin Ansaripour",
      "Ilia Badanin",
      "Harold Benoit",
      "Emanuela Boros",
      "Nicholas Browning",
      "Fabian B\u00f6sch",
      "Maximilian B\u00f6ther",
      "Niklas Canova",
      "Camille Challier",
      "Clement Charmillot",
      "Jonathan Coles",
      "Jan Deriu",
      "Arnout Devos",
      "Lukas Drescher",
      "Daniil Dzenhaliou",
      "Maud Ehrmann",
      "Dongyang Fan",
      "Simin Fan",
      "Silin Gao",
      "Miguel Gila",
      "Mar\u00eda Grandury",
      "Diba Hashemi",
      "Alexander Hoyle",
      "Jiaming Jiang",
      "Mark Klein",
      "Andrei Kucharavy",
      "Anastasiia Kucherenko",
      "Frederike L\u00fcbeck",
      "Roman Machacek",
      "Theofilos Manitaras",
      "Andreas Marfurt",
      "Kyle Matoba",
      "Simon Matrenok",
      "Henrique Mendon\u00e7a",
      "Fawzi Roberto Mohamed",
      "Syrielle Montariol",
      "Luca Mouchel",
      "Sven Najem-Meyer",
      "Jingwei Ni",
      "Gennaro Oliva",
      "Matteo Pagliardini",
      "Elia Palme",
      "Andrei Panferov",
      "L\u00e9o Paoletti",
      "Marco Passerini",
      "Ivan Pavlov",
      "Auguste Poiroux",
      "Kaustubh Ponkshe",
      "Nathan Ranchin",
      "Javi Rando",
      "Mathieu Sauser",
      "Jakhongir Saydaliev",
      "Muhammad Ali Sayfiddinov",
      "Marian Schneider",
      "Stefano Schuppli",
      "Marco Scialanga",
      "Andrei Semenov",
      "Kumar Shridhar",
      "Raghav Singhal",
      "Anna Sotnikova",
      "Alexander Sternfeld",
      "Ayush Kumar Tarun",
      "Paul Teiletche",
      "Jannis Vamvas",
      "Xiaozhe Yao",
      "Hao Zhao",
      "Alexander Ilic",
      "Ana Klimovic",
      "Andreas Krause",
      "Caglar Gulcehre",
      "David Rosenthal",
      "Elliott Ash",
      "Florian Tram\u00e8r",
      "Joost VandeVondele",
      "Livio Veraldi",
      "Martin Rajman",
      "Thomas Schulthess",
      "Torsten Hoefler",
      "Antoine Bosselut",
      "Martin Jaggi",
      "Imanol Schlag"
    ],
    "summary": "We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.",
    "published": "Sep 17",
    "pdf_url": "https://arxiv.org/pdf/2509.14233v2",
    "arxiv_url": "http://arxiv.org/abs/2509.14233v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Large Language Models Discriminate Against Speakers of German Dialects",
    "authors": [
      "Minh Duc Bui",
      "Carolin Holtermann",
      "Valentin Hofmann",
      "Anne Lauscher",
      "Katharina von der Wense"
    ],
    "summary": "Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.",
    "published": "Sep 17",
    "pdf_url": "https://arxiv.org/pdf/2509.13835v1",
    "arxiv_url": "http://arxiv.org/abs/2509.13835v1",
    "queried_author": "Valentin Hofmann",
    "matching_authors": [
      "Valentin Hofmann"
    ]
  },
  {
    "title": "A deep reinforcement learning platform for antibiotic discovery",
    "authors": [
      "Hanqun Cao",
      "Marcelo D. T. Torres",
      "Jingjie Zhang",
      "Zijun Gao",
      "Fang Wu",
      "Chunbin Gu",
      "Jure Leskovec",
      "Yejin Choi",
      "Cesar de la Fuente-Nunez",
      "Guangyong Chen",
      "Pheng-Ann Heng"
    ],
    "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths annually by 2050, underscoring the urgent need for new antibiotics. Here we present ApexAmphion, a deep-learning framework for de novo design of antibiotics that couples a 6.4-billion-parameter protein language model with reinforcement learning. The model is first fine-tuned on curated peptide data to capture antimicrobial sequence regularities, then optimised with proximal policy optimization against a composite reward that combines predictions from a learned minimum inhibitory concentration (MIC) classifier with differentiable physicochemical objectives. In vitro evaluation of 100 designed peptides showed low MIC values (nanomolar range in some cases) for all candidates (100% hit rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial activity against at least two clinically relevant bacteria. The lead molecules killed bacteria primarily by potently targeting the cytoplasmic membrane. By unifying generation, scoring and multi-objective optimization with deep reinforcement learning in a single pipeline, our approach rapidly produces diverse, potent candidates, offering a scalable route to peptide antibiotics and a platform for iterative steering toward potency and developability within hours.",
    "published": "Sep 16",
    "pdf_url": "https://arxiv.org/pdf/2509.18153v1",
    "arxiv_url": "http://arxiv.org/abs/2509.18153v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Harnessing Optimization Dynamics for Curvature-Informed Model Merging",
    "authors": [
      "Pouria Mahdavinia",
      "Hamed Mahdavi",
      "Niloofar Mireshghallah",
      "Mehrdad Mahdavi"
    ],
    "summary": "Model merging is an effective post-training strategy for composing capabilities in large language models without joint retraining. We study this in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT checkpoints -- spanning math, code, precise instruction following, general instruction following, and knowledge recall -- must be consolidated into a single model. We introduce Optimization Trajectory Aware (OTA) Merging, a curvature-aware aggregation that leverages optimizer second-moment statistics as a diagonal curvature proxy to reweight parameter edits and mitigate interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a curvature-driven task-localization step that sparsifies conflicting or low-importance edits. FFG induces extremely low-rank masks concentrated in early attention query/key projections and token embeddings, exploiting shared curvature across capabilities. We further develop a memory-light compression of the second moments that preserves OTA's effect. Across diverse capability-based SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space baselines, reduces negative transfer, and remains robust across sparsity levels. Analyses reveal substantial curvature overlap between checkpoints, offering a novel lens on why simple linear merging can be effective in practice. Ablations confirm that FFG is critical for reducing task interference and that the compressed second moments retain the gains of the full formulation. To facilitate reproducibility, we open-source all code, training and evaluation scrip...",
    "published": "Sep 14",
    "pdf_url": "https://arxiv.org/pdf/2509.11167v1",
    "arxiv_url": "http://arxiv.org/abs/2509.11167v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "Fluid Language Model Benchmarking",
    "authors": [
      "Valentin Hofmann",
      "David Heineman",
      "Ian Magnusson",
      "Kyle Lo",
      "Jesse Dodge",
      "Maarten Sap",
      "Pang Wei Koh",
      "Chun Wang",
      "Hannaneh Hajishirzi",
      "Noah A. Smith"
    ],
    "summary": "Language model (LM) benchmarking faces several challenges: comprehensive evaluations are costly, benchmarks often fail to measure the intended capabilities, and evaluation quality can degrade due to labeling errors and benchmark saturation. Although various strategies have been proposed to mitigate these issues, they tend to address individual aspects in isolation, neglecting broader questions about overall evaluation quality. Here, we introduce Fluid Benchmarking, a new evaluation approach that advances LM benchmarking across multiple dimensions. Inspired by psychometrics, Fluid Benchmarking is based on the insight that the relative value of benchmark items depends on an LM's capability level, suggesting that evaluation should adapt to each LM. Methodologically, Fluid Benchmarking estimates an item response model based on existing LM evaluation results and uses the inferred quantities to select evaluation items dynamically, similar to computerized adaptive testing in education. In our experiments, we compare Fluid Benchmarking against the common practice of random item sampling as well as more sophisticated baselines, including alternative methods grounded in item response theory. We examine four dimensions -- efficiency, validity, variance, and saturation -- and find that Fluid Benchmarking achieves superior performance in all of them (e.g., higher validity and less variance on MMLU with fifty times fewer items). Our analysis shows that the two components of Fluid Benchmarking have distinct effects: item response theory, used to map performance into a latent ability space...",
    "published": "Sep 14",
    "pdf_url": "https://arxiv.org/pdf/2509.11106v1",
    "arxiv_url": "http://arxiv.org/abs/2509.11106v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Ian Magnusson",
      "Jesse Dodge",
      "Kyle Lo",
      "Maarten Sap",
      "Noah A. Smith",
      "Pang Wei Koh",
      "Valentin Hofmann"
    ]
  },
  {
    "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness",
    "authors": [
      "Anand Swaroop",
      "Akshat Nallani",
      "Saksham Uboweja",
      "Adiliia Uzdenova",
      "Michael Nguyen",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Vasu Sharma",
      "Maheep Chaudhary"
    ],
    "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \\href{https://github.com/Anut-py/frit}.",
    "published": "Sep 10",
    "pdf_url": "https://arxiv.org/pdf/2509.13334v1",
    "arxiv_url": "http://arxiv.org/abs/2509.13334v1",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
    "authors": [
      "Nathan Egbuna",
      "Saatvik Gaur",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Maheep Chaudhary"
    ],
    "summary": "Test-time optimization remains impractical at scale due to prohibitive inference costs--techniques like iterative refinement and multi-step verification can require $10-100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-500 benchmarks, ALS achieves $2-5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at https://github.com/negbuna/ALS.",
    "published": "Sep 10",
    "pdf_url": "https://arxiv.org/pdf/2509.18116v2",
    "arxiv_url": "http://arxiv.org/abs/2509.18116v2",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models",
    "authors": [
      "Maheep Chaudhary",
      "Ian Su",
      "Nikhil Hooda",
      "Nishith Shankar",
      "Julia Tan",
      "Kevin Zhu",
      "Ryan Lagasse",
      "Vasu Sharma",
      "Ashwinee Panda"
    ],
    "summary": "Large language models (LLMs) can internally distinguish between evaluation and deployment contexts, a behaviour known as \\emph{evaluation awareness}. This undermines AI safety evaluations, as models may conceal dangerous capabilities during testing. Prior work demonstrated this in a single $70$B model, but the scaling relationship across model sizes remains unknown. We investigate evaluation awareness across $15$ models scaling from $0.27$B to $70$B parameters from four families using linear probing on steering vector activations. Our results reveal a clear power-law scaling: evaluation awareness increases predictably with model size. This scaling law enables forecasting deceptive behavior in future larger models and guides the design of scale-aware evaluation strategies for AI safety. A link to the implementation of this paper can be found at https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.",
    "published": "Sep 10",
    "pdf_url": "https://arxiv.org/pdf/2509.13333v2",
    "arxiv_url": "http://arxiv.org/abs/2509.13333v2",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "Reconstruction Alignment Improves Unified Multimodal Models",
    "authors": [
      "Ji Xie",
      "Trevor Darrell",
      "Luke Zettlemoyer",
      "XuDong Wang"
    ],
    "summary": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit 6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs",
    "published": "Sep 08",
    "pdf_url": "https://arxiv.org/pdf/2509.07295v3",
    "arxiv_url": "http://arxiv.org/abs/2509.07295v3",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties",
    "authors": [
      "William Chen",
      "Chutong Meng",
      "Jiatong Shi",
      "Martijn Bartelds",
      "Shih-Heng Wang",
      "Hsiu-Hsuan Wang",
      "Rafael Mosquera",
      "Sara Hincapie",
      "Dan Jurafsky",
      "Antonis Anastasopoulos",
      "Hung-yi Lee",
      "Karen Livescu",
      "Shinji Watanabe"
    ],
    "summary": "Recent improvements in multilingual ASR have not been equally distributed across languages and language varieties. To advance state-of-the-art (SOTA) ASR models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a new test suite that consists of data from 200+ languages, accents, and dialects to evaluate SOTA multilingual speech models. The challenge also introduces an online evaluation server based on DynaBench, allowing for flexibility in model design and architecture for participants. The challenge received 5 submissions from 3 teams, all of which outperformed our baselines. The best-performing submission achieved an absolute improvement in LID accuracy of 23% and a reduction in CER of 18% when compared to the best baseline on a general multilingual test set. On accented and dialectal data, the best submission obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance of community challenges in making speech technologies more inclusive.",
    "published": "Sep 08",
    "pdf_url": "https://arxiv.org/pdf/2509.07139v1",
    "arxiv_url": "http://arxiv.org/abs/2509.07139v1",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts",
    "authors": [
      "Linlu Qiu",
      "Cedegao E. Zhang",
      "Joshua B. Tenenbaum",
      "Yoon Kim",
      "Roger P. Levy"
    ],
    "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from Wavelength, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans.",
    "published": "Sep 08",
    "pdf_url": "https://arxiv.org/pdf/2509.06952v2",
    "arxiv_url": "http://arxiv.org/abs/2509.06952v2",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
    "authors": [
      "Marc Marone",
      "Orion Weller",
      "William Fleshman",
      "Eugene Yang",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "summary": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.",
    "published": "Sep 08",
    "pdf_url": "https://arxiv.org/pdf/2509.06888v1",
    "arxiv_url": "http://arxiv.org/abs/2509.06888v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives",
    "authors": [
      "Sarah H. Cen",
      "Salil Goyal",
      "Zaynah Javed",
      "Ananya Karthik",
      "Percy Liang",
      "Daniel E. Ho"
    ],
    "summary": "AI audits play a critical role in AI accountability and safety. One branch of the law for which AI audits are particularly salient is anti-discrimination law. Several areas of anti-discrimination law implicate the \"less discriminatory alternative\" (LDA) requirement, in which a protocol (e.g., model) is defensible if no less discriminatory protocol that achieves comparable performance can be found with a reasonable amount of effort. Notably, the burden of proving an LDA exists typically falls on the claimant (the party alleging discrimination). This creates a significant hurdle in AI cases, as the claimant would seemingly need to train a less discriminatory yet high-performing model, a task requiring resources and expertise beyond most litigants. Moreover, developers often shield information about and access to their model and training data as trade secrets, making it difficult to reproduce a similar model from scratch.\n  In this work, we present a procedure enabling claimants to determine if an LDA exists, even when they have limited compute, data, information, and model access. We focus on the setting in which fairness is given by demographic parity and performance by binary cross-entropy loss. As our main result, we provide a novel closed-form upper bound for the loss-fairness Pareto frontier (PF). We show how the claimant can use it to fit a PF in the \"low-resource regime,\" then extrapolate the PF that applies to the (large) model being contested, all without training a single large model. The expression thus serves as a scaling law for loss-fairness PFs. To use this sca...",
    "published": "Sep 06",
    "pdf_url": "https://arxiv.org/pdf/2509.05627v1",
    "arxiv_url": "http://arxiv.org/abs/2509.05627v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Ad hoc conventions generalize to new referents",
    "authors": [
      "Anya Ji",
      "Claire Augusta Bergey",
      "Ron Eliav",
      "Yoav Artzi",
      "Robert D. Hawkins"
    ],
    "summary": "How do people talk about things they've never talked about before? One view suggests that a new shared naming system establishes an arbitrary link to a specific target, like proper names that cannot extend beyond their bearers. An alternative view proposes that forming a shared way of describing objects involves broader conceptual alignment, reshaping each individual's semantic space in ways that should generalize to new referents. We test these competing accounts in a dyadic communication study (N=302) leveraging the recently-released KiloGram dataset containing over 1,000 abstract tangram images. After pairs of participants coordinated on referential conventions for one set of images through repeated communication, we measured the extent to which their descriptions aligned for undiscussed images. We found strong evidence for generalization: partners showed increased alignment relative to their pre-test labels. Generalization also decayed nonlinearly with visual similarity (consistent with Shepard's law) and was robust across levels of the images' nameability. These findings suggest that ad hoc conventions are not arbitrary labels but reflect genuine conceptual coordination, with implications for theories of reference and the design of more adaptive language agents.",
    "published": "Sep 06",
    "pdf_url": "https://arxiv.org/pdf/2509.05566v1",
    "arxiv_url": "http://arxiv.org/abs/2509.05566v1",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining",
    "authors": [
      "Deniz Bayazit",
      "Aaron Mueller",
      "Antoine Bosselut"
    ],
    "summary": "Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.",
    "published": "Sep 05",
    "pdf_url": "https://arxiv.org/pdf/2509.05291v1",
    "arxiv_url": "http://arxiv.org/abs/2509.05291v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "DynaGuard: A Dynamic Guardian Model With User-Defined Policies",
    "authors": [
      "Monte Hoover",
      "Vatsal Baherwani",
      "Neel Jain",
      "Khalid Saifullah",
      "Joseph Vincent",
      "Chirag Jain",
      "Melissa Kazemi Rad",
      "C. Bayan Bruss",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "summary": "Guardian models play a crucial role in ensuring the safety and ethical behavior of user-facing AI applications by enforcing guardrails and detecting harmful content. While standard guardian models are limited to predefined, static harm categories, we introduce DynaGuard, a suite of dynamic guardian models offering novel flexibility by evaluating text based on user-defined policies, and DynaBench, a dataset for training and evaluating dynamic guardian models. Our models provide both rapid detection of policy violations and a chain-of-thought reasoning option that articulate and justify model outputs. Critically, DynaGuard not only surpasses static models in detection accuracy on traditional safety categories, but is competitive with frontier reasoning models on free-form policy violations, all in a fraction of the time. This makes DynaGuard an critical tool for language model guardrails.",
    "published": "Sep 02",
    "pdf_url": "https://arxiv.org/pdf/2509.02563v3",
    "arxiv_url": "http://arxiv.org/abs/2509.02563v3",
    "queried_author": "Ashwinee Panda",
    "matching_authors": [
      "Ashwinee Panda"
    ]
  },
  {
    "title": "SpecEval: Evaluating Model Adherence to Behavior Specifications",
    "authors": [
      "Ahmed Ahmed",
      "Kevin Klyman",
      "Yi Zeng",
      "Sanmi Koyejo",
      "Percy Liang"
    ],
    "summary": "Companies that develop foundation models publish behavioral guidelines they pledge their models will follow, but it remains unclear if models actually do so. While providers such as OpenAI, Anthropic, and Google have published detailed specifications describing both desired safety constraints and qualitative traits for their models, there has been no systematic audit of adherence to these guidelines. We introduce an automated framework that audits models against their providers specifications by parsing behavioral statements, generating targeted prompts, and using models to judge adherence. Our central focus is on three way consistency between a provider specification, its model outputs, and its own models as judges; an extension of prior two way generator validator consistency. This establishes a necessary baseline: at minimum, a foundation model should consistently satisfy the developer behavioral specifications when judged by the developer evaluator models. We apply our framework to 16 models from six developers across more than 100 behavioral statements, finding systematic inconsistencies including compliance gaps of up to 20 percent across providers.",
    "published": "Sep 02",
    "pdf_url": "https://arxiv.org/pdf/2509.02464v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02464v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Fantastic Pretraining Optimizers and Where to Find Them",
    "authors": [
      "Kaiyue Wen",
      "David Hall",
      "Tengyu Ma",
      "Percy Liang"
    ],
    "summary": "AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B p...",
    "published": "Sep 02",
    "pdf_url": "https://arxiv.org/pdf/2509.02046v2",
    "arxiv_url": "http://arxiv.org/abs/2509.02046v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Reinforcement Learning for Machine Learning Engineering Agents",
    "authors": [
      "Sherry Yang",
      "Joy He-Yueya",
      "Percy Liang"
    ],
    "summary": "Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this paper, we show that agents backed by weaker models that improve via reinforcement learning (RL) can outperform agents backed by much larger, but static models. We identify two major challenges with RL in this setting. First, actions can take a variable amount of time (e.g., executing code for different solutions), which leads to asynchronous policy gradient updates that favor faster but suboptimal solutions. To tackle variable-duration actions, we propose duration-aware gradient updates in a distributed asynchronous RL framework to amplify high-cost but high-reward actions. Second, using only test split performance as a reward provides limited feedback. A program that is nearly correct is treated the same as one that fails entirely. To address this, we propose environment instrumentation to offer partial credit, distinguishing almost-correct programs from those that fail early (e.g., during data loading). Environment instrumentation uses a separate static language model to insert print statement to an existing program to log the agent's experimental progress, from which partial credit can be extracted as reward signals for learning. Our experimental results on MLEBench suggest that performing gradient updates on a much smaller model (Qwen2.5-3B) trained with RL outperforms prompting a much larger model (Claude-3.5-Sonnet) with agent scaffolds, by an average of 22% across 12 Kaggle tasks.",
    "published": "Sep 01",
    "pdf_url": "https://arxiv.org/pdf/2509.01684v1",
    "arxiv_url": "http://arxiv.org/abs/2509.01684v1",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Statutory Construction and Interpretation for Artificial Intelligence",
    "authors": [
      "Luxi He",
      "Nimra Nadeem",
      "Michel Liao",
      "Howard Chen",
      "Danqi Chen",
      "Mariano-Florentino Cu\u00e9llar",
      "Peter Henderson"
    ],
    "summary": "AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity arises both from how these principles are written and how they are applied. But while legal systems use institutional safeguards to manage such ambiguity, such as transparent appellate review policing interpretive constraints, AI alignment pipelines offer no comparable protections. Different interpretations of the same rule can lead to inconsistent or unstable model behavior. Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity at both the rule creation and rule application steps. We then propose a computational framework that mirrors two legal mechanisms: (1) a rule refinement pipeline that minimizes interpretive disagreement by revising ambiguous rules (analogous to agency rulemaking or iterative legislative action), and (2) prompt-based interpretive constraints that reduce inconsistency in rule application (analogous to legal canons that guide judicial discretion). We evaluate our framework on a 5,000-scenario subset of the WildChat dataset and show that both interventions significantly improve judgment consistency across a panel of reasonable interpreters. Our approach offers a first step toward systematically managing interpretive ambiguity, an essential step for building more robust, law-following AI systems.",
    "published": "Sep 01",
    "pdf_url": "https://arxiv.org/pdf/2509.01186v1",
    "arxiv_url": "http://arxiv.org/abs/2509.01186v1",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen",
      "Peter Henderson"
    ]
  },
  {
    "title": "Social World Models",
    "authors": [
      "Xuhui Zhou",
      "Jiarui Liu",
      "Akhila Yerukola",
      "Hyunwoo Kim",
      "Maarten Sap"
    ],
    "summary": "Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to structure and reason about implicit social contexts, as they lack explicit representations for unobserved dynamics such as intentions, beliefs, and evolving social states. In this paper, we introduce the concept of social world models (SWMs) to characterize the complex social dynamics. To operationalize SWMs, we introduce a novel structured social world representation formalism (S3AP), which captures the evolving states, actions, and mental states of agents, addressing the lack of explicit structure in traditional free-text-based inputs. Through comprehensive experiments across five social reasoning benchmarks, we show that S3AP significantly enhances LLM performance-achieving a +51% improvement on FANToM over OpenAI's o1. Our ablations further reveal that these gains are driven by the explicit modeling of hidden mental states, which proves more effective than a wide range of baseline methods. Finally, we introduce an algorithm for social world models using S3AP, which enables AI agents to build models of their interlocutors and predict their next actions and mental states. Empirically, S3AP-enabled social world models yield up to +18% improvement on the SOTOPIA multi-turn social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigat...",
    "published": "Aug 30",
    "pdf_url": "https://arxiv.org/pdf/2509.00559v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00559v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
    "authors": [
      "Tony Lee",
      "Haoqin Tu",
      "Chi Heem Wong",
      "Zijun Wang",
      "Siwei Yang",
      "Yifan Mai",
      "Yuyin Zhou",
      "Cihang Xie",
      "Percy Liang"
    ],
    "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with o...",
    "published": "Aug 29",
    "pdf_url": "https://arxiv.org/pdf/2508.21376v2",
    "arxiv_url": "http://arxiv.org/abs/2508.21376v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching",
    "authors": [
      "Farnoush Rezaei Jafari",
      "Oliver Eberle",
      "Ashkan Khakzar",
      "Neel Nanda"
    ],
    "summary": "Activation patching is a standard method in mechanistic interpretability for localizing the components of a model responsible for specific behaviors, but it is computationally expensive to apply at scale. Attribution patching offers a faster, gradient-based approximation, yet suffers from noise and reduced reliability in deep, highly non-linear networks. In this work, we introduce Relevance Patching (RelP), which replaces the local gradients in attribution patching with propagation coefficients derived from Layer-wise Relevance Propagation (LRP). LRP propagates the network's output backward through the layers, redistributing relevance to lower-level components according to local propagation rules that ensure properties such as relevance conservation or improved signal-to-noise ratio. Like attribution patching, RelP requires only two forward passes and one backward pass, maintaining computational efficiency while improving faithfulness. We validate RelP across a range of models and tasks, showing that it more accurately approximates activation patching than standard attribution patching, particularly when analyzing residual stream and MLP outputs in the Indirect Object Identification (IOI) task. For instance, for MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by RelP. Additionally, we compare the faithfulness of sparse feature circuits identified by RelP and Integrated Gradients (IG), showing that RelP achieves comparable faithfulness without the extra computational co...",
    "published": "Aug 28",
    "pdf_url": "https://arxiv.org/pdf/2508.21258v2",
    "arxiv_url": "http://arxiv.org/abs/2508.21258v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "authors": [
      "Orion Weller",
      "Michael Boratko",
      "Iftekhar Naim",
      "Jinhyuk Lee"
    ],
    "summary": "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.",
    "published": "Aug 28",
    "pdf_url": "https://arxiv.org/pdf/2508.21038v1",
    "arxiv_url": "http://arxiv.org/abs/2508.21038v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
    "authors": [
      "Huong Ngo",
      "Matt Deitke",
      "Martijn Bartelds",
      "Sarah Pratt",
      "Josh Gardner",
      "Matt Jordan",
      "Ludwig Schmidt"
    ],
    "summary": "Improvements in training data scale and quality have led to significant advances, yet its influence in speech recognition remains underexplored. In this paper, we present a large-scale dataset, OLMoASR-Pool, and series of models, OLMoASR, to study and develop robust zero-shot speech recognition models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio and 17M transcripts, we design text heuristic filters to remove low-quality or mistranscribed data. Our curation pipeline produces a new dataset containing 1M hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M (tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR achieves comparable average performance to OpenAI's Whisper on short and long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a 12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest English-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and long-form recognition respectively (at equivalent parameter count). OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will be made publicly available to further research on robust speech processing.",
    "published": "Aug 28",
    "pdf_url": "https://arxiv.org/pdf/2508.20869v1",
    "arxiv_url": "http://arxiv.org/abs/2508.20869v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Evaluating Language Model Reasoning about Confidential Information",
    "authors": [
      "Dylan Sam",
      "Alexander Robey",
      "Andy Zou",
      "Matt Fredrikson",
      "J. Zico Kolter"
    ],
    "summary": "As language models are increasingly deployed as autonomous agents in high-stakes settings, ensuring that they reliably follow user-defined rules has become a critical safety concern. To this end, we study whether language models exhibit contextual robustness, or the capability to adhere to context-dependent safety specifications. For this analysis, we develop a benchmark (PasswordEval) that measures whether language models can correctly determine when a user request is authorized (i.e., with a correct password). We find that current open- and closed-source models struggle with this seemingly simple task, and that, perhaps surprisingly, reasoning capabilities do not generally improve performance. In fact, we find that reasoning traces frequently leak confidential information, which calls into question whether reasoning traces should be exposed to users in such applications. We also scale the difficulty of our evaluation along multiple axes: (i) by adding adversarial user pressure through various jailbreaking strategies, and (ii) through longer multi-turn conversations where password verification is more challenging. Overall, our results suggest that current frontier models are not well-suited to handling confidential information, and that reasoning capabilities may need to be trained in a different manner to make them safer for release in high-stakes settings.",
    "published": "Aug 27",
    "pdf_url": "https://arxiv.org/pdf/2508.19980v1",
    "arxiv_url": "http://arxiv.org/abs/2508.19980v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks",
    "authors": [
      "C\u00e9dric Colas",
      "Tracey Mills",
      "Ben Prystawski",
      "Michael Henry Tessler",
      "Noah Goodman",
      "Jacob Andreas",
      "Joshua Tenenbaum"
    ],
    "summary": "The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.",
    "published": "Aug 26",
    "pdf_url": "https://arxiv.org/pdf/2509.00074v2",
    "arxiv_url": "http://arxiv.org/abs/2509.00074v2",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas",
      "Noah Goodman"
    ]
  },
  {
    "title": "Generative Interfaces for Language Models",
    "authors": [
      "Jiaqi Chen",
      "Yanzhe Zhang",
      "Yutong Zhang",
      "Yijia Shao",
      "Diyi Yang"
    ],
    "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with up to a 72% improvement in human preference. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.",
    "published": "Aug 26",
    "pdf_url": "https://arxiv.org/pdf/2508.19227v2",
    "arxiv_url": "http://arxiv.org/abs/2508.19227v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation",
    "authors": [
      "Oscar Obeso",
      "Andy Arditi",
      "Javier Ferrando",
      "Joshua Freeman",
      "Cameron Holmes",
      "Neel Nanda"
    ],
    "summary": "Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets entity-level hallucinations-e.g., fabricated names, dates, citations-rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Despite being trained only to detect hallucinated entities, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective c...",
    "published": "Aug 26",
    "pdf_url": "https://arxiv.org/pdf/2509.03531v2",
    "arxiv_url": "http://arxiv.org/abs/2509.03531v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "UQ: Assessing Language Models on Unsolved Questions",
    "authors": [
      "Fan Nie",
      "Ken Ziyu Liu",
      "Zihao Wang",
      "Rui Sun",
      "Wei Liu",
      "Weijia Shi",
      "Huaxiu Yao",
      "Linjun Zhang",
      "Andrew Y. Ng",
      "James Zou",
      "Sanmi Koyejo",
      "Yejin Choi",
      "Percy Liang",
      "Niklas Muennighoff"
    ],
    "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes ...",
    "published": "Aug 25",
    "pdf_url": "https://arxiv.org/pdf/2508.17580v1",
    "arxiv_url": "http://arxiv.org/abs/2508.17580v1",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff",
      "Percy Liang",
      "Sanmi Koyejo",
      "Yejin Choi"
    ]
  },
  {
    "title": "Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks",
    "authors": [
      "Jack Youstra",
      "Mohammed Mahfoud",
      "Yang Yan",
      "Henry Sleight",
      "Ethan Perez",
      "Mrinank Sharma"
    ],
    "summary": "Large language model fine-tuning APIs enable widespread model customization, yet pose significant safety risks. Recent work shows that adversaries can exploit access to these APIs to bypass model safety mechanisms by encoding harmful content in seemingly harmless fine-tuning data, evading both human monitoring and standard content filters. We formalize the fine-tuning API defense problem, and introduce the Cipher Fine-tuning Robustness benchmark (CIFR), a benchmark for evaluating defense strategies' ability to retain model safety in the face of cipher-enabled attackers while achieving the desired level of fine-tuning functionality. We include diverse cipher encodings and families, with some kept exclusively in the test set to evaluate for generalization across unseen ciphers and cipher families. We then evaluate different defenses on the benchmark and train probe monitors on model internal activations from multiple fine-tunes. We show that probe monitors achieve over 99% detection accuracy, generalize to unseen cipher variants and families, and compare favorably to state-of-the-art monitoring approaches. We open-source CIFR and the code to reproduce our experiments to facilitate further research in this critical area. Code and data are available online https://github.com/JackYoustra/safe-finetuning-api",
    "published": "Aug 23",
    "pdf_url": "https://arxiv.org/pdf/2508.17158v1",
    "arxiv_url": "http://arxiv.org/abs/2508.17158v1",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation",
    "authors": [
      "Yejin Choi",
      "Jaewoo Park",
      "Janghan Yoon",
      "Saejin Kim",
      "Jaehyun Jeon",
      "Youngjae Yu"
    ],
    "summary": "Rapid advances in Multimodal Large Language Models (MLLMs) have expanded information retrieval beyond purely textual inputs, enabling retrieval from complex real world documents that combine text and visuals. However, most documents are private either owned by individuals or confined within corporate silos and current retrievers struggle when faced with unseen domains or languages. To address this gap, we introduce PREMIR, a simple yet effective framework that leverages the broad knowledge of an MLLM to generate cross modal pre questions (preQs) before retrieval. Unlike earlier multimodal retrievers that compare embeddings in a single vector space, PREMIR leverages preQs from multiple complementary modalities to expand the scope of matching to the token level. Experiments show that PREMIR achieves state of the art performance on out of distribution benchmarks, including closed domain and multilingual settings, outperforming strong baselines across all retrieval metrics. We confirm the contribution of each component through in depth ablation studies, and qualitative analyses of the generated preQs further highlight the model's robustness in real world settings.",
    "published": "Aug 23",
    "pdf_url": "https://arxiv.org/pdf/2508.17079v1",
    "arxiv_url": "http://arxiv.org/abs/2508.17079v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model",
    "authors": [
      "NVIDIA",
      ":",
      "Aarti Basant",
      "Abhijit Khairnar",
      "Abhijit Paithankar",
      "Abhinav Khattar",
      "Adithya Renduchintala",
      "Aditya Malte",
      "Akhiad Bercovich",
      "Akshay Hazare",
      "Alejandra Rico",
      "Aleksander Ficek",
      "Alex Kondratenko",
      "Alex Shaposhnikov",
      "Alexander Bukharin",
      "Ali Taghibakhshi",
      "Amelia Barton",
      "Ameya Sunil Mahabaleshwarkar",
      "Amy Shen",
      "Andrew Tao",
      "Ann Guan",
      "Anna Shors",
      "Anubhav Mandarwal",
      "Arham Mehta",
      "Arun Venkatesan",
      "Ashton Sharabiani",
      "Ashwath Aithal",
      "Ashwin Poojary",
      "Ayush Dattagupta",
      "Balaram Buddharaju",
      "Banghua Zhu",
      "Barnaby Simkin",
      "Bilal Kartal",
      "Bita Darvish Rouhani",
      "Bobby Chen",
      "Boris Ginsburg",
      "Brandon Norick",
      "Brian Yu",
      "Bryan Catanzaro",
      "Charles Wang",
      "Charlie Truong",
      "Chetan Mungekar",
      "Chintan Patel",
      "Chris Alexiuk",
      "Christian Munley",
      "Christopher Parisien",
      "Dan Su",
      "Daniel Afrimi",
      "Daniel Korzekwa",
      "Daniel Rohrer",
      "Daria Gitman",
      "David Mosallanezhad",
      "Deepak Narayanan",
      "Dima Rekesh",
      "Dina Yared",
      "Dmytro Pykhtar",
      "Dong Ahn",
      "Duncan Riach",
      "Eileen Long",
      "Elliott Ning",
      "Eric Chung",
      "Erick Galinkin",
      "Evelina Bakhturina",
      "Gargi Prasad",
      "Gerald Shen",
      "Haifeng Qian",
      "Haim Elisha",
      "Harsh Sharma",
      "Hayley Ross",
      "Helen Ngo",
      "Herman Sahota",
      "Hexin Wang",
      "Hoo Chang Shin",
      "Hua Huang",
      "Iain Cunningham",
      "Igor Gitman",
      "Ivan Moshkov",
      "Jaehun Jung",
      "Jan Kautz",
      "Jane Polak Scowcroft",
      "Jared Casper",
      "Jian Zhang",
      "Jiaqi Zeng",
      "Jimmy Zhang",
      "Jinze Xue",
      "Jocelyn Huang",
      "Joey Conway",
      "John Kamalu",
      "Jonathan Cohen",
      "Joseph Jennings",
      "Julien Veron Vialard",
      "Junkeun Yi",
      "Jupinder Parmar",
      "Kari Briski",
      "Katherine Cheung",
      "Katherine Luna",
      "Keith Wyss",
      "Keshav Santhanam",
      "Kezhi Kong",
      "Krzysztof Pawelec",
      "Kumar Anik",
      "Kunlun Li",
      "Kushan Ahmadian",
      "Lawrence McAfee",
      "Laya Sleiman",
      "Leon Derczynski",
      "Luis Vega",
      "Maer Rodrigues de Melo",
      "Makesh Narsimhan Sreedhar",
      "Marcin Chochowski",
      "Mark Cai",
      "Markus Kliegl",
      "Marta Stepniewska-Dziubinska",
      "Matvei Novikov",
      "Mehrzad Samadi",
      "Meredith Price",
      "Meriem Boubdir",
      "Michael Boone",
      "Michael Evans",
      "Michal Bien",
      "Michal Zawalski",
      "Miguel Martinez",
      "Mike Chrzanowski",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Namit Dhameja",
      "Nave Assaf",
      "Negar Habibi",
      "Nidhi Bhatia",
      "Nikki Pope",
      "Nima Tajbakhsh",
      "Nirmal Kumar Juluru",
      "Oleg Rybakov",
      "Oleksii Hrinchuk",
      "Oleksii Kuchaiev",
      "Oluwatobi Olabiyi",
      "Pablo Ribalta",
      "Padmavathy Subramanian",
      "Parth Chadha",
      "Pavlo Molchanov",
      "Peter Dykas",
      "Peter Jin",
      "Piotr Bialecki",
      "Piotr Januszewski",
      "Pradeep Thalasta",
      "Prashant Gaikwad",
      "Prasoon Varshney",
      "Pritam Gundecha",
      "Przemek Tredak",
      "Rabeeh Karimi Mahabadi",
      "Rajen Patel",
      "Ran El-Yaniv",
      "Ranjit Rajan",
      "Ria Cheruvu",
      "Rima Shahbazyan",
      "Ritika Borkar",
      "Ritu Gala",
      "Roger Waleffe",
      "Ruoxi Zhang",
      "Russell J. Hewett",
      "Ryan Prenger",
      "Sahil Jain",
      "Samuel Kriman",
      "Sanjeev Satheesh",
      "Saori Kaji",
      "Sarah Yurick",
      "Saurav Muralidharan",
      "Sean Narenthiran",
      "Seonmyeong Bak",
      "Sepehr Sameni",
      "Seungju Han",
      "Shanmugam Ramasamy",
      "Shaona Ghosh",
      "Sharath Turuvekere Sreenivas",
      "Shelby Thomas",
      "Shizhe Diao",
      "Shreya Gopal",
      "Shrimai Prabhumoye",
      "Shubham Toshniwal",
      "Shuoyang Ding",
      "Siddharth Singh",
      "Siddhartha Jain",
      "Somshubra Majumdar",
      "Soumye Singhal",
      "Stefania Alborghetti",
      "Syeda Nahida Akter",
      "Terry Kong",
      "Tim Moon",
      "Tomasz Hliwiak",
      "Tomer Asida",
      "Tony Wang",
      "Tugrul Konuk",
      "Twinkle Vashishth",
      "Tyler Poon",
      "Udi Karpas",
      "Vahid Noroozi",
      "Venkat Srinivasan",
      "Vijay Korthikanti",
      "Vikram Fugro",
      "Vineeth Kalluru",
      "Vitaly Kurin",
      "Vitaly Lavrukhin",
      "Wasi Uddin Ahmad",
      "Wei Du",
      "Wonmin Byeon",
      "Ximing Lu",
      "Xin Dong",
      "Yashaswi Karnati",
      "Yejin Choi",
      "Yian Zhang",
      "Ying Lin",
      "Yonggan Fu",
      "Yoshi Suhara",
      "Zhen Dong",
      "Zhiyu Li",
      "Zhongbo Zhu",
      "Zijia Chen"
    ],
    "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.",
    "published": "Aug 20",
    "pdf_url": "https://arxiv.org/pdf/2508.14444v4",
    "arxiv_url": "http://arxiv.org/abs/2508.14444v4",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
    "authors": [
      "David Heineman",
      "Valentin Hofmann",
      "Ian Magnusson",
      "Yuling Gu",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Kyle Lo",
      "Jesse Dodge"
    ],
    "summary": "Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models ...",
    "published": "Aug 18",
    "pdf_url": "https://arxiv.org/pdf/2508.13144v1",
    "arxiv_url": "http://arxiv.org/abs/2508.13144v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Ian Magnusson",
      "Jesse Dodge",
      "Kyle Lo",
      "Noah A. Smith",
      "Valentin Hofmann"
    ]
  },
  {
    "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
    "authors": [
      "Pranjal Aggarwal",
      "Seungone Kim",
      "Jack Lanchantin",
      "Sean Welleck",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ],
    "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and overthinking on simpler problems, while non-thinking LLMs are faster and cheaper but underthink on harder reasoning problems. This has led to the development of separate thinking and non-thinking LLM variants, leaving the onus of selecting the optimal model for each query on the end user. We introduce OptimalThinkingBench, a unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench, featuring simple math and general queries in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks along with harder math problems. Using novel thinking-adjusted accuracy metrics, we extensively evaluate 33 different thinking and non-thinking models and show that no model is able to optimally think on our benchmark. Thinking models often overthink for hundreds of tokens on the simplest user queries without improving performance. In contrast, large non-thinking models underthink, often falling short of much smaller thinking models. We further explore several methods to encourage optimal thinking, but find that these approaches often improve on one sub-benchmark at the expense of the other, highlighting the need for better unified and optimal models in the future.",
    "published": "Aug 18",
    "pdf_url": "https://arxiv.org/pdf/2508.13141v2",
    "arxiv_url": "http://arxiv.org/abs/2508.13141v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck",
      "Seungone Kim"
    ]
  },
  {
    "title": "AI Behavioral Science",
    "authors": [
      "Matthew O. Jackson",
      "Qiaozhu Me",
      "Stephanie W. Wang",
      "Yutong Xie",
      "Walter Yuan",
      "Seth Benzell",
      "Erik Brynjolfsson",
      "Colin F. Camerer",
      "James Evans",
      "Brian Jabarian",
      "Jon Kleinberg",
      "Juanjuan Meng",
      "Sendhil Mullainathan",
      "Asuman Ozdaglar",
      "Thomas Pfeiffer",
      "Moshe Tennenholtz",
      "Robb Willer",
      "Diyi Yang",
      "Teng Ye"
    ],
    "summary": "We discuss the three main areas comprising the new and emerging field of \"AI Behavioral Science\". This includes not only how AI can enhance research in the behavioral sciences, but also how the behavioral sciences can be used to study and better design AI and to understand how the world will change as AI and humans interact in increasingly layered and complex ways.",
    "published": "Aug 17",
    "pdf_url": "https://arxiv.org/pdf/2509.13323v1",
    "arxiv_url": "http://arxiv.org/abs/2509.13323v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Searching for Privacy Risks in LLM Agents via Simulation",
    "authors": [
      "Yanzhe Zhang",
      "Diyi Yang"
    ],
    "summary": "The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. However, the evolving nature of such dynamic dialogues makes it challenging to anticipate emerging vulnerabilities and design effective defenses. To tackle this problem, we present a search-based framework that alternates between improving attack and defense strategies through the simulation of privacy-critical agent interactions. Specifically, we employ LLMs as optimizers to analyze simulation trajectories and iteratively propose new agent instructions. To explore the strategy space more efficiently, we further utilize parallel search with multiple threads and cross-thread propagation. Through this process, we find that attack strategies escalate from direct requests to sophisticated tactics, such as impersonation and consent forgery, while defenses evolve from simple rule-based constraints to robust identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.",
    "published": "Aug 14",
    "pdf_url": "https://arxiv.org/pdf/2508.10880v2",
    "arxiv_url": "http://arxiv.org/abs/2508.10880v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage",
    "authors": [
      "Skyler Hallinan",
      "Jaehun Jung",
      "Melanie Sclar",
      "Ximing Lu",
      "Abhilasha Ravichander",
      "Sahana Ramnath",
      "Yejin Choi",
      "Sai Praneeth Karimireddy",
      "Niloofar Mireshghallah",
      "Xiang Ren"
    ],
    "summary": "Membership inference attacks serves as useful tool for fair use of language models, such as detecting potential copyright infringement and auditing data leakage. However, many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4. In this work, we introduce N-Gram Coverage Attack, a membership inference attack that relies solely on text outputs from the target model, enabling attacks on completely black-box models. We leverage the observation that models are more likely to memorize and subsequently generate text patterns that were commonly observed in their training data. Specifically, to make a prediction on a candidate member, N-Gram Coverage Attack first obtains multiple model generations conditioned on a prefix of the candidate. It then uses n-gram overlap metrics to compute and aggregate the similarities of these outputs with the ground truth suffix; high similarities indicate likely membership. We first demonstrate on a diverse set of existing benchmarks that N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks - despite having access to only text outputs. Interestingly, we find that the success rate of our method scales with the attack compute budget - as we increase the number of sequences generated from the target model conditioned on the prefix, attack performance tends to improve. Having verified the accuracy of our m...",
    "published": "Aug 13",
    "pdf_url": "https://arxiv.org/pdf/2508.09603v1",
    "arxiv_url": "http://arxiv.org/abs/2508.09603v1",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah",
      "Yejin Choi"
    ]
  },
  {
    "title": "OpenCUA: Open Foundations for Computer-Use Agents",
    "authors": [
      "Xinyuan Wang",
      "Bowen Wang",
      "Dunjie Lu",
      "Junlin Yang",
      "Tianbao Xie",
      "Junli Wang",
      "Jiaqi Deng",
      "Xiaole Guo",
      "Yiheng Xu",
      "Chen Henry Wu",
      "Zhennan Shen",
      "Zhuokai Li",
      "Ryan Li",
      "Xiaochuan Li",
      "Junda Chen",
      "Boyuan Zheng",
      "Peihang Li",
      "Fangyu Lei",
      "Ruisheng Cao",
      "Yeqiao Fu",
      "Dongchan Shin",
      "Martin Shin",
      "Jiarui Hu",
      "Yuyan Wang",
      "Jixuan Chen",
      "Yuxiao Ye",
      "Danyang Zhang",
      "Dikang Du",
      "Hao Hu",
      "Huarong Chen",
      "Zaida Zhou",
      "Haotian Yao",
      "Ziwei Chen",
      "Qizheng Gu",
      "Yipu Wang",
      "Heng Wang",
      "Diyi Yang",
      "Victor Zhong",
      "Flood Sung",
      "Y. Charles",
      "Zhilin Yang",
      "Tao Yu"
    ],
    "summary": "Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-72B achieves an average success rate of 45.0% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models. Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.",
    "published": "Aug 12",
    "pdf_url": "https://arxiv.org/pdf/2508.09123v3",
    "arxiv_url": "http://arxiv.org/abs/2508.09123v3",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "OverFill: Two-Stage Models for Efficient Language Model Decoding",
    "authors": [
      "Woojeong Kim",
      "Junxiong Wang",
      "Jing Nathan Yan",
      "Mohamed Abdelfattah",
      "Alexander M. Rush"
    ],
    "summary": "Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.",
    "published": "Aug 11",
    "pdf_url": "https://arxiv.org/pdf/2508.08446v1",
    "arxiv_url": "http://arxiv.org/abs/2508.08446v1",
    "queried_author": "Alexander M Rush",
    "matching_authors": [
      "Alexander M Rush",
      "Alexander M. Rush"
    ]
  },
  {
    "title": "LL3M: Large Language 3D Modelers",
    "authors": [
      "Sining Lu",
      "Guan Chen",
      "Nam Anh Dinh",
      "Itai Lang",
      "Ari Holtzman",
      "Rana Hanocka"
    ],
    "summary": "We present LL3M, a multi-agent system that leverages pretrained large language models (LLMs) to generate 3D assets by writing interpretable Python code in Blender. We break away from the typical generative approach that learns from a collection of 3D data. Instead, we reformulate shape generation as a code-writing task, enabling greater modularity, editability, and integration with artist workflows. Given a text prompt, LL3M coordinates a team of specialized LLM agents to plan, retrieve, write, debug, and refine Blender scripts that generate and edit geometry and appearance. The generated code works as a high-level, interpretable, human-readable, well-documented representation of scenes and objects, making full use of sophisticated Blender constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse, unconstrained shapes, materials, and scenes. This code presents many avenues for further agent and human editing and experimentation via code tweaks or procedural parameters. This medium naturally enables a co-creative loop in our system: agents can automatically self-critique using code and visuals, while iterative user instructions provide an intuitive way to refine assets. A shared code context across agents enables awareness of previous attempts, and a retrieval-augmented generation knowledge base built from Blender API documentation, BlenderRAG, equips agents with examples, types, and functions empowering advanced modeling operations and code correctness. We demonstrate the effectiveness of LL3M across diverse shape categories, style and material edits, and use...",
    "published": "Aug 11",
    "pdf_url": "https://arxiv.org/pdf/2508.08228v1",
    "arxiv_url": "http://arxiv.org/abs/2508.08228v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning",
    "authors": [
      "Wenkai Li",
      "Liwen Sun",
      "Zhenxiang Guan",
      "Xuhui Zhou",
      "Maarten Sap"
    ],
    "summary": "Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning into specialized subtasks (extraction, classification), reducing the information load on any single agent while enabling iterative validation and more reliable adherence to contextual privacy norms. To understand how privacy errors emerge and propagate, we conduct a systematic ablation over information-flow topologies, revealing when and why upstream detection mistakes cascade into downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with several open-source and closed-sourced LLMs demonstrate that our best multi-agent configuration substantially reduces private information leakage (\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while preserving the fidelity of public content, outperforming single-agent baselines. These results highlight the promise of principled information-flow design in multi-agent systems for contextual privacy with LLMs.",
    "published": "Aug 11",
    "pdf_url": "https://arxiv.org/pdf/2508.07667v3",
    "arxiv_url": "http://arxiv.org/abs/2508.07667v3",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Position: Beyond Sensitive Attributes, ML Fairness Should Quantify Structural Injustice via Social Determinants",
    "authors": [
      "Zeyu Tang",
      "Alex John London",
      "Atoosa Kasirzadeh",
      "Sarah Stewart de Ramirez",
      "Peter Spirtes",
      "Kun Zhang",
      "Sanmi Koyejo"
    ],
    "summary": "Algorithmic fairness research has largely framed unfairness as discrimination along sensitive attributes. However, this approach limits visibility into unfairness as structural injustice instantiated through social determinants, which are contextual variables that shape attributes and outcomes without pertaining to specific individuals. This position paper argues that the field should quantify structural injustice via social determinants, beyond sensitive attributes. Drawing on cross-disciplinary insights, we argue that prevailing technical paradigms fail to adequately capture unfairness as structural injustice, because contexts are potentially treated as noise to be normalized rather than signal to be audited. We further demonstrate the practical urgency of this shift through a theoretical model of college admissions, a demographic study using U.S. census data, and a high-stakes domain application regarding breast cancer screening within an integrated U.S. healthcare system. Our results indicate that mitigation strategies centered solely on sensitive attributes can introduce new forms of structural injustice. We contend that auditing structural injustice through social determinants must precede mitigation, and call for new technical developments that move beyond sensitive-attribute-centered notions of fairness as non-discrimination.",
    "published": "Aug 10",
    "pdf_url": "https://arxiv.org/pdf/2508.08337v2",
    "arxiv_url": "http://arxiv.org/abs/2508.08337v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge",
    "authors": [
      "Jean de Dieu Nyandwi",
      "Yueqi Song",
      "Simran Khanuja",
      "Graham Neubig"
    ],
    "summary": "Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems.",
    "published": "Aug 10",
    "pdf_url": "https://arxiv.org/pdf/2508.07414v2",
    "arxiv_url": "http://arxiv.org/abs/2508.07414v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Post-training for Efficient Communication via Convention Formation",
    "authors": [
      "Yilun Hua",
      "Evan Wang",
      "Yoav Artzi"
    ],
    "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.",
    "published": "Aug 08",
    "pdf_url": "https://arxiv.org/pdf/2508.06482v1",
    "arxiv_url": "http://arxiv.org/abs/2508.06482v1",
    "queried_author": "Yoav Artzi",
    "matching_authors": [
      "Yoav Artzi"
    ]
  },
  {
    "title": "Devstral: Fine-tuning Language Models for Coding Agent Applications",
    "authors": [
      "Abhinav Rastogi",
      "Adam Yang",
      "Albert Q. Jiang",
      "Alexander H. Liu",
      "Alexandre Sablayrolles",
      "Am\u00e9lie H\u00e9liou",
      "Am\u00e9lie Martin",
      "Anmol Agarwal",
      "Andy Ehrenberg",
      "Andy Lo",
      "Antoine Roux",
      "Arthur Darcet",
      "Arthur Mensch",
      "Baptiste Bout",
      "Baptiste Rozi\u00e8re",
      "Baudouin De Monicault",
      "Chris Bamford",
      "Christian Wallenwein",
      "Christophe Renaudin",
      "Cl\u00e9mence Lanfranchi",
      "Cl\u00e9ment Denoix",
      "Corentin Barreau",
      "Darius Dabert Devon Mizelle",
      "Diego de las Casas",
      "Elliot Chane-Sane",
      "Emilien Fugier",
      "Emma Bou Hanna",
      "Gabrielle Berrada",
      "Gauthier Delerce",
      "Gauthier Guinet",
      "Georgii Novikov",
      "Graham Neubig",
      "Guillaume Lample",
      "Guillaume Martin",
      "Himanshu Jaju",
      "Jan Ludziejewski",
      "Jason Rute",
      "Jean-Malo Delignon",
      "JeanHadrien Chabran",
      "Joachim Studnia",
      "Joep Barmentlo",
      "Jonas Amar",
      "Josselin Somerville Roberts",
      "Julien Denize",
      "Karan Saxena",
      "Karmesh Yadav",
      "Kartik Khandelwal",
      "Khyathi Raghavi Chandu",
      "Kush Jain",
      "L\u00e9lio Renard Lavaud",
      "L\u00e9onard Blier",
      "Lingxiao Zhao",
      "Louis Martin",
      "Lucile Saulnier",
      "Luyu Gao",
      "Marie Pellat",
      "Mathilde Guillaumin",
      "Mathis Felardos",
      "Matthieu Dinot",
      "Maxime Darrin",
      "Maximilian Augustin",
      "Micka\u00ebl Seznec",
      "Neha Gupta",
      "Nikhil Raghuraman",
      "Olivier Duchenne",
      "Patricia Wang",
      "Patrick von Platen",
      "Patryk Saffer",
      "Paul Jacob",
      "Paul Wambergue",
      "Paula Kurylowicz",
      "Philom\u00e8ne Chagniot",
      "Pierre Stock",
      "Pravesh Agrawal",
      "R\u00e9mi Delacourt",
      "Roman Soletskyi",
      "Romain Sauvestre",
      "Sagar Vaze",
      "Sanchit Gandhi",
      "Sandeep Subramanian",
      "Shashwat Dalal",
      "Siddharth Gandhi",
      "Soham Ghosh",
      "Srijan Mishra",
      "Sumukh Aithal",
      "Szymon Antoniak",
      "Teven Le Scao",
      "Thibaut Lavril",
      "Thibault Schueller",
      "Thomas Foubert",
      "Thomas Robert",
      "Thomas Wang",
      "Timoth\u00e9e Lacroix",
      "Tom Bewley",
      "Valeriia Nemychnikova",
      "Victor Paltz",
      "Virgile Richard",
      "Wen-Ding Li",
      "William Marshall",
      "Xingyao Wang",
      "Xuanyu Zhang",
      "Yihan Wan",
      "Yunhao Tang"
    ],
    "summary": "We introduce Devstral-Small, a lightweight open source model for code agents with the best performance among models below 100B size. In this technical report, we give an overview of how we design and develop a model and craft specializations in agentic software development. The resulting model, Devstral-Small is a small 24B model, fast and easy to serve. Despite its size, Devstral-Small still attains competitive performance compared to models more than an order of magnitude larger.",
    "published": "Aug 08",
    "pdf_url": "https://arxiv.org/pdf/2509.25193v1",
    "arxiv_url": "http://arxiv.org/abs/2509.25193v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes",
    "authors": [
      "Zachary Robertson",
      "Sanmi Koyejo"
    ],
    "summary": "We study evaluation of AI systems without ground truth by exploiting a link between strategic gaming and information loss. We analyze which information-theoretic mechanisms resist adversarial manipulation, extending finite-sample bounds to show that bounded f-divergences (e.g., total variation distance) maintain polynomial guarantees under attacks while unbounded measures (e.g., KL divergence) degrade exponentially. To implement these mechanisms, we model the overseer as an agent and characterize incentive-compatible scoring rules as f-mutual information objectives. Under adversarial attacks, TVD-MI maintains effectiveness (area under curve 0.70-0.77) while traditional judge queries are near change (AUC $\\approx$ 0.50), demonstrating that querying the same LLM for information relationships rather than quality judgments provides both theoretical and practical robustness. The mechanisms decompose pairwise evaluations into reliable item-level quality scores without ground truth, addressing a key limitation of traditional peer prediction. We release preregistration and code.",
    "published": "Aug 07",
    "pdf_url": "https://arxiv.org/pdf/2508.05469v2",
    "arxiv_url": "http://arxiv.org/abs/2508.05469v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization",
    "authors": [
      "Negar Foroutan",
      "Clara Meister",
      "Debjit Paul",
      "Joel Niklaus",
      "Sina Ahmadi",
      "Antoine Bosselut",
      "Rico Sennrich"
    ],
    "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with <UNK> placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds. To remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity. We find empirically that Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.",
    "published": "Aug 06",
    "pdf_url": "https://arxiv.org/pdf/2508.04796v2",
    "arxiv_url": "http://arxiv.org/abs/2508.04796v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs",
    "authors": [
      "Noah Ziems",
      "Dilara Soylu",
      "Lakshya A Agrawal",
      "Isaac Miller",
      "Liheng Lai",
      "Chen Qian",
      "Kaiqiang Song",
      "Meng Jiang",
      "Dan Klein",
      "Matei Zaharia",
      "Karel D'Oosterlinck",
      "Christopher Potts",
      "Omar Khattab"
    ],
    "summary": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.",
    "published": "Aug 06",
    "pdf_url": "https://arxiv.org/pdf/2508.04660v1",
    "arxiv_url": "http://arxiv.org/abs/2508.04660v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs",
    "authors": [
      "Aryan Gulati",
      "Brando Miranda",
      "Eric Chen",
      "Emily Xia",
      "Kai Fronsdal",
      "Bruno Dumont",
      "Elyas Obbad",
      "Sanmi Koyejo"
    ],
    "summary": "Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.",
    "published": "Aug 05",
    "pdf_url": "https://arxiv.org/pdf/2508.08292v2",
    "arxiv_url": "http://arxiv.org/abs/2508.08292v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction",
    "authors": [
      "Yong Lin",
      "Shange Tang",
      "Bohan Lyu",
      "Ziran Yang",
      "Jui-Hui Chung",
      "Haoyu Zhao",
      "Lai Jiang",
      "Yihan Geng",
      "Jiawei Ge",
      "Jingruo Sun",
      "Jiayun Wu",
      "Jiri Gesi",
      "Ximing Lu",
      "David Acuna",
      "Kaiyu Yang",
      "Hongzhou Lin",
      "Yejin Choi",
      "Danqi Chen",
      "Sanjeev Arora",
      "Chi Jin"
    ],
    "summary": "We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. ...",
    "published": "Aug 05",
    "pdf_url": "https://arxiv.org/pdf/2508.03613v1",
    "arxiv_url": "http://arxiv.org/abs/2508.03613v1",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen",
      "Yejin Choi"
    ]
  },
  {
    "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2",
    "authors": [
      "Oliver McLaughlin",
      "Arjun Khurana",
      "Jack Merullo"
    ],
    "summary": "Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding. In this work, we investigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic information. Our results suggest that Llama uses a rich internal model of phonemes to complete phonetic tasks. We provide evidence for high-level organization of phoneme representations in its latent space. In doing so, we also identify a ``phoneme mover head\" which promotes phonetic information during rhyming tasks. We visualize the output space of this head and find that, while notable differences exist, Llama learns a model of vowels similar to the standard IPA vowel chart for humans, despite receiving no direct supervision to do so.",
    "published": "Aug 04",
    "pdf_url": "https://arxiv.org/pdf/2508.02527v2",
    "arxiv_url": "http://arxiv.org/abs/2508.02527v2",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "WebDS: An End-to-End Benchmark for Web-based Data Science",
    "authors": [
      "Ethan Hsu",
      "Hong Meng Yam",
      "Ines Bouissou",
      "Aaron Murali John",
      "Raj Thota",
      "Josh Koe",
      "Vivek Sarath Putta",
      "G K Dharesan",
      "Alexander Spangher",
      "Shikhar Murty",
      "Tenghao Huang",
      "Christopher D. Manning"
    ],
    "summary": "A large portion of real-world data science tasks are complex and require multi-hop web-based interactions: finding appropriate data available on the internet, synthesizing real-time data of various modalities from different locations, and producing summarized analyses. Existing web benchmarks often focus on simplistic interactions, such as form submissions or e-commerce transactions, and often do not require diverse tool-using capabilities required for web based data science. Conversely, traditional data science benchmarks typically concentrate on static, often textually bound datasets and do not assess end-to-end workflows that encompass data acquisition, cleaning, analysis, and insight generation. In response, we introduce WebDS, the first end-to-end web-based data science benchmark. It comprises 870 web-based data science tasks across 29 diverse websites from structured government data portals to unstructured news media, challenging agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats that better reflect the realities of modern data analytics. Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web Voyager, successfully completes only 15% of tasks in WebDS, which our analysis suggests is due to new failure modes like poor information grounding, repetitive behavior and shortcut-taking that agents performing WebDS' tasks display. By providing a more robust and realistic testing ground, WebDS sets the stag...",
    "published": "Aug 02",
    "pdf_url": "https://arxiv.org/pdf/2508.01222v1",
    "arxiv_url": "http://arxiv.org/abs/2508.01222v1",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal",
    "authors": [
      "Yuhan Liu",
      "Michael J. Q. Zhang",
      "Eunsol Choi"
    ],
    "summary": "Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting implicit user feedback from user-LM interaction logs. We study two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation logs, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. Specifically, we study whether incorporating the contents of user feedback (e.g., user wanted clarification), in addition to the polarity of the feedback, can improve the model performance. We observe mixed results, showing this helps in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.",
    "published": "Jul 30",
    "pdf_url": "https://arxiv.org/pdf/2507.23158v2",
    "arxiv_url": "http://arxiv.org/abs/2507.23158v2",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
    "authors": [
      "Yung-Sung Chuang",
      "Yang Li",
      "Dong Wang",
      "Ching-Feng Yeh",
      "Kehan Lyu",
      "Ramya Raghavendra",
      "James Glass",
      "Lifei Huang",
      "Jason Weston",
      "Luke Zettlemoyer",
      "Xinlei Chen",
      "Zhuang Liu",
      "Saining Xie",
      "Wen-tau Yih",
      "Shang-Wen Li",
      "Hu Xu"
    ],
    "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.",
    "published": "Jul 29",
    "pdf_url": "https://arxiv.org/pdf/2507.22062v3",
    "arxiv_url": "http://arxiv.org/abs/2507.22062v3",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition",
    "authors": [
      "Andy Zou",
      "Maxwell Lin",
      "Eliot Jones",
      "Micha Nowak",
      "Mateusz Dziemian",
      "Nick Winter",
      "Alexander Grattan",
      "Valent Nathanael",
      "Ayla Croft",
      "Xander Davies",
      "Jai Patel",
      "Robert Kirk",
      "Nate Burnikell",
      "Yarin Gal",
      "Dan Hendrycks",
      "J. Zico Kolter",
      "Matt Fredrikson"
    ],
    "summary": "Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.",
    "published": "Jul 28",
    "pdf_url": "https://arxiv.org/pdf/2507.20526v1",
    "arxiv_url": "http://arxiv.org/abs/2507.20526v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations",
    "authors": [
      "Eunkyu Park",
      "Wesley Hanwen Deng",
      "Gunhee Kim",
      "Motahhare Eslami",
      "Maarten Sap"
    ],
    "summary": "Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.",
    "published": "Jul 27",
    "pdf_url": "https://arxiv.org/pdf/2507.20409v1",
    "arxiv_url": "http://arxiv.org/abs/2507.20409v1",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
    "authors": [
      "Lakshya A Agrawal",
      "Shangyin Tan",
      "Dilara Soylu",
      "Noah Ziems",
      "Rishi Khare",
      "Krista Opsahl-Ong",
      "Arnav Singhvi",
      "Herumb Shandilya",
      "Michael J Ryan",
      "Meng Jiang",
      "Christopher Potts",
      "Koushik Sen",
      "Alexandros G. Dimakis",
      "Ion Stoica",
      "Dan Klein",
      "Matei Zaharia",
      "Omar Khattab"
    ],
    "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language often provides a much richer learning medium for LLMs, compared to policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across six tasks, GEPA outperforms GRPO by 6% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% (e.g., +12% accuracy on AIME-2025), and demonstrates promising results as an inference-time search strategy for code optimization. We release our code at https://github.com/gepa-ai/gepa .",
    "published": "Jul 25",
    "pdf_url": "https://arxiv.org/pdf/2507.19457v2",
    "arxiv_url": "http://arxiv.org/abs/2507.19457v2",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences",
    "authors": [
      "Yusuke Hirota",
      "Boyi Li",
      "Ryo Hachiuma",
      "Yueh-Hua Wu",
      "Boris Ivanovic",
      "Yuta Nakashima",
      "Marco Pavone",
      "Yejin Choi",
      "Yu-Chiang Frank Wang",
      "Chao-Han Huck Yang"
    ],
    "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.",
    "published": "Jul 25",
    "pdf_url": "https://arxiv.org/pdf/2507.19362v1",
    "arxiv_url": "http://arxiv.org/abs/2507.19362v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Closing the Modality Gap for Mixed Modality Search",
    "authors": [
      "Binxu Li",
      "Yuhui Zhang",
      "Xiaohan Wang",
      "Weixin Liang",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ],
    "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet underexplored real-world application. In this work, we investigate how contrastive vision-language models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.",
    "published": "Jul 25",
    "pdf_url": "https://arxiv.org/pdf/2507.19054v1",
    "arxiv_url": "http://arxiv.org/abs/2507.19054v1",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt"
    ]
  },
  {
    "title": "Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator",
    "authors": [
      "YuXin Li",
      "Felix Dangel",
      "Derek Tam",
      "Colin Raffel"
    ],
    "summary": "The diagonal of a model's Fisher Information Matrix (the \"Fisher diagonal\") has frequently been used as a way to measure parameter sensitivity. Typically, the Fisher diagonal is estimated via squared sampled gradients of the model's likelihood with respect to its parameters, averaged over a few hundred or thousand examples -- a process which incurs nontrivial computational costs. At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training. This paper therefore explores whether an approximation of the Fisher diagonal can be obtained \"for free\" by recycling the squared gradient accumulator that has already been computed over the course of training. Through a comprehensive set of experiments covering five applications of the Fisher diagonal, we demonstrate that the \"Squisher\" (SQUared gradient accumulator as an approximation of the FISHER) consistently performs similarly to the Fisher diagonal while outperforming baseline methods. Additionally, we clarify the exact differences between the Squisher and the Fisher diagonal and provide empirical quantification of their respective impact.",
    "published": "Jul 24",
    "pdf_url": "https://arxiv.org/pdf/2507.18807v1",
    "arxiv_url": "http://arxiv.org/abs/2507.18807v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
    "authors": [
      "Vijay Viswanathan",
      "Yanchao Sun",
      "Shuang Ma",
      "Xiang Kong",
      "Meng Cao",
      "Graham Neubig",
      "Tongshuang Wu"
    ],
    "summary": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs.",
    "published": "Jul 24",
    "pdf_url": "https://arxiv.org/pdf/2507.18624v2",
    "arxiv_url": "http://arxiv.org/abs/2507.18624v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "A New Pair of GloVes",
    "authors": [
      "Riley Carlson",
      "John Bauer",
      "Christopher D. Manning"
    ],
    "summary": "This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.",
    "published": "Jul 24",
    "pdf_url": "https://arxiv.org/pdf/2507.18103v1",
    "arxiv_url": "http://arxiv.org/abs/2507.18103v1",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation",
    "authors": [
      "Jaechul Roh",
      "Zachary Novack",
      "Yuefeng Peng",
      "Niloofar Mireshghallah",
      "Taylor Berg-Kirkpatrick",
      "Amir Houmansadr"
    ],
    "summary": "Generative AI systems for music and video commonly use text-based filters to prevent regurgitation of copyrighted material. We expose a significant vulnerability in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization--the tendency of models to bind sub-lexical acoustic patterns (phonemes, rhyme, stress, cadence) to memorized copyrighted content. APT replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving phonetic structure while evading lexical filters. We evaluate APT on leading lyrics-to-song models (Suno, YuE) across English and Korean songs spanning rap, pop, and K-pop. APT achieves 91% average similarity to copyrighted originals, versus 13.7% for random lyrics and 42.2% for semantic paraphrases. Embedding analysis confirms the mechanism: YuE's text encoder treats APT-modified lyrics as near-identical to originals (cosine similarity 0.90) while Sentence-BERT semantic similarity drops to 0.71, showing the model encodes phonetic structure over meaning. This vulnerability extends cross-modally--Veo 3 reconstructs visual scenes from original music videos when prompted with APT lyrics alone, despite no visual cues in the prompt. We further show that phonetic-semantic defense signatures fail, as APT prompts exhibit higher semantic similarity than benign paraphrases. Our findings reveal that sub-lexical acoustic structure acts as a cross-modal retrieval key, rendering current copyright filters sys...",
    "published": "Jul 23",
    "pdf_url": "https://arxiv.org/pdf/2507.17937v4",
    "arxiv_url": "http://arxiv.org/abs/2507.17937v4",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah"
    ]
  },
  {
    "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
    "authors": [
      "Mehul Damani",
      "Isha Puri",
      "Stewart Slocum",
      "Idan Shenfeld",
      "Leshem Choshen",
      "Yoon Kim",
      "Jacob Andreas"
    ],
    "summary": "When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate tha...",
    "published": "Jul 22",
    "pdf_url": "https://arxiv.org/pdf/2507.16806v1",
    "arxiv_url": "http://arxiv.org/abs/2507.16806v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas",
      "Yoon Kim"
    ]
  },
  {
    "title": "Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning",
    "authors": [
      "Helena Casademunt",
      "Caden Juang",
      "Adam Karvonen",
      "Samuel Marks",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.",
    "published": "Jul 22",
    "pdf_url": "https://arxiv.org/pdf/2507.16795v2",
    "arxiv_url": "http://arxiv.org/abs/2507.16795v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Distributional Machine Unlearning via Selective Data Removal",
    "authors": [
      "Youssef Allouah",
      "Rachid Guerraoui",
      "Sanmi Koyejo"
    ],
    "summary": "Machine learning systems increasingly face requirements to remove entire domains of information--such as toxic language or biases--rather than individual user data. This task presents a dilemma: full removal of the unwanted domain data is computationally expensive, while random partial removal is statistically inefficient. We find that a domain's statistical influence is often concentrated in a small subset of its data samples, suggesting a path between ineffective partial removal and unnecessary complete removal. We formalize this as distributional unlearning: a framework to select a small subset that balances forgetting an unwanted distribution while preserving a desired one. Using Kullback-Leibler divergence constraints, we derive the exact removal-preservation Pareto frontier for Gaussian distributions and prove that models trained on the edited data achieve corresponding log-loss bounds. We propose a distance-based selection algorithm and show it is quadratically more sample-efficient than random removal in the challenging low-divergence regime. Experiments across synthetic, text, and image datasets (Jigsaw, CIFAR-10, SMS spam) show our method requires 15-82% less deletion than full removal for strong unlearning effects, e.g., halving initial forget set accuracy. Ultimately, by showing a small forget set often suffices, our framework lays the foundations for more scalable and rigorous subpopulation unlearning.",
    "published": "Jul 20",
    "pdf_url": "https://arxiv.org/pdf/2507.15112v4",
    "arxiv_url": "http://arxiv.org/abs/2507.15112v4",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "The Invisible Leash: Why RLVR May or May Not Escape Its Origin",
    "authors": [
      "Fang Wu",
      "Weihao Xuan",
      "Ximing Lu",
      "Mingjie Liu",
      "Yi Dong",
      "Zaid Harchaoui",
      "Yejin Choi"
    ],
    "summary": "Recent advances highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing LLMs' capabilities. However, it remains unclear whether the current practice of RLVR truly expands a model's reasoning boundary or mainly amplifies high-reward outputs that the base model already knows, thereby improving precision. This study presents an empirical investigation that provides fresh insights into the limits of RLVR. We examine how RLVR can operate as a support-constrained optimization mechanism that may restrict the discovery of entirely original solutions, remaining constrained by the base model's initial distribution. We also identify an entropy-reward trade-off: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves \\texttt{pass@1}, \\textit{the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets}, failing to recover correct answers that were previously accessible to the base model. Interestingly, while RLVR sometimes increases token-level entropy, it results in greater uncertainty at each generation step and declining answer-level entropy. This indicates that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, we reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash requires future innovations that seed probabilit...",
    "published": "Jul 20",
    "pdf_url": "https://arxiv.org/pdf/2507.14843v4",
    "arxiv_url": "http://arxiv.org/abs/2507.14843v4",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Inverse Scaling in Test-Time Compute",
    "authors": [
      "Aryo Pradipta Gema",
      "Alexander H\u00e4gele",
      "Runjin Chen",
      "Andy Arditi",
      "Jacob Goldman-Wetzler",
      "Kit Fraser-Taliente",
      "Henry Sleight",
      "Linda Petrini",
      "Julian Michael",
      "Beatrice Alex",
      "Pasquale Minervini",
      "Yanda Chen",
      "Joe Benton",
      "Ethan Perez"
    ],
    "summary": "We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.",
    "published": "Jul 19",
    "pdf_url": "https://arxiv.org/pdf/2507.14417v2",
    "arxiv_url": "http://arxiv.org/abs/2507.14417v2",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Intent-Aware Schema Generation And Refinement For Literature Review Tables",
    "authors": [
      "Vishakh Padmakumar",
      "Joseph Chee Chang",
      "Kyle Lo",
      "Doug Downey",
      "Aakanksha Naik"
    ],
    "summary": "The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with \\emph{synthesized intents}, and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Next, we propose several LLM-based schema refinement techniques and show that these can further improve schemas generated by these methods.",
    "published": "Jul 18",
    "pdf_url": "https://arxiv.org/pdf/2507.19521v2",
    "arxiv_url": "http://arxiv.org/abs/2507.19521v2",
    "queried_author": "Kyle Lo",
    "matching_authors": [
      "Kyle Lo"
    ]
  },
  {
    "title": "AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation",
    "authors": [
      "Potsawee Manakul",
      "Woody Haosheng Gan",
      "Michael J. Ryan",
      "Ali Sartaz Khan",
      "Warit Sirichotedumrong",
      "Kunat Pipatanakul",
      "William Held",
      "Diyi Yang"
    ],
    "summary": "Current speech evaluation suffers from two critical limitations: the need and difficulty of designing specialized systems targeting individual audio characteristics, and poor correlation between automatic evaluation methods and human preferences. This work presents a systematic study of Large Audio Model (LAM) as a Judge, AudioJudge, investigating whether it can provide a unified evaluation framework that addresses both challenges. We systematically explore AudioJudge across audio characteristic detection tasks, including pronunciation, speaking rate, speaker identification and speech quality, and system-level human preference simulation for automated benchmarking. We investigate different prompt engineering strategies, finding that audio concatenation combined with in-context learning significantly improves performance across both audio characteristic detection and human preference simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to enable general-purpose multi-aspect audio evaluation. This method decomposes speech assessment into specialized judges for lexical content, speech quality, and paralinguistic features, achieving up to 0.91 Spearman correlation with human preferences on our system ranking benchmark. Robustness analysis reveals that while LAMs maintain strong performance under acoustic noise, they exhibit significant verbosity and positional biases that require careful mitigation.",
    "published": "Jul 17",
    "pdf_url": "https://arxiv.org/pdf/2507.12705v1",
    "arxiv_url": "http://arxiv.org/abs/2507.12705v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "William Held"
    ]
  },
  {
    "title": "Reasoning-Finetuning Repurposes Latent Representations in Base Models",
    "authors": [
      "Jake Ward",
      "Chuqiao Lin",
      "Constantin Venhoff",
      "Neel Nanda"
    ],
    "summary": "Backtracking, an emergent behavior elicited by reasoning fine-tuning, has been shown to be a key mechanism in reasoning models' enhanced capabilities. Prior work has succeeded in manipulating this behavior via steering vectors, but the underlying mechanism remains poorly understood. In this work, we show that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part driven by a repurposed direction already present in base model activations. Specifically, we identify a direction in base Llama-3.1-8B's residual stream which systematically induces backtracking when used to steer the distilled reasoning model, and find that the effects of steering with this direction cannot be trivially explained by token-level attributes. We further find that this direction does not induce backtracking in the base model, suggesting that the reasoning finetuning process repurposes pre-existing representations to form new behavioral circuits. Additionally, we hypothesize that this direction is one of several which may work together to mediate backtracking. Our findings offer a compelling picture that reasoning-finetuned models repurpose pre-existing base model representations, rather than learn new capabilities from scratch.",
    "published": "Jul 16",
    "pdf_url": "https://arxiv.org/pdf/2507.12638v1",
    "arxiv_url": "http://arxiv.org/abs/2507.12638v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training",
    "authors": [
      "Mingjie Liu",
      "Shizhe Diao",
      "Jian Hu",
      "Ximing Lu",
      "Xin Dong",
      "Hao Zhang",
      "Alexander Bukharin",
      "Shaokun Zhang",
      "Jiaqi Zeng",
      "Makesh Narsimhan Sreedhar",
      "Gerald Shen",
      "David Mosallanezhad",
      "Di Zhang",
      "Jonas Yang",
      "June Yang",
      "Oleksii Kuchaiev",
      "Guilin Liu",
      "Zhiding Yu",
      "Pavlo Molchanov",
      "Yejin Choi",
      "Jan Kautz",
      "Yi Dong"
    ],
    "summary": "Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate continued research, we release our model publicly.",
    "published": "Jul 16",
    "pdf_url": "https://arxiv.org/pdf/2507.12507v1",
    "arxiv_url": "http://arxiv.org/abs/2507.12507v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety",
    "authors": [
      "Tomek Korbak",
      "Mikita Balesni",
      "Elizabeth Barnes",
      "Yoshua Bengio",
      "Joe Benton",
      "Joseph Bloom",
      "Mark Chen",
      "Alan Cooney",
      "Allan Dafoe",
      "Anca Dragan",
      "Scott Emmons",
      "Owain Evans",
      "David Farhi",
      "Ryan Greenblatt",
      "Dan Hendrycks",
      "Marius Hobbhahn",
      "Evan Hubinger",
      "Geoffrey Irving",
      "Erik Jenner",
      "Daniel Kokotajlo",
      "Victoria Krakovna",
      "Shane Legg",
      "David Lindner",
      "David Luan",
      "Aleksander M\u0105dry",
      "Julian Michael",
      "Neel Nanda",
      "Dave Orr",
      "Jakub Pachocki",
      "Ethan Perez",
      "Mary Phuong",
      "Fabien Roger",
      "Joshua Saxe",
      "Buck Shlegeris",
      "Mart\u00edn Soto",
      "Eric Steinberger",
      "Jasmine Wang",
      "Wojciech Zaremba",
      "Bowen Baker",
      "Rohin Shah",
      "Vlad Mikulik"
    ],
    "summary": "AI systems that \"think\" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.",
    "published": "Jul 15",
    "pdf_url": "https://arxiv.org/pdf/2507.11473v2",
    "arxiv_url": "http://arxiv.org/abs/2507.11473v2",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez",
      "Neel Nanda"
    ]
  },
  {
    "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
    "authors": [
      "Orion Weller",
      "Kathryn Ricci",
      "Marc Marone",
      "Antoine Chaffin",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "summary": "The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.",
    "published": "Jul 15",
    "pdf_url": "https://arxiv.org/pdf/2507.11412v1",
    "arxiv_url": "http://arxiv.org/abs/2507.11412v1",
    "queried_author": "Orion Weller",
    "matching_authors": [
      "Orion Weller"
    ]
  },
  {
    "title": "The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality",
    "authors": [
      "Benjamin Newman",
      "Abhilasha Ravichander",
      "Jaehun Jung",
      "Rui Xin",
      "Hamish Ivison",
      "Yegor Kuznetsov",
      "Pang Wei Koh",
      "Yejin Choi"
    ],
    "summary": "Language models are prone to hallucination - generating text that is factually incorrect. Finetuning models on high-quality factual information can potentially reduce hallucination, but concerns remain; obtaining factual gold data can be expensive and training on correct but unfamiliar data may potentially lead to even more downstream hallucination. What data should practitioners finetune on to mitigate hallucinations in language models? In this work, we study the relationship between the factuality of finetuning data and the prevalence of hallucinations in long-form generation tasks. Counterintuitively, we find that finetuning on factual gold data is not as helpful as finetuning on model-generated data that models believe to be factual. Next, we evaluate filtering strategies applied on both factual gold data and model-generated data, and find that finetuning on model-generated data that is filtered by models' own internal judgments often leads to better overall factuality compared to other configurations: training on gold data filtered by models' judgments, training on gold data alone, or training on model-generated data that is supported by gold data. These factuality improvements transfer across three domains we study, suggesting that a models' own beliefs can provide a powerful signal for factuality.",
    "published": "Jul 11",
    "pdf_url": "https://arxiv.org/pdf/2507.08371v1",
    "arxiv_url": "http://arxiv.org/abs/2507.08371v1",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison",
      "Pang Wei Koh",
      "Yejin Choi"
    ]
  },
  {
    "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
    "authors": [
      "Atticus Wang",
      "Joshua Engels",
      "Oliver Clive-Griffin",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs exhibit surprisingly deep out-of-distribution generalization. Rather than learning shallow heuristics, they implicitly internalize and act on the consequences of observations scattered throughout the fine-tuning data. In this work, we investigate this phenomenon mechanistically and find that many instances of OOCR in the literature have a simple explanation: the LoRA fine-tuning essentially adds a constant steering vector, steering the model towards a general concept. This improves performance on the fine-tuning task and in many other concept-related domains, causing the surprising generalization. Moreover, we can directly train steering vectors for these tasks from scratch, which also induces OOCR. We find that our results hold even for a task that seems like it must involve conditional behavior (model backdoors); it turns out that unconditionally adding a steering vector is sufficient. Overall, our work presents one explanation of what gets learned during fine-tuning for OOCR tasks, contributing to the key question of why LLMs can reason out of context, an advanced capability that is highly relevant to their safe and reliable deployment.",
    "published": "Jul 10",
    "pdf_url": "https://arxiv.org/pdf/2507.08218v2",
    "arxiv_url": "http://arxiv.org/abs/2507.08218v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling",
    "authors": [
      "Sukjun Hwang",
      "Brandon Wang",
      "Albert Gu"
    ],
    "summary": "Major progress on language models (LMs) in recent years has largely resulted from moving away from specialized models designed for specific tasks, to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data. Despite this trend, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. We introduce a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content- and context- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching the token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improve...",
    "published": "Jul 10",
    "pdf_url": "https://arxiv.org/pdf/2507.07955v2",
    "arxiv_url": "http://arxiv.org/abs/2507.07955v2",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "FlexOlmo: Open Language Models for Flexible Data Use",
    "authors": [
      "Weijia Shi",
      "Akshita Bhagia",
      "Kevin Farhat",
      "Niklas Muennighoff",
      "Pete Walsh",
      "Jacob Morrison",
      "Dustin Schwenk",
      "Shayne Longpre",
      "Jake Poznanski",
      "Allyson Ettinger",
      "Daogao Liu",
      "Margaret Li",
      "Dirk Groeneveld",
      "Mike Lewis",
      "Wen-tau Yih",
      "Luca Soldaini",
      "Kyle Lo",
      "Noah A. Smith",
      "Luke Zettlemoyer",
      "Pang Wei Koh",
      "Hannaneh Hajishirzi",
      "Ali Farhadi",
      "Sewon Min"
    ],
    "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fi...",
    "published": "Jul 09",
    "pdf_url": "https://arxiv.org/pdf/2507.07024v4",
    "arxiv_url": "http://arxiv.org/abs/2507.07024v4",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Kyle Lo",
      "Luca Soldaini",
      "Luke Zettlemoyer",
      "Mike Lewis",
      "Niklas Muennighoff",
      "Noah A. Smith",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
    "authors": [
      "Zeming Chen",
      "Angelika Romanou",
      "Gail Weiss",
      "Antoine Bosselut"
    ],
    "summary": "Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.",
    "published": "Jul 08",
    "pdf_url": "https://arxiv.org/pdf/2507.06415v2",
    "arxiv_url": "http://arxiv.org/abs/2507.06415v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Humans overrely on overconfident language models, across languages",
    "authors": [
      "Neil Rathi",
      "Dan Jurafsky",
      "Kaitlyn Zhou"
    ],
    "summary": "As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Prior work shows that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., 'I think it's') differs sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate LLM safety in a global context. Our work finds that overreliance risks are high across languages. We first analyze the distribution of LLM-generated epistemic markers and observe that LLMs are overconfident across languages, frequently generating strengtheners even as part of incorrect responses. Model generations are, however, sensitive to documented cross-linguistic variation in usage: for example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. Next, we measure human reliance rates across languages, finding that reliance behaviors differ cross-linguistically: for example, participants are significantly more likely to discount expressions of uncertainty in Japanese than in English (i.e., ignore their 'hedging' function and rely on generations that contain them). Taken together, these results indicate a high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress th...",
    "published": "Jul 08",
    "pdf_url": "https://arxiv.org/pdf/2507.06306v2",
    "arxiv_url": "http://arxiv.org/abs/2507.06306v2",
    "queried_author": "Dan Jurafsky",
    "matching_authors": [
      "Dan Jurafsky"
    ]
  },
  {
    "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains",
    "authors": [
      "Scott Geng",
      "Hamish Ivison",
      "Chun-Liang Li",
      "Maarten Sap",
      "Jerry Li",
      "Ranjay Krishna",
      "Pang Wei Koh"
    ],
    "summary": "Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training. To better understand delta learning, we prove in logistic regression that the performance gap between two weak teacher models provides useful signal for improving a stronger student. Overall, our work shows that models can learn surprisingly well from paired data that might typically be considered weak.",
    "published": "Jul 08",
    "pdf_url": "https://arxiv.org/pdf/2507.06187v1",
    "arxiv_url": "http://arxiv.org/abs/2507.06187v1",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison",
      "Maarten Sap",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety",
    "authors": [
      "Sanidhya Vijayvargiya",
      "Aditya Bharat Soni",
      "Xuhui Zhou",
      "Zora Zhiruo Wang",
      "Nouha Dziri",
      "Graham Neubig",
      "Maarten Sap"
    ],
    "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.",
    "published": "Jul 08",
    "pdf_url": "https://arxiv.org/pdf/2507.06134v2",
    "arxiv_url": "http://arxiv.org/abs/2507.06134v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig",
      "Maarten Sap"
    ]
  },
  {
    "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
    "authors": [
      "Weihua Du",
      "Pranjal Aggarwal",
      "Sean Welleck",
      "Yiming Yang"
    ],
    "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill",
    "published": "Jul 08",
    "pdf_url": "https://arxiv.org/pdf/2507.05707v2",
    "arxiv_url": "http://arxiv.org/abs/2507.05707v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents",
    "authors": [
      "Prahaladh Chandrahasan",
      "Jiahe Jin",
      "Zhihan Zhang",
      "Tevin Wang",
      "Andy Tang",
      "Lucy Mo",
      "Morteza Ziyadi",
      "Leonardo F. R. Ribeiro",
      "Zimeng Qiu",
      "Markus Dreyer",
      "Akari Asai",
      "Chenyan Xiong"
    ],
    "summary": "Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.",
    "published": "Jul 07",
    "pdf_url": "https://arxiv.org/pdf/2507.05495v2",
    "arxiv_url": "http://arxiv.org/abs/2507.05495v2",
    "queried_author": "Akari Asai",
    "matching_authors": [
      "Akari Asai"
    ]
  },
  {
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
    "authors": [
      "Gheorghe Comanici",
      "Eric Bieber",
      "Mike Schaekermann",
      "Ice Pasupat",
      "Noveen Sachdeva",
      "Inderjit Dhillon",
      "Marcel Blistein",
      "Ori Ram",
      "Dan Zhang",
      "Evan Rosen",
      "Luke Marris",
      "Sam Petulla",
      "Colin Gaffney",
      "Asaf Aharoni",
      "Nathan Lintz",
      "Tiago Cardal Pais",
      "Henrik Jacobsson",
      "Idan Szpektor",
      "Nan-Jiang Jiang",
      "Krishna Haridasan",
      "Ahmed Omran",
      "Nikunj Saunshi",
      "Dara Bahri",
      "Gaurav Mishra",
      "Eric Chu",
      "Toby Boyd",
      "Brad Hekman",
      "Aaron Parisi",
      "Chaoyi Zhang",
      "Kornraphop Kawintiranon",
      "Tania Bedrax-Weiss",
      "Oliver Wang",
      "Ya Xu",
      "Ollie Purkiss",
      "Uri Mendlovic",
      "Ila\u00ef Deutel",
      "Nam Nguyen",
      "Adam Langley",
      "Flip Korn",
      "Lucia Rossazza",
      "Alexandre Ram\u00e9",
      "Sagar Waghmare",
      "Helen Miller",
      "Nathan Byrd",
      "Ashrith Sheshan",
      "Raia Hadsell",
      "Sangnie Bhardwaj",
      "Pawel Janus",
      "Tero Rissa",
      "Dan Horgan",
      "Alvin Abdagic",
      "Lior Belenki",
      "James Allingham",
      "Anima Singh",
      "Theo Guidroz",
      "Srivatsan Srinivasan",
      "Herman Schmit",
      "Kristen Chiafullo",
      "Andre Elisseeff",
      "Nilpa Jha",
      "Prateek Kolhar",
      "Leonard Berrada",
      "Frank Ding",
      "Xiance Si",
      "Shrestha Basu Mallick",
      "Franz Och",
      "Sofia Erell",
      "Eric Ni",
      "Tejasi Latkar",
      "Sherry Yang",
      "Petar Sirkovic",
      "Ziqiang Feng",
      "Robert Leland",
      "Rachel Hornung",
      "Gang Wu",
      "Charles Blundell",
      "Hamidreza Alvari",
      "Po-Sen Huang",
      "Cathy Yip",
      "Sanja Deur",
      "Li Liu",
      "Gabriela Surita",
      "Pablo Duque",
      "Dima Damen",
      "Johnson Jia",
      "Arthur Guez",
      "Markus Mircea",
      "Animesh Sinha",
      "Alberto Magni",
      "Pawe\u0142 Stradomski",
      "Tal Marian",
      "Vlado Gali\u0107",
      "Wenhu Chen",
      "Hisham Husain",
      "Achintya Singhal",
      "Dominik Grewe",
      "Fran\u00e7ois-Xavier Aubet",
      "Shuang Song",
      "Lorenzo Blanco",
      "Leland Rechis",
      "Lewis Ho",
      "Rich Munoz",
      "Kelvin Zheng",
      "Jessica Hamrick",
      "Kevin Mather",
      "Hagai Taitelbaum",
      "Eliza Rutherford",
      "Yun Lei",
      "Kuangyuan Chen",
      "Anand Shukla",
      "Erica Moreira",
      "Eric Doi",
      "Berivan Isik",
      "Nir Shabat",
      "Dominika Rogozi\u0144ska",
      "Kashyap Kolipaka",
      "Jason Chang",
      "Eugen Vu\u0161ak",
      "Srinivasan Venkatachary",
      "Shadi Noghabi",
      "Tarun Bharti",
      "Younghoon Jun",
      "Aleksandr Zaks",
      "Simon Green",
      "Jeshwanth Challagundla",
      "William Wong",
      "Muqthar Mohammad",
      "Dean Hirsch",
      "Yong Cheng",
      "Iftekhar Naim",
      "Lev Proleev",
      "Damien Vincent",
      "Aayush Singh",
      "Maxim Krikun",
      "Dilip Krishnan",
      "Zoubin Ghahramani",
      "Aviel Atias",
      "Rajeev Aggarwal",
      "Christo Kirov",
      "Dimitrios Vytiniotis",
      "Christy Koh",
      "Alexandra Chronopoulou",
      "Pawan Dogra",
      "Vlad-Doru Ion",
      "Gladys Tyen",
      "Jason Lee",
      "Felix Weissenberger",
      "Trevor Strohman",
      "Ashwin Balakrishna",
      "Jack Rae",
      "Marko Velic",
      "Raoul de Liedekerke",
      "Oded Elyada",
      "Wentao Yuan",
      "Canoee Liu",
      "Lior Shani",
      "Sergey Kishchenko",
      "Bea Alessio",
      "Yandong Li",
      "Richard Song",
      "Sam Kwei",
      "Orion Jankowski",
      "Aneesh Pappu",
      "Youhei Namiki",
      "Yenai Ma",
      "Nilesh Tripuraneni",
      "Colin Cherry",
      "Marissa Ikonomidis",
      "Yu-Cheng Ling",
      "Colin Ji",
      "Beka Westberg",
      "Auriel Wright",
      "Da Yu",
      "David Parkinson",
      "Swaroop Ramaswamy",
      "Jerome Connor",
      "Soheil Hassas Yeganeh",
      "Snchit Grover",
      "George Kenwright",
      "Lubo Litchev",
      "Chris Apps",
      "Alex Tomala",
      "Felix Halim",
      "Alex Castro-Ros",
      "Zefei Li",
      "Anudhyan Boral",
      "Pauline Sho",
      "Michal Yarom",
      "Eric Malmi",
      "David Klinghoffer",
      "Rebecca Lin",
      "Alan Ansell",
      "Pradeep Kumar S",
      "Shubin Zhao",
      "Siqi Zuo",
      "Adam Santoro",
      "Heng-Tze Cheng",
      "Solomon Demmessie",
      "Yuchi Liu",
      "Nicole Brichtova",
      "Allie Culp",
      "Nathaniel Braun",
      "Dan Graur",
      "Will Ng",
      "Nikhil Mehta",
      "Aaron Phillips",
      "Patrik Sundberg",
      "Varun Godbole",
      "Fangyu Liu",
      "Yash Katariya",
      "David Rim",
      "Mojtaba Seyedhosseini",
      "Sean Ammirati",
      "Jonas Valfridsson",
      "Mahan Malihi",
      "Timothy Knight",
      "Andeep Toor",
      "Thomas Lampe",
      "Abe Ittycheriah",
      "Lewis Chiang",
      "Chak Yeung",
      "Alexandre Fr\u00e9chette",
      "Jinmeng Rao",
      "Huisheng Wang",
      "Himanshu Srivastava",
      "Richard Zhang",
      "Rocky Rhodes",
      "Ariel Brand",
      "Dean Weesner",
      "Ilya Figotin",
      "Felix Gimeno",
      "Rachana Fellinger",
      "Pierre Marcenac",
      "Jos\u00e9 Leal",
      "Eyal Marcus",
      "Victor Cotruta",
      "Rodrigo Cabrera",
      "Sheryl Luo",
      "Dan Garrette",
      "Vera Axelrod",
      "Sorin Baltateanu",
      "David Barker",
      "Dongkai Chen",
      "Horia Toma",
      "Ben Ingram",
      "Jason Riesa",
      "Chinmay Kulkarni",
      "Yujing Zhang",
      "Hongbin Liu",
      "Chao Wang",
      "Martin Polacek",
      "Will Wu",
      "Kai Hui",
      "Adrian N Reyes",
      "Yi Su",
      "Megan Barnes",
      "Ishaan Malhi",
      "Anfal Siddiqui",
      "Qixuan Feng",
      "Mihai Damaschin",
      "Daniele Pighin",
      "Andreas Steiner",
      "Samuel Yang",
      "Ramya Sree Boppana",
      "Simeon Ivanov",
      "Arun Kandoor",
      "Aditya Shah",
      "Asier Mujika",
      "Da Huang",
      "Christopher A. Choquette-Choo",
      "Mohak Patel",
      "Tianhe Yu",
      "Toni Creswell",
      "Jerry",
      "Liu",
      "Catarina Barros",
      "Yasaman Razeghi",
      "Aurko Roy",
      "Phil Culliton",
      "Binbin Xiong",
      "Jiaqi Pan",
      "Thomas Strohmann",
      "Tolly Powell",
      "Babi Seal",
      "Doug DeCarlo",
      "Pranav Shyam",
      "Kaan Katircioglu",
      "Xuezhi Wang",
      "Cassidy Hardin",
      "Immanuel Odisho",
      "Josef Broder",
      "Oscar Chang",
      "Arun Nair",
      "Artem Shtefan",
      "Maura O'Brien",
      "Manu Agarwal",
      "Sahitya Potluri",
      "Siddharth Goyal",
      "Amit Jhindal",
      "Saksham Thakur",
      "Yury Stuken",
      "James Lyon",
      "Kristina Toutanova",
      "Fangxiaoyu Feng",
      "Austin Wu",
      "Ben Horn",
      "Alek Wang",
      "Alex Cullum",
      "Gabe Taubman",
      "Disha Shrivastava",
      "Chongyang Shi",
      "Hamish Tomlinson",
      "Roma Patel",
      "Tao Tu",
      "Ada Maksutaj Oflazer",
      "Francesco Pongetti",
      "Mingyao Yang",
      "Adrien Ali Ta\u00efga",
      "Vincent Perot",
      "Nuo Wang Pierse",
      "Feng Han",
      "Yoel Drori",
      "I\u00f1aki Iturrate",
      "Ayan Chakrabarti",
      "Legg Yeung",
      "Dave Dopson",
      "Yi-ting Chen",
      "Apoorv Kulshreshtha",
      "Tongfei Guo",
      "Philip Pham",
      "Tal Schuster",
      "Junquan Chen",
      "Alex Polozov",
      "Jinwei Xing",
      "Huanjie Zhou",
      "Praneeth Kacham",
      "Doron Kukliansky",
      "Antoine Miech",
      "Sergey Yaroshenko",
      "Ed Chi",
      "Sholto Douglas",
      "Hongliang Fei",
      "Mathieu Blondel",
      "Preethi Myla",
      "Lior Madmoni",
      "Xing Wu",
      "Daniel Keysers",
      "Kristian Kjems",
      "Isabela Albuquerque",
      "Lijun Yu",
      "Joel D'sa",
      "Michelle Plantan",
      "Vlad Ionescu",
      "Jaume Sanchez Elias",
      "Abhirut Gupta",
      "Manish Reddy Vuyyuru",
      "Fred Alcober",
      "Tong Zhou",
      "Kaiyang Ji",
      "Florian Hartmann",
      "Subha Puttagunta",
      "Hugo Song",
      "Ehsan Amid",
      "Anca Stefanoiu",
      "Andrew Lee",
      "Paul Pucciarelli",
      "Emma Wang",
      "Amit Raul",
      "Slav Petrov",
      "Isaac Tian",
      "Valentin Anklin",
      "Nana Nti",
      "Victor Gomes",
      "Max Schumacher",
      "Grace Vesom",
      "Alex Panagopoulos",
      "Konstantinos Bousmalis",
      "Daniel Andor",
      "Josh Jacob",
      "Yuan Zhang",
      "Bill Rosgen",
      "Matija Kecman",
      "Matthew Tung",
      "Alexandra Belias",
      "Noah Goodman",
      "Paul Covington",
      "Brian Wieder",
      "Nikita Saxena",
      "Elnaz Davoodi",
      "Muhuan Huang",
      "Sharath Maddineni",
      "Vincent Roulet",
      "Folawiyo Campbell-Ajala",
      "Pier Giuseppe Sessa",
      "Xintian",
      "Wu",
      "Guangda Lai",
      "Paul Collins",
      "Alex Haig",
      "Vytenis Sakenas",
      "Xiaowei Xu",
      "Marissa Giustina",
      "Laurent El Shafey",
      "Pichi Charoenpanit",
      "Shefali Garg",
      "Joshua Ainslie",
      "Boone Severson",
      "Montse Gonzalez Arenas",
      "Shreya Pathak",
      "Sujee Rajayogam",
      "Jie Feng",
      "Michiel Bakker",
      "Sheng Li",
      "Nevan Wichers",
      "Jamie Rogers",
      "Xinyang Geng",
      "Yeqing Li",
      "Rolf Jagerman",
      "Chao Jia",
      "Nadav Olmert",
      "David Sharon",
      "Matthew Mauger",
      "Sandeep Mariserla",
      "Hongxu Ma",
      "Megha Mohabey",
      "Kyuyeun Kim",
      "Alek Andreev",
      "Scott Pollom",
      "Juliette Love",
      "Vihan Jain",
      "Priyanka Agrawal",
      "Yannick Schroecker",
      "Alisa Fortin",
      "Manfred Warmuth",
      "Ji Liu",
      "Andrew Leach",
      "Irina Blok",
      "Ganesh Poomal Girirajan",
      "Roee Aharoni",
      "Benigno Uria",
      "Andrei Sozanschi",
      "Dan Goldberg",
      "Lucian Ionita",
      "Marco Tulio Ribeiro",
      "Martin Zlocha",
      "Vighnesh Birodkar",
      "Sami Lachgar",
      "Liangzhe Yuan",
      "Himadri Choudhury",
      "Matt Ginsberg",
      "Fei Zheng",
      "Gregory Dibb",
      "Emily Graves",
      "Swachhand Lokhande",
      "Gabriel Rasskin",
      "George-Cristian Muraru",
      "Corbin Quick",
      "Sandeep Tata",
      "Pierre Sermanet",
      "Aditya Chawla",
      "Itay Karo",
      "Yan Wang",
      "Susan Zhang",
      "Orgad Keller",
      "Anca Dragan",
      "Guolong Su",
      "Ian Chou",
      "Xi Liu",
      "Yiqing Tao",
      "Shruthi Prabhakara",
      "Marc Wilson",
      "Ruibo Liu",
      "Shibo Wang",
      "Georgie Evans",
      "David Du",
      "Alfonso Casta\u00f1o",
      "Gautam Prasad",
      "Mona El Mahdy",
      "Sebastian Gerlach",
      "Machel Reid",
      "Jarrod Kahn",
      "Amir Zait",
      "Thanumalayan Sankaranarayana Pillai",
      "Thatcher Ulrich",
      "Guanyu Wang",
      "Jan Wassenberg",
      "Efrat Farkash",
      "Kiran Yalasangi",
      "Congchao Wang",
      "Maria Bauza",
      "Simon Bucher",
      "Ting Liu",
      "Jun Yan",
      "Gary Leung",
      "Vikas Sindhwani",
      "Parker Barnes",
      "Avi Singh",
      "Ivan Jurin",
      "Jichuan Chang",
      "Niket Kumar Bhumihar",
      "Sivan Eiger",
      "Gui Citovsky",
      "Ben Withbroe",
      "Zhang Li",
      "Siyang Xue",
      "Niccol\u00f2 Dal Santo",
      "Georgi Stoyanov",
      "Yves Raimond",
      "Steven Zheng",
      "Yilin Gao",
      "V\u00edt List\u00edk",
      "S\u0142awek Kwasiborski",
      "Rachel Saputro",
      "Adnan Ozturel",
      "Ganesh Mallya",
      "Kushal Majmundar",
      "Ross West",
      "Paul Caron",
      "Jinliang Wei",
      "Lluis Castrejon",
      "Sharad Vikram",
      "Deepak Ramachandran",
      "Nikhil Dhawan",
      "Jiho Park",
      "Sara Smoot",
      "George van den Driessche",
      "Yochai Blau",
      "Chase Malik",
      "Wei Liang",
      "Roy Hirsch",
      "Cicero Nogueira dos Santos",
      "Eugene Weinstein",
      "A\u00e4ron van den Oord",
      "Sid Lall",
      "Nicholas FitzGerald",
      "Zixuan Jiang",
      "Xuan Yang",
      "Dale Webster",
      "Ali Elqursh",
      "Aedan Pope",
      "Georges Rotival",
      "David Raposo",
      "Wanzheng Zhu",
      "Jeff Dean",
      "Sami Alabed",
      "Dustin Tran",
      "Arushi Gupta",
      "Zach Gleicher",
      "Jessica Austin",
      "Edouard Rosseel",
      "Megh Umekar",
      "Dipanjan Das",
      "Yinghao Sun",
      "Kai Chen",
      "Karolis Misiunas",
      "Xiang Zhou",
      "Yixian Di",
      "Alyssa Loo",
      "Josh Newlan",
      "Bo Li",
      "Vinay Ramasesh",
      "Ying Xu",
      "Alex Chen",
      "Sudeep Gandhe",
      "Radu Soricut",
      "Nikita Gupta",
      "Shuguang Hu",
      "Seliem El-Sayed",
      "Xavier Garcia",
      "Idan Brusilovsky",
      "Pu-Chin Chen",
      "Andrew Bolt",
      "Lu Huang",
      "Alex Gurney",
      "Zhiying Zhang",
      "Alexander Pritzel",
      "Jarek Wilkiewicz",
      "Bryan Seybold",
      "Bhargav Kanagal Shamanna",
      "Felix Fischer",
      "Josef Dean",
      "Karan Gill",
      "Ross Mcilroy",
      "Abhishek Bhowmick",
      "Jeremy Selier",
      "Antoine Yang",
      "Derek Cheng",
      "Vladimir Magay",
      "Jie Tan",
      "Dhriti Varma",
      "Christian Walder",
      "Tomas Kocisky",
      "Ryo Nakashima",
      "Paul Natsev",
      "Mike Kwong",
      "Ionel Gog",
      "Chiyuan Zhang",
      "Sander Dieleman",
      "Thomas Jimma",
      "Andrey Ryabtsev",
      "Siddhartha Brahma",
      "David Steiner",
      "Dayou Du",
      "Ante \u017du\u017eul",
      "Mislav \u017dani\u0107",
      "Mukund Raghavachari",
      "Willi Gierke",
      "Zeyu Zheng",
      "Dessie Petrova",
      "Yann Dauphin",
      "Yuchuan Liu",
      "Ido Kessler",
      "Steven Hand",
      "Chris Duvarney",
      "Seokhwan Kim",
      "Hyo Lee",
      "L\u00e9onard Hussenot",
      "Jeffrey Hui",
      "Josh Smith",
      "Deepali Jain",
      "Jiawei Xia",
      "Gaurav Singh Tomar",
      "Keyvan Amiri",
      "Du Phan",
      "Fabian Fuchs",
      "Tobias Weyand",
      "Nenad Tomasev",
      "Alexandra Cordell",
      "Xin Liu",
      "Jonathan Mallinson",
      "Pankaj Joshi",
      "Andy Crawford",
      "Arun Suggala",
      "Steve Chien",
      "Nick Fernando",
      "Mariella Sanchez-Vargas",
      "Duncan Williams",
      "Phil Crone",
      "Xiyang Luo",
      "Igor Karpov",
      "Jyn Shan",
      "Terry Thurk",
      "Robin Strudel",
      "Paul Voigtlaender",
      "Piyush Patil",
      "Tim Dozat",
      "Ali Khodaei",
      "Sahil Singla",
      "Piotr Ambroszczyk",
      "Qiyin Wu",
      "Yifan Chang",
      "Brian Roark",
      "Chaitra Hegde",
      "Tianli Ding",
      "Angelos Filos",
      "Zhongru Wu",
      "Andr\u00e9 Susano Pinto",
      "Shuang Liu",
      "Saarthak Khanna",
      "Aditya Pandey",
      "Siobhan Mcloughlin",
      "Qiujia Li",
      "Sam Haves",
      "Allan Zhou",
      "Elena Buchatskaya",
      "Isabel Leal",
      "Peter de Boursac",
      "Nami Akazawa",
      "Nina Anderson",
      "Terry Chen",
      "Krishna Somandepalli",
      "Chen Liang",
      "Sheela Goenka",
      "Stephanie Winkler",
      "Alexander Grushetsky",
      "Yifan Ding",
      "Jamie Smith",
      "Fan Ye",
      "Jordi Pont-Tuset",
      "Eric Li",
      "Ruichao Li",
      "Tomer Golany",
      "Dawid Wegner",
      "Tao Jiang",
      "Omer Barak",
      "Yuan Shangguan",
      "Eszter V\u00e9rtes",
      "Renee Wong",
      "J\u00f6rg Bornschein",
      "Alex Tudor",
      "Michele Bevilacqua",
      "Tom Schaul",
      "Ankit Singh Rawat",
      "Yang Zhao",
      "Kyriakos Axiotis",
      "Lei Meng",
      "Cory McLean",
      "Jonathan Lai",
      "Jennifer Beattie",
      "Nate Kushman",
      "Yaxin Liu",
      "Blair Kutzman",
      "Fiona Lang",
      "Jingchen Ye",
      "Praneeth Netrapalli",
      "Pushkar Mishra",
      "Myriam Khan",
      "Megha Goel",
      "Rob Willoughby",
      "David Tian",
      "Honglei Zhuang",
      "JD Chen",
      "Zak Tsai",
      "Tasos Kementsietsidis",
      "Arjun Khare",
      "James Keeling",
      "Keyang Xu",
      "Nathan Waters",
      "Florent Altch\u00e9",
      "Ashok Popat",
      "Bhavishya Mittal",
      "David Saxton",
      "Dalia El Badawy",
      "Michael Mathieu",
      "Zheng Zheng",
      "Hao Zhou",
      "Nishant Ranka",
      "Richard Shin",
      "Qingnan Duan",
      "Tim Salimans",
      "Ioana Mihailescu",
      "Uri Shaham",
      "Ming-Wei Chang",
      "Yannis Assael",
      "Nishanth Dikkala",
      "Martin Izzard",
      "Vincent Cohen-Addad",
      "Cat Graves",
      "Vlad Feinberg",
      "Grace Chung",
      "DJ Strouse",
      "Danny Karmon",
      "Sahand Sharifzadeh",
      "Zoe Ashwood",
      "Khiem Pham",
      "Jon Blanton",
      "Alex Vasiloff",
      "Jarred Barber",
      "Mark Geller",
      "Aurick Zhou",
      "Fedir Zubach",
      "Tzu-Kuo Huang",
      "Lei Zhang",
      "Himanshu Gupta",
      "Matt Young",
      "Julia Proskurnia",
      "Ronny Votel",
      "Valentin Gabeur",
      "Gabriel Barcik",
      "Aditya Tripathi",
      "Hongkun Yu",
      "Geng Yan",
      "Beer Changpinyo",
      "Filip Paveti\u0107",
      "Amy Coyle",
      "Yasuhisa Fujii",
      "Jorge Gonzalez Mendez",
      "Tianhao Zhou",
      "Harish Rajamani",
      "Blake Hechtman",
      "Eddie Cao",
      "Da-Cheng Juan",
      "Yi-Xuan Tan",
      "Valentin Dalibard",
      "Yilun Du",
      "Natalie Clay",
      "Kaisheng Yao",
      "Wenhao Jia",
      "Dimple Vijaykumar",
      "Yuxiang Zhou",
      "Xinyi Bai",
      "Wei-Chih Hung",
      "Steven Pecht",
      "Georgi Todorov",
      "Nikhil Khadke",
      "Pramod Gupta",
      "Preethi Lahoti",
      "Arnaud Autef",
      "Karthik Duddu",
      "James Lee-Thorp",
      "Alexander Bykovsky",
      "Tautvydas Misiunas",
      "Sebastian Flennerhag",
      "Santhosh Thangaraj",
      "Jed McGiffin",
      "Zack Nado",
      "Markus Kunesch",
      "Andreas Noever",
      "Amir Hertz",
      "Marco Liang",
      "Victor Stone",
      "Evan Palmer",
      "Samira Daruki",
      "Arijit Pramanik",
      "Siim P\u00f5der",
      "Austin Kyker",
      "Mina Khan",
      "Evgeny Sluzhaev",
      "Marvin Ritter",
      "Avraham Ruderman",
      "Wenlei Zhou",
      "Chirag Nagpal",
      "Kiran Vodrahalli",
      "George Necula",
      "Paul Barham",
      "Ellie Pavlick",
      "Jay Hartford",
      "Izhak Shafran",
      "Long Zhao",
      "Maciej Miku\u0142a",
      "Tom Eccles",
      "Hidetoshi Shimokawa",
      "Kanav Garg",
      "Luke Vilnis",
      "Hanwen Chen",
      "Ilia Shumailov",
      "Kuang-Huei Lee",
      "Abdelrahman Abdelhamed",
      "Meiyan Xie",
      "Vered Cohen",
      "Ester Hlavnova",
      "Dan Malkin",
      "Chawin Sitawarin",
      "James Lottes",
      "Pauline Coquinot",
      "Tianli Yu",
      "Sandeep Kumar",
      "Jingwei Zhang",
      "Aroma Mahendru",
      "Zafarali Ahmed",
      "James Martens",
      "Tao Chen",
      "Aviel Boag",
      "Daiyi Peng",
      "Coline Devin",
      "Arseniy Klimovskiy",
      "Mary Phuong",
      "Danny Vainstein",
      "Jin Xie",
      "Bhuvana Ramabhadran",
      "Nathan Howard",
      "Xinxin Yu",
      "Gitartha Goswami",
      "Jingyu Cui",
      "Sam Shleifer",
      "Mario Pinto",
      "Chih-Kuan Yeh",
      "Ming-Hsuan Yang",
      "Sara Javanmardi",
      "Dan Ethier",
      "Chace Lee",
      "Jordi Orbay",
      "Suyog Kotecha",
      "Carla Bromberg",
      "Pete Shaw",
      "James Thornton",
      "Adi Gerzi Rosenthal",
      "Shane Gu",
      "Matt Thomas",
      "Ian Gemp",
      "Aditya Ayyar",
      "Asahi Ushio",
      "Aarush Selvan",
      "Joel Wee",
      "Chenxi Liu",
      "Maryam Majzoubi",
      "Weiren Yu",
      "Jake Abernethy",
      "Tyler Liechty",
      "Renke Pan",
      "Hoang Nguyen",
      "Qiong",
      "Hu",
      "Sarah Perrin",
      "Abhinav Arora",
      "Emily Pitler",
      "Weiyi Wang",
      "Kaushik Shivakumar",
      "Flavien Prost",
      "Ben Limonchik",
      "Jing Wang",
      "Yi Gao",
      "Timothee Cour",
      "Shyamal Buch",
      "Huan Gui",
      "Maria Ivanova",
      "Philipp Neubeck",
      "Kelvin Chan",
      "Lucy Kim",
      "Huizhong Chen",
      "Naman Goyal",
      "Da-Woon Chung",
      "Lu Liu",
      "Yao Su",
      "Anastasia Petrushkina",
      "Jiajun Shen",
      "Armand Joulin",
      "Yuanzhong Xu",
      "Stein Xudong Lin",
      "Yana Kulizhskaya",
      "Ciprian Chelba",
      "Shobha Vasudevan",
      "Eli Collins",
      "Vasilisa Bashlovkina",
      "Tony Lu",
      "Doug Fritz",
      "Jongbin Park",
      "Yanqi Zhou",
      "Chen Su",
      "Richard Tanburn",
      "Mikhail Sushkov",
      "Mitchelle Rasquinha",
      "Jinning Li",
      "Jennifer Prendki",
      "Yiming Li",
      "Pallavi LV",
      "Shriya Sharma",
      "Hen Fitoussi",
      "Hui Huang",
      "Andrew Dai",
      "Phuong Dao",
      "Mike Burrows",
      "Henry Prior",
      "Danfeng Qin",
      "Golan Pundak",
      "Lars Lowe Sjoesund",
      "Art Khurshudov",
      "Zhenkai Zhu",
      "Albert Webson",
      "Elizabeth Kemp",
      "Tat Tan",
      "Saurabh Agrawal",
      "Susie Sargsyan",
      "Liqun Cheng",
      "Jim Stephan",
      "Tom Kwiatkowski",
      "David Reid",
      "Arunkumar Byravan",
      "Assaf Hurwitz Michaely",
      "Nicolas Heess",
      "Luowei Zhou",
      "Sonam Goenka",
      "Viral Carpenter",
      "Anselm Levskaya",
      "Bo Wang",
      "Reed Roberts",
      "R\u00e9mi Leblond",
      "Sharat Chikkerur",
      "Stav Ginzburg",
      "Max Chang",
      "Robert Riachi",
      "Chuqiao",
      "Xu",
      "Zal\u00e1n Borsos",
      "Michael Pliskin",
      "Julia Pawar",
      "Morgane Lustman",
      "Hannah Kirkwood",
      "Ankit Anand",
      "Aditi Chaudhary",
      "Norbert Kalb",
      "Kieran Milan",
      "Sean Augenstein",
      "Anna Goldie",
      "Laurel Prince",
      "Karthik Raman",
      "Yanhua Sun",
      "Vivian Xia",
      "Aaron Cohen",
      "Zhouyuan Huo",
      "Josh Camp",
      "Seher Ellis",
      "Lukas Zilka",
      "David Vilar Torres",
      "Lisa Patel",
      "Sho Arora",
      "Betty Chan",
      "Jonas Adler",
      "Kareem Ayoub",
      "Jacky Liang",
      "Fayaz Jamil",
      "Jiepu Jiang",
      "Simon Baumgartner",
      "Haitian Sun",
      "Yael Karov",
      "Yaroslav Akulov",
      "Hui Zheng",
      "Irene Cai",
      "Claudio Fantacci",
      "James Rubin",
      "Alex Rav Acha",
      "Mengchao Wang",
      "Nina D'Souza",
      "Rohit Sathyanarayana",
      "Shengyang Dai",
      "Simon Rowe",
      "Andrey Simanovsky",
      "Omer Goldman",
      "Yuheng Kuang",
      "Xiaoyue Pan",
      "Andrew Rosenberg",
      "Tania Rojas-Esponda",
      "Praneet Dutta",
      "Amy Zeng",
      "Irina Jurenka",
      "Greg Farquhar",
      "Yamini Bansal",
      "Shariq Iqbal",
      "Becca Roelofs",
      "Ga-Young Joung",
      "Parker Beak",
      "Changwan Ryu",
      "Ryan Poplin",
      "Yan Wu",
      "Jean-Baptiste Alayrac",
      "Senaka Buthpitiya",
      "Olaf Ronneberger",
      "Caleb Habtegebriel",
      "Wei Li",
      "Paul Cavallaro",
      "Aurora Wei",
      "Guy Bensky",
      "Timo Denk",
      "Harish Ganapathy",
      "Jeff Stanway",
      "Pratik Joshi",
      "Francesco Bertolini",
      "Jessica Lo",
      "Olivia Ma",
      "Zachary Charles",
      "Geta Sampemane",
      "Himanshu Sahni",
      "Xu Chen",
      "Harry Askham",
      "David Gaddy",
      "Peter Young",
      "Jiewen Tan",
      "Matan Eyal",
      "Arthur Bra\u017einskas",
      "Li Zhong",
      "Zhichun Wu",
      "Mark Epstein",
      "Kai Bailey",
      "Andrew Hard",
      "Kamyu Lee",
      "Sasha Goldshtein",
      "Alex Ruiz",
      "Mohammed Badawi",
      "Matthias Lochbrunner",
      "JK Kearns",
      "Ashley Brown",
      "Fabio Pardo",
      "Theophane Weber",
      "Haichuan Yang",
      "Pan-Pan Jiang",
      "Berkin Akin",
      "Zhao Fu",
      "Marcus Wainwright",
      "Chi Zou",
      "Meenu Gaba",
      "Pierre-Antoine Manzagol",
      "Wendy Kan",
      "Yang Song",
      "Karina Zainullina",
      "Rui Lin",
      "Jeongwoo Ko",
      "Salil Deshmukh",
      "Apoorv Jindal",
      "James Svensson",
      "Divya Tyam",
      "Heri Zhao",
      "Christine Kaeser-Chen",
      "Scott Baird",
      "Pooya Moradi",
      "Jamie Hall",
      "Qiuchen Guo",
      "Vincent Tsang",
      "Bowen Liang",
      "Fernando Pereira",
      "Suhas Ganesh",
      "Ivan Korotkov",
      "Jakub Adamek",
      "Sridhar Thiagarajan",
      "Vinh Tran",
      "Charles Chen",
      "Chris Tar",
      "Sanil Jain",
      "Ishita Dasgupta",
      "Taylan Bilal",
      "David Reitter",
      "Kai Zhao",
      "Giulia Vezzani",
      "Yasmin Gehman",
      "Pulkit Mehta",
      "Lauren Beltrone",
      "Xerxes Dotiwalla",
      "Sergio Guadarrama",
      "Zaheer Abbas",
      "Stefani Karp",
      "Petko Georgiev",
      "Chun-Sung Ferng",
      "Marc Brockschmidt",
      "Liqian Peng",
      "Christoph Hirnschall",
      "Vikas Verma",
      "Yingying Bi",
      "Ying Xiao",
      "Avigail Dabush",
      "Kelvin Xu",
      "Phil Wallis",
      "Randall Parker",
      "Qifei Wang",
      "Yang Xu",
      "Ilkin Safarli",
      "Dinesh Tewari",
      "Yin Zhang",
      "Seungyeon Kim",
      "Andrea Gesmundo",
      "Mackenzie Thomas",
      "Sergey Levi",
      "Ahmed Chowdhury",
      "Kanishka Rao",
      "Peter Garst",
      "Sam Conway-Rahman",
      "Helen Ran",
      "Kay McKinney",
      "Zhisheng Xiao",
      "Wenhao Yu",
      "Rohan Agrawal",
      "Axel Stjerngren",
      "Catalin Ionescu",
      "Jingjing Chen",
      "Vivek Sharma",
      "Justin Chiu",
      "Fei Liu",
      "Ken Franko",
      "Clayton Sanford",
      "Xingyu Cai",
      "Paul Michel",
      "Sanjay Ganapathy",
      "Jane Labanowski",
      "Zachary Garrett",
      "Ben Vargas",
      "Sean Sun",
      "Bryan Gale",
      "Thomas Buschmann",
      "Guillaume Desjardins",
      "Nimesh Ghelani",
      "Palak Jain",
      "Mudit Verma",
      "Chulayuth Asawaroengchai",
      "Julian Eisenschlos",
      "Jitendra Harlalka",
      "Hideto Kazawa",
      "Don Metzler",
      "Joshua Howland",
      "Ying Jian",
      "Jake Ades",
      "Viral Shah",
      "Tynan Gangwani",
      "Seungji Lee",
      "Roman Ring",
      "Steven M. Hernandez",
      "Dean Reich",
      "Amer Sinha",
      "Ashutosh Sathe",
      "Joe Kovac",
      "Ashleah Gill",
      "Ajay Kannan",
      "Andrea D'olimpio",
      "Martin Sevenich",
      "Jay Whang",
      "Been Kim",
      "Khe Chai Sim",
      "Jilin Chen",
      "Jiageng Zhang",
      "Shuba Lall",
      "Yossi Matias",
      "Bill Jia",
      "Abe Friesen",
      "Sara Nasso",
      "Ashish Thapliyal",
      "Bryan Perozzi",
      "Ting Yu",
      "Anna Shekhawat",
      "Safeen Huda",
      "Peter Grabowski",
      "Eric Wang",
      "Ashwin Sreevatsa",
      "Hilal Dib",
      "Mehadi Hassen",
      "Parker Schuh",
      "Vedrana Milutinovic",
      "Chris Welty",
      "Michael Quinn",
      "Ali Shah",
      "Bangju Wang",
      "Gabe Barth-Maron",
      "Justin Frye",
      "Natalie Axelsson",
      "Tao Zhu",
      "Yukun Ma",
      "Irene Giannoumis",
      "Hanie Sedghi",
      "Chang Ye",
      "Yi Luan",
      "Kevin Aydin",
      "Bilva Chandra",
      "Vivek Sampathkumar",
      "Ronny Huang",
      "Victor Lavrenko",
      "Ahmed Eleryan",
      "Zhi Hong",
      "Steven Hansen",
      "Sara Mc Carthy",
      "Bidisha Samanta",
      "Domagoj \u0106evid",
      "Xin Wang",
      "Fangtao Li",
      "Michael Voznesensky",
      "Matt Hoffman",
      "Andreas Terzis",
      "Vikash Sehwag",
      "Gil Fidel",
      "Luheng He",
      "Mu Cai",
      "Yanzhang He",
      "Alex Feng",
      "Martin Nikoltchev",
      "Samrat Phatale",
      "Jason Chase",
      "Rory Lawton",
      "Ming Zhang",
      "Tom Ouyang",
      "Manuel Tragut",
      "Mehdi Hafezi Manshadi",
      "Arjun Narayanan",
      "Jiaming Shen",
      "Xu Gao",
      "Tolga Bolukbasi",
      "Nick Roy",
      "Xin Li",
      "Daniel Golovin",
      "Liviu Panait",
      "Zhen Qin",
      "Guangxing Han",
      "Thomas Anthony",
      "Sneha Kudugunta",
      "Viorica Patraucean",
      "Aniket Ray",
      "Xinyun Chen",
      "Xiaochen Yang",
      "Tanuj Bhatia",
      "Pranav Talluri",
      "Alex Morris",
      "Andrija Ra\u017enatovi\u0107",
      "Bethanie Brownfield",
      "James An",
      "Sheng Peng",
      "Patrick Kane",
      "Ce Zheng",
      "Nico Duduta",
      "Joshua Kessinger",
      "James Noraky",
      "Siqi Liu",
      "Keran Rong",
      "Petar Veli\u010dkovi\u0107",
      "Keith Rush",
      "Alex Goldin",
      "Fanny Wei",
      "Shiva Mohan Reddy Garlapati",
      "Caroline Pantofaru",
      "Okwan Kwon",
      "Jianmo Ni",
      "Eric Noland",
      "Julia Di Trapani",
      "Fran\u00e7oise Beaufays",
      "Abhijit Guha Roy",
      "Yinlam Chow",
      "Aybuke Turker",
      "Geoffrey Cideron",
      "Lantao Mei",
      "Jon Clark",
      "Qingyun Dou",
      "Matko Bo\u0161njak",
      "Ralph Leith",
      "Yuqing Du",
      "Amir Yazdanbakhsh",
      "Milad Nasr",
      "Chester Kwak",
      "Suraj Satishkumar Sheth",
      "Alex Kaskasoli",
      "Ankesh Anand",
      "Balaji Lakshminarayanan",
      "Sammy Jerome",
      "David Bieber",
      "Chun-Te Chu",
      "Alexandre Senges",
      "Tianxiao Shen",
      "Mukund Sridhar",
      "Ndaba Ndebele",
      "Benjamin Beyret",
      "Shakir Mohamed",
      "Mia Chen",
      "Markus Freitag",
      "Jiaxian Guo",
      "Luyang Liu",
      "Paul Roit",
      "Heng Chen",
      "Shen Yan",
      "Tom Stone",
      "JD Co-Reyes",
      "Jeremy Cole",
      "Salvatore Scellato",
      "Shekoofeh Azizi",
      "Hadi Hashemi",
      "Alicia Jin",
      "Anand Iyer",
      "Marcella Valentine",
      "Andr\u00e1s Gy\u00f6rgy",
      "Arun Ahuja",
      "Daniel Hernandez Diaz",
      "Chen-Yu Lee",
      "Nathan Clement",
      "Weize Kong",
      "Drew Garmon",
      "Ishaan Watts",
      "Kush Bhatia",
      "Khyatti Gupta",
      "Matt Miecnikowski",
      "Hugo Vallet",
      "Ankur Taly",
      "Edward Loper",
      "Saket Joshi",
      "James Atwood",
      "Jo Chick",
      "Mark Collier",
      "Fotis Iliopoulos",
      "Ryan Trostle",
      "Beliz Gunel",
      "Ramiro Leal-Cavazos",
      "Arnar Mar Hrafnkelsson",
      "Michael Guzman",
      "Xiaoen Ju",
      "Andy Forbes",
      "Jesse Emond",
      "Kushal Chauhan",
      "Ben Caine",
      "Li Xiao",
      "Wenjun Zeng",
      "Alexandre Moufarek",
      "Daniel Murphy",
      "Maya Meng",
      "Nitish Gupta",
      "Felix Riedel",
      "Anil Das",
      "Elijah Lawal",
      "Shashi Narayan",
      "Tiberiu Sosea",
      "James Swirhun",
      "Linda Friso",
      "Behnam Neyshabur",
      "Jing Lu",
      "Sertan Girgin",
      "Michael Wunder",
      "Edouard Yvinec",
      "Aroonalok Pyne",
      "Victor Carbune",
      "Shruti Rijhwani",
      "Yang Guo",
      "Tulsee Doshi",
      "Anton Briukhov",
      "Max Bain",
      "Ayal Hitron",
      "Xuanhui Wang",
      "Ashish Gupta",
      "Ke Chen",
      "Cosmo Du",
      "Weiyang Zhang",
      "Dhruv Shah",
      "Arjun Akula",
      "Max Dylla",
      "Ashyana Kachra",
      "Weicheng Kuo",
      "Tingting Zou",
      "Lily Wang",
      "Luyao Xu",
      "Jifan Zhu",
      "Justin Snyder",
      "Sachit Menon",
      "Orhan Firat",
      "Igor Mordatch",
      "Yuan Yuan",
      "Natalia Ponomareva",
      "Rory Blevins",
      "Lawrence Moore",
      "Weijun Wang",
      "Phil Chen",
      "Martin Scholz",
      "Artur Dwornik",
      "Jason Lin",
      "Sicheng Li",
      "Diego Antognini",
      "Te I",
      "Xiaodan Song",
      "Matt Miller",
      "Uday Kalra",
      "Adam Raveret",
      "Oscar Akerlund",
      "Felix Wu",
      "Andrew Nystrom",
      "Namrata Godbole",
      "Tianqi Liu",
      "Hannah DeBalsi",
      "Jewel Zhao",
      "Buhuang Liu",
      "Avi Caciularu",
      "Lauren Lax",
      "Urvashi Khandelwal",
      "Victoria Langston",
      "Eric Bailey",
      "Silvio Lattanzi",
      "Yufei Wang",
      "Neel Kovelamudi",
      "Sneha Mondal",
      "Guru Guruganesh",
      "Nan Hua",
      "Ofir Roval",
      "Pawe\u0142 Weso\u0142owski",
      "Rishikesh Ingale",
      "Jonathan Halcrow",
      "Tim Sohn",
      "Christof Angermueller",
      "Bahram Raad",
      "Eli Stickgold",
      "Eva Lu",
      "Alec Kosik",
      "Jing Xie",
      "Timothy Lillicrap",
      "Austin Huang",
      "Lydia Lihui Zhang",
      "Dominik Paulus",
      "Clement Farabet",
      "Alex Wertheim",
      "Bing Wang",
      "Rishabh Joshi",
      "Chu-ling Ko",
      "Yonghui Wu",
      "Shubham Agrawal",
      "Lily Lin",
      "XiangHai Sheng",
      "Peter Sung",
      "Tyler Breland-King",
      "Christina Butterfield",
      "Swapnil Gawde",
      "Sumeet Singh",
      "Qiao Zhang",
      "Raj Apte",
      "Shilpa Shetty",
      "Adrian Hutter",
      "Tao Li",
      "Elizabeth Salesky",
      "Federico Lebron",
      "Jonni Kanerva",
      "Michela Paganini",
      "Arthur Nguyen",
      "Rohith Vallu",
      "Jan-Thorsten Peter",
      "Sarmishta Velury",
      "David Kao",
      "Jay Hoover",
      "Anna Bortsova",
      "Colton Bishop",
      "Shoshana Jakobovits",
      "Alessandro Agostini",
      "Alekh Agarwal",
      "Chang Liu",
      "Charles Kwong",
      "Sasan Tavakkol",
      "Ioana Bica",
      "Alex Greve",
      "Anirudh GP",
      "Jake Marcus",
      "Le Hou",
      "Tom Duerig",
      "Rivka Moroshko",
      "Dave Lacey",
      "Andy Davis",
      "Julien Amelot",
      "Guohui Wang",
      "Frank Kim",
      "Theofilos Strinopoulos",
      "Hui Wan",
      "Charline Le Lan",
      "Shankar Krishnan",
      "Haotian Tang",
      "Peter Humphreys",
      "Junwen Bai",
      "Idan Heimlich Shtacher",
      "Diego Machado",
      "Chenxi Pang",
      "Ken Burke",
      "Dangyi Liu",
      "Renga Aravamudhan",
      "Yue Song",
      "Ed Hirst",
      "Abhimanyu Singh",
      "Brendan Jou",
      "Liang Bai",
      "Francesco Piccinno",
      "Chuyuan Kelly Fu",
      "Robin Alazard",
      "Barak Meiri",
      "Daniel Winter",
      "Charlie Chen",
      "Mingda Zhang",
      "Jens Heitkaemper",
      "John Lambert",
      "Jinhyuk Lee",
      "Alexander Fr\u00f6mmgen",
      "Sergey Rogulenko",
      "Pranav Nair",
      "Paul Niemczyk",
      "Anton Bulyenov",
      "Bibo Xu",
      "Hadar Shemtov",
      "Morteza Zadimoghaddam",
      "Serge Toropov",
      "Mateo Wirth",
      "Hanjun Dai",
      "Sreenivas Gollapudi",
      "Daniel Zheng",
      "Alex Kurakin",
      "Chansoo Lee",
      "Kalesha Bullard",
      "Nicolas Serrano",
      "Ivana Balazevic",
      "Yang Li",
      "Johan Schalkwyk",
      "Mark Murphy",
      "Mingyang Zhang",
      "Kevin Sequeira",
      "Romina Datta",
      "Nishant Agrawal",
      "Charles Sutton",
      "Nithya Attaluri",
      "Mencher Chiang",
      "Wael Farhan",
      "Gregory Thornton",
      "Kate Lin",
      "Travis Choma",
      "Hung Nguyen",
      "Kingshuk Dasgupta",
      "Dirk Robinson",
      "Iulia Com\u015fa",
      "Michael Riley",
      "Arjun Pillai",
      "Basil Mustafa",
      "Ben Golan",
      "Amir Zandieh",
      "Jean-Baptiste Lespiau",
      "Billy Porter",
      "David Ross",
      "Sujeevan Rajayogam",
      "Mohit Agarwal",
      "Subhashini Venugopalan",
      "Bobak Shahriari",
      "Qiqi Yan",
      "Hao Xu",
      "Taylor Tobin",
      "Pavel Dubov",
      "Hongzhi Shi",
      "Adri\u00e0 Recasens",
      "Anton Kovsharov",
      "Sebastian Borgeaud",
      "Lucio Dery",
      "Shanthal Vasanth",
      "Elena Gribovskaya",
      "Linhai Qiu",
      "Mahdis Mahdieh",
      "Wojtek Skut",
      "Elizabeth Nielsen",
      "CJ Zheng",
      "Adams Yu",
      "Carrie Grimes Bostock",
      "Shaleen Gupta",
      "Aaron Archer",
      "Chris Rawles",
      "Elinor Davies",
      "Alexey Svyatkovskiy",
      "Tomy Tsai",
      "Yoni Halpern",
      "Christian Reisswig",
      "Bartek Wydrowski",
      "Bo Chang",
      "Joan Puigcerver",
      "Mor Hazan Taege",
      "Jian Li",
      "Eva Schnider",
      "Xinjian Li",
      "Dragos Dena",
      "Yunhan Xu",
      "Umesh Telang",
      "Tianze Shi",
      "Heiga Zen",
      "Kyle Kastner",
      "Yeongil Ko",
      "Neesha Subramaniam",
      "Aviral Kumar",
      "Pete Blois",
      "Zhuyun Dai",
      "John Wieting",
      "Yifeng Lu",
      "Yoel Zeldes",
      "Tian Xie",
      "Anja Hauth",
      "Alexandru \u0162ifrea",
      "Yuqi Li",
      "Sam El-Husseini",
      "Dan Abolafia",
      "Howard Zhou",
      "Wen Ding",
      "Sahra Ghalebikesabi",
      "Carlos Gu\u00eda",
      "Andrii Maksai",
      "\u00c1goston Weisz",
      "Sercan Arik",
      "Nick Sukhanov",
      "Aga \u015awietlik",
      "Xuhui Jia",
      "Luo Yu",
      "Weiyue Wang",
      "Mark Brand",
      "Dawn Bloxwich",
      "Sean Kirmani",
      "Zhe Chen",
      "Alec Go",
      "Pablo Sprechmann",
      "Nithish Kannen",
      "Alen Carin",
      "Paramjit Sandhu",
      "Isabel Edkins",
      "Leslie Nooteboom",
      "Jai Gupta",
      "Loren Maggiore",
      "Javad Azizi",
      "Yael Pritch",
      "Pengcheng Yin",
      "Mansi Gupta",
      "Danny Tarlow",
      "Duncan Smith",
      "Desi Ivanov",
      "Mohammad Babaeizadeh",
      "Ankita Goel",
      "Satish Kambala",
      "Grace Chu",
      "Matej Kastelic",
      "Michelle Liu",
      "Hagen Soltau",
      "Austin Stone",
      "Shivani Agrawal",
      "Min Kim",
      "Kedar Soparkar",
      "Srinivas Tadepalli",
      "Oskar Bunyan",
      "Rachel Soh",
      "Arvind Kannan",
      "DY Kim",
      "Blake JianHang Chen",
      "Afief Halumi",
      "Sudeshna Roy",
      "Yulong Wang",
      "Olcan Sercinoglu",
      "Gena Gibson",
      "Sijal Bhatnagar",
      "Motoki Sano",
      "Daniel von Dincklage",
      "Qingchun Ren",
      "Blagoj Mitrevski",
      "Mirek Ol\u0161\u00e1k",
      "Jennifer She",
      "Carl Doersch",
      "Jilei",
      "Wang",
      "Bingyuan Liu",
      "Qijun Tan",
      "Tamar Yakar",
      "Tris Warkentin",
      "Alex Ramirez",
      "Carl Lebsack",
      "Josh Dillon",
      "Rajiv Mathews",
      "Tom Cobley",
      "Zelin Wu",
      "Zhuoyuan Chen",
      "Jon Simon",
      "Swaroop Nath",
      "Tara Sainath",
      "Alexei Bendebury",
      "Ryan Julian",
      "Bharath Mankalale",
      "Daria \u0106urko",
      "Paulo Zacchello",
      "Adam R. Brown",
      "Kiranbir Sodhia",
      "Heidi Howard",
      "Sergi Caelles",
      "Abhinav Gupta",
      "Gareth Evans",
      "Anna Bulanova",
      "Lesley Katzen",
      "Roman Goldenberg",
      "Anton Tsitsulin",
      "Joe Stanton",
      "Benoit Schillings",
      "Vitaly Kovalev",
      "Corey Fry",
      "Rushin Shah",
      "Kuo Lin",
      "Shyam Upadhyay",
      "Cheng Li",
      "Soroush Radpour",
      "Marcello Maggioni",
      "Jing Xiong",
      "Lukas Haas",
      "Jenny Brennan",
      "Aishwarya Kamath",
      "Nikolay Savinov",
      "Arsha Nagrani",
      "Trevor Yacovone",
      "Ryan Kappedal",
      "Kostas Andriopoulos",
      "Li Lao",
      "YaGuang Li",
      "Grigory Rozhdestvenskiy",
      "Kazuma Hashimoto",
      "Andrew Audibert",
      "Sophia Austin",
      "Daniel Rodriguez",
      "Anian Ruoss",
      "Garrett Honke",
      "Deep Karkhanis",
      "Xi Xiong",
      "Qing Wei",
      "James Huang",
      "Zhaoqi Leng",
      "Vittal Premachandran",
      "Stan Bileschi",
      "Georgios Evangelopoulos",
      "Thomas Mensink",
      "Jay Pavagadhi",
      "Denis Teplyashin",
      "Paul Chang",
      "Linting Xue",
      "Garrett Tanzer",
      "Sally Goldman",
      "Kaushal Patel",
      "Shixin Li",
      "Jeremy Wiesner",
      "Ivy Zheng",
      "Ian Stewart-Binks",
      "Jie Han",
      "Zhi Li",
      "Liangchen Luo",
      "Karel Lenc",
      "Mario Lu\u010di\u0107",
      "Fuzhao Xue",
      "Ryan Mullins",
      "Alexey Guseynov",
      "Chung-Ching Chang",
      "Isaac Galatzer-Levy",
      "Adam Zhang",
      "Garrett Bingham",
      "Grace Hu",
      "Ale Hartman",
      "Yue Ma",
      "Jordan Griffith",
      "Alex Irpan",
      "Carey Radebaugh",
      "Summer Yue",
      "Lijie Fan",
      "Victor Ungureanu",
      "Christina Sorokin",
      "Hannah Teufel",
      "Peiran Li",
      "Rohan Anil",
      "Dimitris Paparas",
      "Todd Wang",
      "Chu-Cheng Lin",
      "Hui Peng",
      "Megan Shum",
      "Goran Petrovic",
      "Demetra Brady",
      "Richard Nguyen",
      "Klaus Macherey",
      "Zhihao Li",
      "Harman Singh",
      "Madhavi Yenugula",
      "Mariko Iinuma",
      "Xinyi Chen",
      "Kavya Kopparapu",
      "Alexey Stern",
      "Shachi Dave",
      "Chandu Thekkath",
      "Florence Perot",
      "Anurag Kumar",
      "Fangda Li",
      "Yang Xiao",
      "Matthew Bilotti",
      "Mohammad Hossein Bateni",
      "Isaac Noble",
      "Lisa Lee",
      "Amelio V\u00e1zquez-Reina",
      "Julian Salazar",
      "Xiaomeng Yang",
      "Boyu Wang",
      "Ela Gruzewska",
      "Anand Rao",
      "Sindhu Raghuram",
      "Zheng Xu",
      "Eyal Ben-David",
      "Jieru Mei",
      "Sid Dalmia",
      "Zhaoyi Zhang",
      "Yuchen Liu",
      "Gagan Bansal",
      "Helena Pankov",
      "Steven Schwarcz",
      "Andrea Burns",
      "Christine Chan",
      "Sumit Sanghai",
      "Ricky Liang",
      "Ethan Liang",
      "Antoine He",
      "Amy Stuart",
      "Arun Narayanan",
      "Yukun Zhu",
      "Christian Frank",
      "Bahar Fatemi",
      "Amit Sabne",
      "Oran Lang",
      "Indro Bhattacharya",
      "Shane Settle",
      "Maria Wang",
      "Brendan McMahan",
      "Andrea Tacchetti",
      "Livio Baldini Soares",
      "Majid Hadian",
      "Serkan Cabi",
      "Timothy Chung",
      "Nikita Putikhin",
      "Gang Li",
      "Jeremy Chen",
      "Austin Tarango",
      "Henryk Michalewski",
      "Mehran Kazemi",
      "Hussain Masoom",
      "Hila Sheftel",
      "Rakesh Shivanna",
      "Archita Vadali",
      "Ramona Comanescu",
      "Doug Reid",
      "Joss Moore",
      "Arvind Neelakantan",
      "Micha\u00ebl Sander",
      "Jonathan Herzig",
      "Aviv Rosenberg",
      "Mostafa Dehghani",
      "JD Choi",
      "Michael Fink",
      "Reid Hayes",
      "Eric Ge",
      "Shitao Weng",
      "Chia-Hua Ho",
      "John Karro",
      "Kalpesh Krishna",
      "Lam Nguyen Thiet",
      "Amy Skerry-Ryan",
      "Daniel Eppens",
      "Marco Andreetto",
      "Navin Sarma",
      "Silvano Bonacina",
      "Burcu Karagol Ayan",
      "Megha Nawhal",
      "Zhihao Shan",
      "Mike Dusenberry",
      "Shantanu Thakoor",
      "Sagar Gubbi",
      "Duc Dung Nguyen",
      "Reut Tsarfaty",
      "Samuel Albanie",
      "Jovana Mitrovi\u0107",
      "Meet Gandhi",
      "Bo-Juen Chen",
      "Alessandro Epasto",
      "Georgi Stephanov",
      "Ye Jin",
      "Samuel Gehman",
      "Aida Amini",
      "Jack Weber",
      "Feryal Behbahani",
      "Shawn Xu",
      "Miltos Allamanis",
      "Xi Chen",
      "Myle Ott",
      "Claire Sha",
      "Michal Jastrzebski",
      "Hang Qi",
      "David Greene",
      "Xinyi Wu",
      "Abodunrinwa Toki",
      "Daniel Vlasic",
      "Jane Shapiro",
      "Ragha Kotikalapudi",
      "Zhe Shen",
      "Takaaki Saeki",
      "Sirui Xie",
      "Albin Cassirer",
      "Shikhar Bharadwaj",
      "Tatsuya Kiyono",
      "Srinadh Bhojanapalli",
      "Elan Rosenfeld",
      "Sam Ritter",
      "Jieming Mao",
      "Jo\u00e3o Gabriel Oliveira",
      "Zoltan Egyed",
      "Bernd Bandemer",
      "Emilio Parisotto",
      "Keisuke Kinoshita",
      "Juliette Pluto",
      "Petros Maniatis",
      "Steve Li",
      "Yaohui Guo",
      "Golnaz Ghiasi",
      "Jean Tarbouriech",
      "Srimon Chatterjee",
      "Julie Jin",
      "Katrina",
      "Xu",
      "Jennimaria Palomaki",
      "S\u00e9b Arnold",
      "Madhavi Sewak",
      "Federico Piccinini",
      "Mohit Sharma",
      "Ben Albrecht",
      "Sean Purser-haskell",
      "Ashwin Vaswani",
      "Chongyan Chen",
      "Matheus Wisniewski",
      "Qin Cao",
      "John Aslanides",
      "Nguyet Minh Phu",
      "Maximilian Sieb",
      "Lauren Agubuzu",
      "Anne Zheng",
      "Daniel Sohn",
      "Marco Selvi",
      "Anders Andreassen",
      "Krishan Subudhi",
      "Prem Eruvbetine",
      "Oliver Woodman",
      "Tomas Mery",
      "Sebastian Krause",
      "Xiaoqi Ren",
      "Xiao Ma",
      "Jincheng Luo",
      "Dawn Chen",
      "Wei Fan",
      "Henry Griffiths",
      "Christian Schuler",
      "Alice Li",
      "Shujian Zhang",
      "Jean-Michel Sarr",
      "Shixin Luo",
      "Riccardo Patana",
      "Matthew Watson",
      "Dani Naboulsi",
      "Michael Collins",
      "Sailesh Sidhwani",
      "Emiel Hoogeboom",
      "Sharon Silver",
      "Emily Caveness",
      "Xiaokai Zhao",
      "Mikel Rodriguez",
      "Maxine Deines",
      "Libin Bai",
      "Patrick Griffin",
      "Marco Tagliasacchi",
      "Emily Xue",
      "Spandana Raj Babbula",
      "Bo Pang",
      "Nan Ding",
      "Gloria Shen",
      "Elijah Peake",
      "Remi Crocker",
      "Shubha Srinivas Raghvendra",
      "Danny Swisher",
      "Woohyun Han",
      "Richa Singh",
      "Ling Wu",
      "Vladimir Pchelin",
      "Tsendsuren Munkhdalai",
      "Dana Alon",
      "Geoff Bacon",
      "Efren Robles",
      "Jannis Bulian",
      "Melvin Johnson",
      "George Powell",
      "Felipe Tiengo Ferreira",
      "Yaoyiran Li",
      "Frederik Benzing",
      "Mihajlo Velimirovi\u0107",
      "Hubert Soyer",
      "William Kong",
      "Tony",
      "Nguy\u00ean",
      "Zhen Yang",
      "Jeremiah Liu",
      "Joost van Amersfoort",
      "Daniel Gillick",
      "Baochen Sun",
      "Nathalie Rauschmayr",
      "Katie Zhang",
      "Serena Zhan",
      "Tao Zhou",
      "Alexey Frolov",
      "Chengrun Yang",
      "Denis Vnukov",
      "Louis Rouillard",
      "Hongji Li",
      "Amol Mandhane",
      "Nova Fallen",
      "Rajesh Venkataraman",
      "Clara Huiyi Hu",
      "Jennifer Brennan",
      "Jenny Lee",
      "Jerry Chang",
      "Martin Sundermeyer",
      "Zhufeng Pan",
      "Rosemary Ke",
      "Simon Tong",
      "Alex Fabrikant",
      "William Bono",
      "Jindong Gu",
      "Ryan Foley",
      "Yiran Mao",
      "Manolis Delakis",
      "Dhruva Bhaswar",
      "Roy Frostig",
      "Nick Li",
      "Avital Zipori",
      "Cath Hope",
      "Olga Kozlova",
      "Swaroop Mishra",
      "Josip Djolonga",
      "Craig Schiff",
      "Majd Al Merey",
      "Eleftheria Briakou",
      "Peter Morgan",
      "Andy Wan",
      "Avinatan Hassidim",
      "RJ Skerry-Ryan",
      "Kuntal Sengupta",
      "Mary Jasarevic",
      "Praveen Kallakuri",
      "Paige Kunkle",
      "Hannah Brennan",
      "Tom Lieber",
      "Hassan Mansoor",
      "Julian Walker",
      "Bing Zhang",
      "Annie Xie",
      "Goran \u017du\u017ei\u0107",
      "Adaeze Chukwuka",
      "Alex Druinsky",
      "Donghyun Cho",
      "Rui Yao",
      "Ferjad Naeem",
      "Shiraz Butt",
      "Eunyoung Kim",
      "Zhipeng Jia",
      "Mandy Jordan",
      "Adam Lelkes",
      "Mark Kurzeja",
      "Sophie Wang",
      "James Zhao",
      "Andrew Over",
      "Abhishek Chakladar",
      "Marcel Prasetya",
      "Neha Jha",
      "Sriram Ganapathy",
      "Yale Cong",
      "Prakash Shroff",
      "Carl Saroufim",
      "Sobhan Miryoosefi",
      "Mohamed Hammad",
      "Tajwar Nasir",
      "Weijuan Xi",
      "Yang Gao",
      "Young Maeng",
      "Ben Hora",
      "Chin-Yi Cheng",
      "Parisa Haghani",
      "Yoad Lewenberg",
      "Caden Lu",
      "Martin Matysiak",
      "Naina Raisinghani",
      "Huiyu Wang",
      "Lexi Baugher",
      "Rahul Sukthankar",
      "Minh Giang",
      "John Schultz",
      "Noah Fiedel",
      "Minmin Chen",
      "Cheng-Chun Lee",
      "Tapomay Dey",
      "Hao Zheng",
      "Shachi Paul",
      "Celine Smith",
      "Andy Ly",
      "Yicheng Wang",
      "Rishabh Bansal",
      "Bartek Perz",
      "Susanna Ricco",
      "Stasha Blank",
      "Vaishakh Keshava",
      "Deepak Sharma",
      "Marvin Chow",
      "Kunal Lad",
      "Komal Jalan",
      "Simon Osindero",
      "Craig Swanson",
      "Jacob Scott",
      "Anastasija Ili\u0107",
      "Xiaowei Li",
      "Siddhartha Reddy Jonnalagadda",
      "Afzal Shama Soudagar",
      "Yan Xiong",
      "Bat-Orgil Batsaikhan",
      "Daniel Jarrett",
      "Naveen Kumar",
      "Maulik Shah",
      "Matt Lawlor",
      "Austin Waters",
      "Mark Graham",
      "Rhys May",
      "Sabela Ramos",
      "Sandra Lefdal",
      "Zeynep Cankara",
      "Nacho Cano",
      "Brendan O'Donoghue",
      "Jed Borovik",
      "Frederick Liu",
      "Jordan Grimstad",
      "Mahmoud Alnahlawi",
      "Katerina Tsihlas",
      "Tom Hudson",
      "Nikolai Grigorev",
      "Yiling Jia",
      "Terry Huang",
      "Tobenna Peter Igwe",
      "Sergei Lebedev",
      "Xiaodan Tang",
      "Igor Krivokon",
      "Frankie Garcia",
      "Melissa Tan",
      "Eric Jia",
      "Peter Stys",
      "Shikhar Vashishth",
      "Yu Liang",
      "Balaji Venkatraman",
      "Chenjie Gu",
      "Anastasios Kementsietsidis",
      "Chen Zhu",
      "Junehyuk Jung",
      "Yunfei Bai",
      "Mohammad Javad Hosseini",
      "Faruk Ahmed",
      "Aditya Gupta",
      "Xin Yuan",
      "Shereen Ashraf",
      "Shitij Nigam",
      "Gautam Vasudevan",
      "Pranjal Awasthi",
      "Adi Mayrav Gilady",
      "Zelda Mariet",
      "Ramy Eskander",
      "Haiguang Li",
      "Hexiang Hu",
      "Guillermo Garrido",
      "Philippe Schlattner",
      "George Zhang",
      "Rohun Saxena",
      "Petar Devi\u0107",
      "Kritika Muralidharan",
      "Ashwin Murthy",
      "Yiqian Zhou",
      "Min Choi",
      "Arissa Wongpanich",
      "Zhengdong Wang",
      "Premal Shah",
      "Yuntao Xu",
      "Yiling Huang",
      "Stephen Spencer",
      "Alice Chen",
      "James Cohan",
      "Junjie Wang",
      "Jonathan Tompson",
      "Junru Wu",
      "Ruba Haroun",
      "Haiqiong Li",
      "Blanca Huergo",
      "Fan Yang",
      "Tongxin Yin",
      "James Wendt",
      "Michael Bendersky",
      "Rahma Chaabouni",
      "Javier Snaider",
      "Johan Ferret",
      "Abhishek Jindal",
      "Tara Thompson",
      "Andrew Xue",
      "Will Bishop",
      "Shubham Milind Phal",
      "Archit Sharma",
      "Yunhsuan Sung",
      "Prabakar Radhakrishnan",
      "Mo Shomrat",
      "Reeve Ingle",
      "Roopali Vij",
      "Justin Gilmer",
      "Mihai Dorin Istin",
      "Sam Sobell",
      "Yang Lu",
      "Emily Nottage",
      "Dorsa Sadigh",
      "Jeremiah Willcock",
      "Tingnan Zhang",
      "Steve Xu",
      "Sasha Brown",
      "Katherine Lee",
      "Gary Wang",
      "Yun Zhu",
      "Yi Tay",
      "Cheolmin Kim",
      "Audrey Gutierrez",
      "Abhanshu Sharma",
      "Yongqin Xian",
      "Sungyong Seo",
      "Claire Cui",
      "Elena Pochernina",
      "Cip Baetu",
      "Krzysztof Jastrz\u0119bski",
      "Mimi Ly",
      "Mohamed Elhawaty",
      "Dan Suh",
      "Eren Sezener",
      "Pidong Wang",
      "Nancy Yuen",
      "George Tucker",
      "Jiahao Cai",
      "Zuguang Yang",
      "Cindy Wang",
      "Alex Muzio",
      "Hai Qian",
      "Jae Yoo",
      "Derek Lockhart",
      "Kevin R. McKee",
      "Mandy Guo",
      "Malika Mehrotra",
      "Artur Mendon\u00e7a",
      "Sanket Vaibhav Mehta",
      "Sherry Ben",
      "Chetan Tekur",
      "Jiaqi Mu",
      "Muye Zhu",
      "Victoria Krakovna",
      "Hongrae Lee",
      "AJ Maschinot",
      "S\u00e9bastien Cevey",
      "HyunJeong Choe",
      "Aijun Bai",
      "Hansa Srinivasan",
      "Derek Gasaway",
      "Nick Young",
      "Patrick Siegler",
      "Dan Holtmann-Rice",
      "Vihari Piratla",
      "Kate Baumli",
      "Roey Yogev",
      "Alex Hofer",
      "Hado van Hasselt",
      "Svetlana Grant",
      "Yuri Chervonyi",
      "David Silver",
      "Andrew Hogue",
      "Ayushi Agarwal",
      "Kathie Wang",
      "Preeti Singh",
      "Four Flynn",
      "Josh Lipschultz",
      "Robert David",
      "Lizzetth Bellot",
      "Yao-Yuan Yang",
      "Long Le",
      "Filippo Graziano",
      "Kate Olszewska",
      "Kevin Hui",
      "Akanksha Maurya",
      "Nikos Parotsidis",
      "Weijie Chen",
      "Tayo Oguntebi",
      "Joe Kelley",
      "Anirudh Baddepudi",
      "Johannes Mauerer",
      "Gregory Shaw",
      "Alex Siegman",
      "Lin Yang",
      "Shravya Shetty",
      "Subhrajit Roy",
      "Yunting Song",
      "Wojciech Stokowiec",
      "Ryan Burnell",
      "Omkar Savant",
      "Robert Busa-Fekete",
      "Jin Miao",
      "Samrat Ghosh",
      "Liam MacDermed",
      "Phillip Lippe",
      "Mikhail Dektiarev",
      "Zach Behrman",
      "Fabian Mentzer",
      "Kelvin Nguyen",
      "Meng Wei",
      "Siddharth Verma",
      "Chris Knutsen",
      "Sudeep Dasari",
      "Zhipeng Yan",
      "Petr Mitrichev",
      "Xingyu Wang",
      "Virat Shejwalkar",
      "Jacob Austin",
      "Srinivas Sunkara",
      "Navneet Potti",
      "Yan Virin",
      "Christian Wright",
      "Ga\u00ebl Liu",
      "Oriana Riva",
      "Etienne Pot",
      "Greg Kochanski",
      "Quoc Le",
      "Gargi Balasubramaniam",
      "Arka Dhar",
      "Yuguo Liao",
      "Adam Bloniarz",
      "Divyansh Shukla",
      "Elizabeth Cole",
      "Jong Lee",
      "Sheng Zhang",
      "Sushant Kafle",
      "Siddharth Vashishtha",
      "Parsa Mahmoudieh",
      "Grace Chen",
      "Raphael Hoffmann",
      "Pranesh Srinivasan",
      "Agustin Dal Lago",
      "Yoav Ben Shalom",
      "Zi Wang",
      "Michael Elabd",
      "Anuj Sharma",
      "Junhyuk Oh",
      "Suraj Kothawade",
      "Maigo Le",
      "Marianne Monteiro",
      "Shentao Yang",
      "Kaiz Alarakyia",
      "Robert Geirhos",
      "Diana Mincu",
      "H\u00e5vard Garnes",
      "Hayato Kobayashi",
      "Soroosh Mariooryad",
      "Kacper Krasowiak",
      "Zhixin",
      "Lai",
      "Shibl Mourad",
      "Mingqiu Wang",
      "Fan Bu",
      "Ophir Aharoni",
      "Guanjie Chen",
      "Abhimanyu Goyal",
      "Vadim Zubov",
      "Ankur Bapna",
      "Elahe Dabir",
      "Nisarg Kothari",
      "Kay Lamerigts",
      "Nicola De Cao",
      "Jeremy Shar",
      "Christopher Yew",
      "Nitish Kulkarni",
      "Dre Mahaarachchi",
      "Mandar Joshi",
      "Zhenhai Zhu",
      "Jared Lichtarge",
      "Yichao Zhou",
      "Hannah Muckenhirn",
      "Vittorio Selo",
      "Oriol Vinyals",
      "Peter Chen",
      "Anthony Brohan",
      "Vaibhav Mehta",
      "Sarah Cogan",
      "Ruth Wang",
      "Ty Geri",
      "Wei-Jen Ko",
      "Wei Chen",
      "Fabio Viola",
      "Keshav Shivam",
      "Lisa Wang",
      "Madeleine Clare Elish",
      "Raluca Ada Popa",
      "S\u00e9bastien Pereira",
      "Jianqiao Liu",
      "Raphael Koster",
      "Donnie Kim",
      "Gufeng Zhang",
      "Sayna Ebrahimi",
      "Partha Talukdar",
      "Yanyan Zheng",
      "Petra Poklukar",
      "Ales Mikhalap",
      "Dale Johnson",
      "Anitha Vijayakumar",
      "Mark Omernick",
      "Matt Dibb",
      "Ayush Dubey",
      "Qiong Hu",
      "Apurv Suman",
      "Vaibhav Aggarwal",
      "Ilya Kornakov",
      "Fei Xia",
      "Wing Lowe",
      "Alexey Kolganov",
      "Ted Xiao",
      "Vitaly Nikolaev",
      "Steven Hemingray",
      "Bonnie Li",
      "Joana Iljazi",
      "Miko\u0142aj Rybi\u0144ski",
      "Ballie Sandhu",
      "Peggy Lu",
      "Thang Luong",
      "Rodolphe Jenatton",
      "Vineetha Govindaraj",
      "Hui",
      "Li",
      "Gabriel Dulac-Arnold",
      "Wonpyo Park",
      "Henry Wang",
      "Abhinit Modi",
      "Jean Pouget-Abadie",
      "Kristina Greller",
      "Rahul Gupta",
      "Robert Berry",
      "Prajit Ramachandran",
      "Jinyu Xie",
      "Liam McCafferty",
      "Jianling Wang",
      "Kilol Gupta",
      "Hyeontaek Lim",
      "Bla\u017e Bratani\u010d",
      "Andy Brock",
      "Ilia Akolzin",
      "Jim Sproch",
      "Dan Karliner",
      "Duhyeon Kim",
      "Adrian Goedeckemeyer",
      "Noam Shazeer",
      "Cordelia Schmid",
      "Daniele Calandriello",
      "Parul Bhatia",
      "Krzysztof Choromanski",
      "Ceslee Montgomery",
      "Dheeru Dua",
      "Ana Ramalho",
      "Helen King",
      "Yue Gao",
      "Lynn Nguyen",
      "David Lindner",
      "Divya Pitta",
      "Oleaser Johnson",
      "Khalid Salama",
      "Diego Ardila",
      "Michael Han",
      "Erin Farnese",
      "Seth Odoom",
      "Ziyue Wang",
      "Xiangzhuo Ding",
      "Norman Rink",
      "Ray Smith",
      "Harshal Tushar Lehri",
      "Eden Cohen",
      "Neera Vats",
      "Tong He",
      "Parthasarathy Gopavarapu",
      "Adam Paszke",
      "Miteyan Patel",
      "Wouter Van Gansbeke",
      "Lucia Loher",
      "Luis Castro",
      "Maria Voitovich",
      "Tamara von Glehn",
      "Nelson George",
      "Simon Niklaus",
      "Zach Eaton-Rosen",
      "Nemanja Raki\u0107evi\u0107",
      "Erik Jue",
      "Sagi Perel",
      "Carrie Zhang",
      "Yuval Bahat",
      "Ang\u00e9line Pouget",
      "Zhi Xing",
      "Fantine Huot",
      "Ashish Shenoy",
      "Taylor Bos",
      "Vincent Coriou",
      "Bryan Richter",
      "Natasha Noy",
      "Yaqing Wang",
      "Santiago Ontanon",
      "Siyang Qin",
      "Gleb Makarchuk",
      "Demis Hassabis",
      "Zhuowan Li",
      "Mandar Sharma",
      "Kumaran Venkatesan",
      "Iurii Kemaev",
      "Roxanne Daniel",
      "Shiyu Huang",
      "Saloni Shah",
      "Octavio Ponce",
      "Warren",
      "Chen",
      "Manaal Faruqui",
      "Jialin Wu",
      "Slavica Anda\u010di\u0107",
      "Szabolcs Payrits",
      "Daniel McDuff",
      "Tom Hume",
      "Yuan Cao",
      "MH Tessler",
      "Qingze Wang",
      "Yinan Wang",
      "Ivor Rendulic",
      "Eirikur Agustsson",
      "Matthew Johnson",
      "Tanya Lando",
      "Andrew Howard",
      "Sri Gayatri Sundara Padmanabhan",
      "Mayank Daswani",
      "Andrea Banino",
      "Michael Kilgore",
      "Jonathan Heek",
      "Ziwei Ji",
      "Alvaro Caceres",
      "Conglong Li",
      "Nora Kassner",
      "Alexey Vlaskin",
      "Zeyu Liu",
      "Alex Grills",
      "Yanhan Hou",
      "Roykrong Sukkerd",
      "Gowoon Cheon",
      "Nishita Shetty",
      "Larisa Markeeva",
      "Piotr Stanczyk",
      "Tejas Iyer",
      "Yuan Gong",
      "Shawn Gao",
      "Keerthana Gopalakrishnan",
      "Tim Blyth",
      "Malcolm Reynolds",
      "Avishkar Bhoopchand",
      "Misha Bilenko",
      "Dero Gharibian",
      "Vicky Zayats",
      "Aleksandra Faust",
      "Abhinav Singh",
      "Min Ma",
      "Hongyang Jiao",
      "Sudheendra Vijayanarasimhan",
      "Lora Aroyo",
      "Vikas Yadav",
      "Sarah Chakera",
      "Ashwin Kakarla",
      "Vilobh Meshram",
      "Karol Gregor",
      "Gabriela Botea",
      "Evan Senter",
      "Dawei Jia",
      "Geza Kovacs",
      "Neha Sharma",
      "Sebastien Baur",
      "Kai Kang",
      "Yifan He",
      "Lin Zhuo",
      "Marija Kostelac",
      "Itay Laish",
      "Songyou Peng",
      "Louis O'Bryan",
      "Daniel Kasenberg",
      "Girish Ramchandra Rao",
      "Edouard Leurent",
      "Biao Zhang",
      "Sage Stevens",
      "Ana Salazar",
      "Ye Zhang",
      "Ivan Lobov",
      "Jake Walker",
      "Allen Porter",
      "Morgan Redshaw",
      "Han Ke",
      "Abhishek Rao",
      "Alex Lee",
      "Hoi Lam",
      "Michael Moffitt",
      "Jaeyoun Kim",
      "Siyuan Qiao",
      "Terry Koo",
      "Robert Dadashi",
      "Xinying Song",
      "Mukund Sundararajan",
      "Peng Xu",
      "Chizu Kawamoto",
      "Yan Zhong",
      "Clara Barbu",
      "Apoorv Reddy",
      "Mauro Verzetti",
      "Leon Li",
      "George Papamakarios",
      "Hanna Klimczak-Pluci\u0144ska",
      "Mary Cassin",
      "Koray Kavukcuoglu",
      "Rigel Swavely",
      "Alain Vaucher",
      "Jeffrey Zhao",
      "Ross Hemsley",
      "Michael Tschannen",
      "Heming Ge",
      "Gaurav Menghani",
      "Yang Yu",
      "Natalie Ha",
      "Wei He",
      "Xiao Wu",
      "Maggie Song",
      "Rachel Sterneck",
      "Stefan Zinke",
      "Dan A. Calian",
      "Annie Marsden",
      "Alejandro Cruzado Ruiz",
      "Matteo Hessel",
      "Almog Gueta",
      "Benjamin Lee",
      "Brian Farris",
      "Manish Gupta",
      "Yunjie Li",
      "Mohammad Saleh",
      "Vedant Misra",
      "Kefan Xiao",
      "Piermaria Mendolicchio",
      "Gavin Buttimore",
      "Varvara Krayvanova",
      "Nigamaa Nayakanti",
      "Matthew Wiethoff",
      "Yash Pande",
      "Azalia Mirhoseini",
      "Ni Lao",
      "Jasmine Liu",
      "Yiqing Hua",
      "Angie Chen",
      "Yury Malkov",
      "Dmitry Kalashnikov",
      "Shubham Gupta",
      "Kartik Audhkhasi",
      "Yuexiang Zhai",
      "Sudhindra Kopalle",
      "Prateek Jain",
      "Eran Ofek",
      "Clemens Meyer",
      "Khuslen Baatarsukh",
      "Hana Strej\u010dek",
      "Jun Qian",
      "James Freedman",
      "Ricardo Figueira",
      "Michal Sokolik",
      "Olivier Bachem",
      "Raymond Lin",
      "Dia Kharrat",
      "Chris Hidey",
      "Pingmei Xu",
      "Dennis Duan",
      "Yin Li",
      "Muge Ersoy",
      "Richard Everett",
      "Kevin Cen",
      "Rebeca Santamaria-Fernandez",
      "Amir Taubenfeld",
      "Ian Mackinnon",
      "Linda Deng",
      "Polina Zablotskaia",
      "Shashank Viswanadha",
      "Shivanker Goel",
      "Damion Yates",
      "Yunxiao Deng",
      "Peter Choy",
      "Mingqing Chen",
      "Abhishek Sinha",
      "Alex Mossin",
      "Yiming Wang",
      "Arthur Szlam",
      "Susan Hao",
      "Paul Kishan Rubenstein",
      "Metin Toksoz-Exley",
      "Miranda Aperghis",
      "Yin Zhong",
      "Junwhan Ahn",
      "Michael Isard",
      "Olivier Lacombe",
      "Florian Luisier",
      "Chrysovalantis Anastasiou",
      "Yogesh Kalley",
      "Utsav Prabhu",
      "Emma Dunleavy",
      "Shaan Bijwadia",
      "Justin Mao-Jones",
      "Kelly Chen",
      "Rama Pasumarthi",
      "Emily Wood",
      "Adil Dostmohamed",
      "Nate Hurley",
      "Jiri Simsa",
      "Alicia Parrish",
      "Mantas Pajarskas",
      "Matt Harvey",
      "Ondrej Skopek",
      "Yony Kochinski",
      "Javier Rey",
      "Verena Rieser",
      "Denny Zhou",
      "Sun Jae Lee",
      "Trilok Acharya",
      "Guowang Li",
      "Joe Jiang",
      "Xiaofan Zhang",
      "Bryant Gipson",
      "Ethan Mahintorabi",
      "Marco Gelmi",
      "Nima Khajehnouri",
      "Angel Yeh",
      "Kayi Lee",
      "Loic Matthey",
      "Leslie Baker",
      "Trang Pham",
      "Han Fu",
      "Alex Pak",
      "Prakhar Gupta",
      "Cristina Vasconcelos",
      "Adam Sadovsky",
      "Brian Walker",
      "Sissie Hsiao",
      "Patrik Zochbauer",
      "Andreea Marzoca",
      "Noam Velan",
      "Junhao Zeng",
      "Gilles Baechler",
      "Danny Driess",
      "Divya Jain",
      "Yanping Huang",
      "Lizzie Tao",
      "John Maggs",
      "Nir Levine",
      "Jon Schneider",
      "Erika Gemzer",
      "Samuel Petit",
      "Shan Han",
      "Zach Fisher",
      "Dustin Zelle",
      "Courtney Biles",
      "Eugene Ie",
      "Asya Fadeeva",
      "Casper Liu",
      "Juliana Vicente Franco",
      "Adrian Collister",
      "Hao Zhang",
      "Renshen Wang",
      "Ruizhe Zhao",
      "Leandro Kieliger",
      "Kurt Shuster",
      "Rui Zhu",
      "Boqing Gong",
      "Lawrence Chan",
      "Ruoxi Sun",
      "Sujoy Basu",
      "Roland Zimmermann",
      "Jamie Hayes",
      "Abhishek Bapna",
      "Jasper Snoek",
      "Weel Yang",
      "Puranjay Datta",
      "Jad Al Abdallah",
      "Kevin Kilgour",
      "Lu Li",
      "SQ Mah",
      "Yennie Jun",
      "Morgane Rivi\u00e8re",
      "Abhijit Karmarkar",
      "Tammo Spalink",
      "Tao Huang",
      "Lucas Gonzalez",
      "Duc-Hieu Tran",
      "Averi Nowak",
      "John Palowitch",
      "Martin Chadwick",
      "Ellie Talius",
      "Harsh Mehta",
      "Thibault Sellam",
      "Philipp Fr\u00e4nken",
      "Massimo Nicosia",
      "Kyle He",
      "Aditya Kini",
      "David Amos",
      "Sugato Basu",
      "Harrison Jobe",
      "Eleni Shaw",
      "Qiantong Xu",
      "Colin Evans",
      "Daisuke Ikeda",
      "Chaochao Yan",
      "Larry Jin",
      "Lun Wang",
      "Sachin Yadav",
      "Ilia Labzovsky",
      "Ramesh Sampath",
      "Ada Ma",
      "Candice Schumann",
      "Aditya Siddhant",
      "Rohin Shah",
      "John Youssef",
      "Rishabh Agarwal",
      "Natalie Dabney",
      "Alessio Tonioni",
      "Moran Ambar",
      "Jing Li",
      "Isabelle Guyon",
      "Benny Li",
      "David Soergel",
      "Boya Fang",
      "Georgi Karadzhov",
      "Cristian Udrescu",
      "Trieu Trinh",
      "Vikas Raunak",
      "Seb Noury",
      "Dee Guo",
      "Sonal Gupta",
      "Mara Finkelstein",
      "Denis Petek",
      "Lihao Liang",
      "Greg Billock",
      "Pei Sun",
      "David Wood",
      "Yiwen Song",
      "Xiaobin Yu",
      "Tatiana Matejovicova",
      "Regev Cohen",
      "Kalyan Andra",
      "David D'Ambrosio",
      "Zhiwei Deng",
      "Vincent Nallatamby",
      "Ebrahim Songhori",
      "Rumen Dangovski",
      "Andrew Lampinen",
      "Pankil Botadra",
      "Adam Hillier",
      "Jiawei Cao",
      "Nagabhushan Baddi",
      "Adhi Kuncoro",
      "Toshihiro Yoshino",
      "Ankit Bhagatwala",
      "Marc\u00e1urelio Ranzato",
      "Rylan Schaeffer",
      "Tianlin Liu",
      "Shuai Ye",
      "Obaid Sarvana",
      "John Nham",
      "Chenkai Kuang",
      "Isabel Gao",
      "Jinoo Baek",
      "Shubham Mittal",
      "Ayzaan Wahid",
      "Anita Gergely",
      "Bin Ni",
      "Josh Feldman",
      "Carrie Muir",
      "Pascal Lamblin",
      "Wolfgang Macherey",
      "Ethan Dyer",
      "Logan Kilpatrick",
      "V\u00edctor Campos",
      "Mukul Bhutani",
      "Stanislav Fort",
      "Yanif Ahmad",
      "Aliaksei Severyn",
      "Kleopatra Chatziprimou",
      "Oleksandr Ferludin",
      "Mason Dimarco",
      "Aditya Kusupati",
      "Joe Heyward",
      "Dan Bahir",
      "Kevin Villela",
      "Katie Millican",
      "Dror Marcus",
      "Sanaz Bahargam",
      "Caglar Unlu",
      "Nicholas Roth",
      "Zichuan Wei",
      "Siddharth Gopal",
      "Deepanway Ghoshal",
      "Edward Lee",
      "Sharon Lin",
      "Jennie Lees",
      "Dayeong Lee",
      "Anahita Hosseini",
      "Connie Fan",
      "Seth Neel",
      "Marcus Wu",
      "Yasemin Altun",
      "Honglong Cai",
      "Enrique Piqueras",
      "Josh Woodward",
      "Alessandro Bissacco",
      "Salem Haykal",
      "Mahyar Bordbar",
      "Prasha Sundaram",
      "Sarah Hodkinson",
      "Daniel Toyama",
      "George Polovets",
      "Austin Myers",
      "Anu Sinha",
      "Tomer Levinboim",
      "Kashyap Krishnakumar",
      "Rachita Chhaparia",
      "Tatiana Sholokhova",
      "Nitesh Bharadwaj Gundavarapu",
      "Ganesh Jawahar",
      "Haroon Qureshi",
      "Jieru Hu",
      "Nikola Momchev",
      "Matthew Rahtz",
      "Renjie Wu",
      "Aishwarya P S",
      "Kedar Dhamdhere",
      "Meiqi Guo",
      "Umang Gupta",
      "Ali Eslami",
      "Mariano Schain",
      "Michiel Blokzijl",
      "David Welling",
      "Dave Orr",
      "Levent Bolelli",
      "Nicolas Perez-Nieves",
      "Mikhail Sirotenko",
      "Aman Prasad",
      "Arjun Kar",
      "Borja De Balle Pigem",
      "Tayfun Terzi",
      "Gell\u00e9rt Weisz",
      "Dipankar Ghosh",
      "Aditi Mavalankar",
      "Dhruv Madeka",
      "Kaspar Daugaard",
      "Hartwig Adam",
      "Viraj Shah",
      "Dana Berman",
      "Maggie Tran",
      "Steven Baker",
      "Ewa Andrejczuk",
      "Grishma Chole",
      "Ganna Raboshchuk",
      "Mahdi Mirzazadeh",
      "Thais Kagohara",
      "Shimu Wu",
      "Christian Schallhart",
      "Bernett Orlando",
      "Chen Wang",
      "Alban Rrustemi",
      "Hao Xiong",
      "Hao Liu",
      "Arpi Vezer",
      "Nolan Ramsden",
      "Shuo-yiin Chang",
      "Sidharth Mudgal",
      "Yan Li",
      "Nino Vieillard",
      "Yedid Hoshen",
      "Farooq Ahmad",
      "Ambrose Slone",
      "Amy Hua",
      "Natan Potikha",
      "Mirko Rossini",
      "Jon Stritar",
      "Sushant Prakash",
      "Zifeng Wang",
      "Xuanyi Dong",
      "Alireza Nazari",
      "Efrat Nehoran",
      "Kaan Tekelioglu",
      "Yinxiao Li",
      "Kartikeya Badola",
      "Tom Funkhouser",
      "Yuanzhen Li",
      "Varun Yerram",
      "Ramya Ganeshan",
      "Daniel Formoso",
      "Karol Langner",
      "Tian Shi",
      "Huijian Li",
      "Yumeya Yamamori",
      "Amayika Panda",
      "Alaa Saade",
      "Angelo Scorza Scarpati",
      "Chris Breaux",
      "CJ Carey",
      "Zongwei Zhou",
      "Cho-Jui Hsieh",
      "Sophie Bridgers",
      "Alena Butryna",
      "Nishesh Gupta",
      "Vaibhav Tulsyan",
      "Sanghyun Woo",
      "Evgenii Eltyshev",
      "Will Grathwohl",
      "Chanel Parks",
      "Seth Benjamin",
      "Rina Panigrahy",
      "Shenil Dodhia",
      "Daniel De Freitas",
      "Chris Sauer",
      "Will Song",
      "Ferran Alet",
      "Jackson Tolins",
      "Cosmin Paduraru",
      "Xingyi Zhou",
      "Brian Albert",
      "Zizhao Zhang",
      "Lei Shu",
      "Mudit Bansal",
      "Sarah Nguyen",
      "Amir Globerson",
      "Owen Xiao",
      "James Manyika",
      "Tom Hennigan",
      "Rong Rong",
      "Josip Matak",
      "Anton Bakalov",
      "Ankur Sharma",
      "Danila Sinopalnikov",
      "Andrew Pierson",
      "Stephen Roller",
      "Geoff Brown",
      "Mingcen Gao",
      "Toshiyuki Fukuzawa",
      "Amin Ghafouri",
      "Kenny Vassigh",
      "Iain Barr",
      "Zhicheng Wang",
      "Anna Korsun",
      "Rajesh Jayaram",
      "Lijie Ren",
      "Tim Zaman",
      "Samira Khan",
      "Yana Lunts",
      "Dan Deutsch",
      "Dave Uthus",
      "Nitzan Katz",
      "Masha Samsikova",
      "Amr Khalifa",
      "Nikhil Sethi",
      "Jiao Sun",
      "Luming Tang",
      "Uri Alon",
      "Xianghong Luo",
      "Dian Yu",
      "Abhishek Nayyar",
      "Bryce Petrini",
      "Will Truong",
      "Vincent Hellendoorn",
      "Nikolai Chinaev",
      "Chris Alberti",
      "Wei Wang",
      "Jingcao Hu",
      "Vahab Mirrokni",
      "Ananth Balashankar",
      "Avia Aharon",
      "Aahil Mehta",
      "Ahmet Iscen",
      "Joseph Kready",
      "Lucas Manning",
      "Anhad Mohananey",
      "Yuankai Chen",
      "Anshuman Tripathi",
      "Allen Wu",
      "Igor Petrovski",
      "Dawsen Hwang",
      "Martin Baeuml",
      "Shreyas Chandrakaladharan",
      "Yuan Liu",
      "Rey Coaguila",
      "Maxwell Chen",
      "Sally Ma",
      "Pouya Tafti",
      "Susheel Tatineni",
      "Terry Spitz",
      "Jiayu Ye",
      "Paul Vicol",
      "Mihaela Rosca",
      "Adri\u00e0 Puigdom\u00e8nech",
      "Zohar Yahav",
      "Sanjay Ghemawat",
      "Hanzhao Lin",
      "Phoebe Kirk",
      "Zaid Nabulsi",
      "Sergey Brin",
      "Bernd Bohnet",
      "Ken Caluwaerts",
      "Aditya Srikanth Veerubhotla",
      "Dan Zheng",
      "Zihang Dai",
      "Petre Petrov",
      "Yichong Xu",
      "Ramin Mehran",
      "Zhuo Xu",
      "Luisa Zintgraf",
      "Jiho Choi",
      "Spurthi Amba Hombaiah",
      "Romal Thoppilan",
      "Sashank Reddi",
      "Lukasz Lew",
      "Li Li",
      "Kellie Webster",
      "KP Sawhney",
      "Lampros Lamprou",
      "Siamak Shakeri",
      "Mayank Lunayach",
      "Jianmin Chen",
      "Sumit Bagri",
      "Alex Salcianu",
      "Ying Chen",
      "Yani Donchev",
      "Charlotte Magister",
      "Signe N\u00f8rly",
      "Vitor Rodrigues",
      "Tomas Izo",
      "Hila Noga",
      "Joe Zou",
      "Thomas K\u00f6ppe",
      "Wenxuan Zhou",
      "Kenton Lee",
      "Xiangzhu Long",
      "Danielle Eisenbud",
      "Anthony Chen",
      "Connor Schenck",
      "Chi Ming To",
      "Peilin Zhong",
      "Emanuel Taropa",
      "Minh Truong",
      "Omer Levy",
      "Danilo Martins",
      "Zhiyuan Zhang",
      "Christopher Semturs",
      "Kelvin Zhang",
      "Alex Yakubovich",
      "Pol Moreno",
      "Lara McConnaughey",
      "Di Lu",
      "Sam Redmond",
      "Lotte Weerts",
      "Yonatan Bitton",
      "Tiziana Refice",
      "Nicolas Lacasse",
      "Arthur Conmy",
      "Corentin Tallec",
      "Julian Odell",
      "Hannah Forbes-Pollard",
      "Arkadiusz Socala",
      "Jonathan Hoech",
      "Pushmeet Kohli",
      "Alanna Walton",
      "Rui Wang",
      "Mikita Sazanovich",
      "Kexin Zhu",
      "Andrei Kapishnikov",
      "Rich Galt",
      "Matthew Denton",
      "Ben Murdoch",
      "Caitlin Sikora",
      "Kareem Mohamed",
      "Wei Wei",
      "Uri First",
      "Tim McConnell",
      "Luis C. Cobo",
      "James Qin",
      "Thi Avrahami",
      "Daniel Balle",
      "Yu Watanabe",
      "Annie Louis",
      "Adam Kraft",
      "Setareh Ariafar",
      "Yiming Gu",
      "Eug\u00e9nie Rives",
      "Charles Yoon",
      "Andrei Rusu",
      "James Cobon-Kerr",
      "Chris Hahn",
      "Jiaming Luo",
      "Yuvein",
      "Zhu",
      "Niharika Ahuja",
      "Rodrigo Benenson",
      "Rapha\u00ebl Lopez Kaufman",
      "Honglin Yu",
      "Lloyd Hightower",
      "Junlin Zhang",
      "Darren Ni",
      "Lisa Anne Hendricks",
      "Gabby Wang",
      "Gal Yona",
      "Lalit Jain",
      "Pablo Barrio",
      "Surya Bhupatiraju",
      "Siva Velusamy",
      "Allan Dafoe",
      "Sebastian Riedel",
      "Tara Thomas",
      "Zhe Yuan",
      "Mathias Bellaiche",
      "Sheena Panthaplackel",
      "Klemen Kloboves",
      "Sarthak Jauhari",
      "Canfer Akbulut",
      "Todor Davchev",
      "Evgeny Gladchenko",
      "David Madras",
      "Aleksandr Chuklin",
      "Tyrone Hill",
      "Quan Yuan",
      "Mukundan Madhavan",
      "Luke Leonhard",
      "Dylan Scandinaro",
      "Qihang Chen",
      "Ning Niu",
      "Arthur Douillard",
      "Bogdan Damoc",
      "Yasumasa Onoe",
      "Fabian Pedregosa",
      "Fred Bertsch",
      "Chas Leichner",
      "Joseph Pagadora",
      "Jonathan Malmaud",
      "Sameera Ponda",
      "Andy Twigg",
      "Oleksii Duzhyi",
      "Jingwei Shen",
      "Miaosen Wang",
      "Roopal Garg",
      "Jing Chen",
      "Utku Evci",
      "Jonathan Lee",
      "Leon Liu",
      "Koji Kojima",
      "Masa Yamaguchi",
      "Arunkumar Rajendran",
      "AJ Piergiovanni",
      "Vinodh Kumar Rajendran",
      "Marco Fornoni",
      "Gabriel Ibagon",
      "Harry Ragan",
      "Sadh MNM Khan",
      "John Blitzer",
      "Andrew Bunner",
      "Guan Sun",
      "Takahiro Kosakai",
      "Scott Lundberg",
      "Ndidi Elue",
      "Kelvin Guu",
      "SK Park",
      "Jane Park",
      "Arunachalam Narayanaswamy",
      "Chengda Wu",
      "Jayaram Mudigonda",
      "Trevor Cohn",
      "Hairong Mu",
      "Ravi Kumar",
      "Laura Graesser",
      "Yichi Zhang",
      "Richard Killam",
      "Vincent Zhuang",
      "Mai Gim\u00e9nez",
      "Wael Al Jishi",
      "Ruy Ley-Wild",
      "Alex Zhai",
      "Kazuki Osawa",
      "Diego Cedillo",
      "Jialu Liu",
      "Mayank Upadhyay",
      "Marcin Sieniek",
      "Roshan Sharma",
      "Tom Paine",
      "Anelia Angelova",
      "Sravanti Addepalli",
      "Carolina Parada",
      "Kingshuk Majumder",
      "Avery Lamp",
      "Sanjiv Kumar",
      "Xiang Deng",
      "Artiom Myaskovsky",
      "Tea Saboli\u0107",
      "Jeffrey Dudek",
      "Sarah York",
      "F\u00e9lix de Chaumont Quitry",
      "Jiazhong Nie",
      "Dee Cattle",
      "Alok Gunjan",
      "Bilal Piot",
      "Waleed Khawaja",
      "Seojin Bang",
      "Simon Wang",
      "Siavash Khodadadeh",
      "Raghavender R",
      "Praynaa Rawlani",
      "Richard Powell",
      "Kevin Lee",
      "Johannes Griesser",
      "GS Oh",
      "Cesar Magalhaes",
      "Yujia Li",
      "Simon Tokumine",
      "Hadas Natalie Vogel",
      "Dennis Hsu",
      "Arturo BC",
      "Disha Jindal",
      "Matan Cohen",
      "Zi Yang",
      "Junwei Yuan",
      "Dario de Cesare",
      "Tony Bruguier",
      "Jun Xu",
      "Monica Roy",
      "Alon Jacovi",
      "Dan Belov",
      "Rahul Arya",
      "Phoenix Meadowlark",
      "Shlomi Cohen-Ganor",
      "Wenting Ye",
      "Patrick Morris-Suzuki",
      "Praseem Banzal",
      "Gan Song",
      "Pranavaraj Ponnuramu",
      "Fred Zhang",
      "George Scrivener",
      "Salah Zaiem",
      "Alif Raditya Rochman",
      "Kehang Han",
      "Badih Ghazi",
      "Kate Lee",
      "Shahar Drath",
      "Daniel Suo",
      "Antonious Girgis",
      "Pradeep Shenoy",
      "Duy Nguyen",
      "Douglas Eck",
      "Somit Gupta",
      "Le Yan",
      "Joao Carreira",
      "Anmol Gulati",
      "Ruoxin Sang",
      "Daniil Mirylenka",
      "Emma Cooney",
      "Edward Chou",
      "Mingyang Ling",
      "Cindy Fan",
      "Ben Coleman",
      "Guilherme Tubone",
      "Ravin Kumar",
      "Jason Baldridge",
      "Felix Hernandez-Campos",
      "Angeliki Lazaridou",
      "James Besley",
      "Itay Yona",
      "Neslihan Bulut",
      "Quentin Wellens",
      "AJ Pierigiovanni",
      "Jasmine George",
      "Richard Green",
      "Pu Han",
      "Connie Tao",
      "Geoff Clark",
      "Chong You",
      "Abbas Abdolmaleki",
      "Justin Fu",
      "Tongzhou Chen",
      "Ashwin Chaugule",
      "Angad Chandorkar",
      "Altaf Rahman",
      "Will Thompson",
      "Penporn Koanantakool",
      "Mike Bernico",
      "Jie Ren",
      "Andrey Vlasov",
      "Sergei Vassilvitskii",
      "Maciej Kula",
      "Yizhong Liang",
      "Dahun Kim",
      "Yangsibo Huang",
      "Chengxi Ye",
      "Dmitry Lepikhin",
      "Wesley Helmholz"
    ],
    "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.",
    "published": "Jul 07",
    "pdf_url": "https://arxiv.org/pdf/2507.06261v6",
    "arxiv_url": "http://arxiv.org/abs/2507.06261v6",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "MedVAL: Toward Expert-Level Medical Text Validation with Language Models",
    "authors": [
      "Asad Aali",
      "Vasiliki Bikia",
      "Maya Varma",
      "Nicole Chiou",
      "Sophie Ostmeier",
      "Arnav Singhvi",
      "Magdalini Paschali",
      "Ashwin Kumar",
      "Andrew Johnston",
      "Karimar Amador-Martinez",
      "Eduardo Juan Perez Guerrero",
      "Paola Naovi Cruz Rivera",
      "Sergios Gatidis",
      "Christian Bluethgen",
      "Eduardo Pontes Reis",
      "Eddy D. Zandee van Rilland",
      "Poonam Laxmappa Hosamani",
      "Kevin R Keet",
      "Minjoung Go",
      "Evelyn Ling",
      "David B. Larson",
      "Curtis Langlotz",
      "Roxana Daneshjou",
      "Jason Hom",
      "Sanmi Koyejo",
      "Emily Alsentzer",
      "Akshay S. Chaudhari"
    ],
    "summary": "With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the \"LLM-as-a-judge\" paradigm offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. We propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert on a subset annotated by multiple physicians (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) C...",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.03152v5",
    "arxiv_url": "http://arxiv.org/abs/2507.03152v5",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
    "authors": [
      "Almog Hilel",
      "Idan Shenfeld",
      "Jacob Andreas",
      "Leshem Choshen"
    ],
    "summary": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.02850v2",
    "arxiv_url": "http://arxiv.org/abs/2507.02850v2",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Generalizing Verifiable Instruction Following",
    "authors": [
      "Valentina Pyatkin",
      "Saumya Malik",
      "Victoria Graf",
      "Hamish Ivison",
      "Shengyi Huang",
      "Pradeep Dasigi",
      "Nathan Lambert",
      "Hannaneh Hajishirzi"
    ],
    "summary": "A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.02833v3",
    "arxiv_url": "http://arxiv.org/abs/2507.02833v3",
    "queried_author": "Hamish Ivison",
    "matching_authors": [
      "Hamish Ivison",
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
    "authors": [
      "Yuxuan Zhu",
      "Tengjun Jin",
      "Yada Pruksachatkun",
      "Andy Zhang",
      "Shu Liu",
      "Sasha Cui",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Kevin Meng",
      "Rebecca Weiss",
      "Fazl Barez",
      "Rahul Gupta",
      "Jwala Dhamala",
      "Jacob Merizian",
      "Mario Giulianelli",
      "Harry Coppock",
      "Cozmin Ududec",
      "Jasjeet Sekhon",
      "Jacob Steinhardt",
      "Antony Kellermann",
      "Sarah Schwettmann",
      "Matei Zaharia",
      "Ion Stoica",
      "Percy Liang",
      "Daniel Kang"
    ],
    "summary": "Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues in task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation of agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.02825v5",
    "arxiv_url": "http://arxiv.org/abs/2507.02825v5",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Understanding and Improving Length Generalization in Recurrent Models",
    "authors": [
      "Ricardo Buitrago Ruiz",
      "Albert Gu"
    ],
    "summary": "Recently, recurrent models such as state space models and linear attention have become popular due to their linear complexity in the sequence length. Thanks to their recurrent nature, in principle they can process arbitrarily long sequences, but their performance sometimes drops considerably beyond their training context lengths-i.e. they fail to length generalize. In this work, we provide comprehensive empirical and theoretical analysis to support the unexplored states hypothesis, which posits that models fail to length generalize when during training they are only exposed to a limited subset of the distribution of all attainable states (i.e. states that would be attained if the recurrence was applied to long sequences). Furthermore, we investigate simple training interventions that aim to increase the coverage of the states that the model is trained on, e.g. by initializing the state with Gaussian noise or with the final state of a different input sequence. With only 500 post-training steps ($\\sim 0.1\\%$ of the pre-training budget), these interventions enable length generalization for sequences that are orders of magnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and show improved performance in long context tasks, thus presenting a simple and efficient way to enable robust length generalization in general recurrent models.",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.02782v2",
    "arxiv_url": "http://arxiv.org/abs/2507.02782v2",
    "queried_author": "Albert Gu",
    "matching_authors": [
      "Albert Gu"
    ]
  },
  {
    "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards",
    "authors": [
      "Shirley Wu",
      "Parth Sarthi",
      "Shiyu Zhao",
      "Aaron Lee",
      "Herumb Shandilya",
      "Adrian Mladenic Grobelnik",
      "Nurendra Choudhary",
      "Eddie Huang",
      "Karthik Subbian",
      "Linjun Zhang",
      "Diyi Yang",
      "James Zou",
      "Jure Leskovec"
    ],
    "summary": "Compound AI systems integrating multiple components, such as Large Language Models, specialized tools, and traditional machine learning models, are increasingly deployed to solve complex real-world tasks. However, optimizing compound systems remains challenging due to their non-differentiable structures and diverse configuration types across components, including prompts, hyperparameters, and model parameters. To address this challenge, we propose Optimas, a unified framework for effective optimization of compound systems. The core idea of Optimas is to maintain one Local Reward Function (LRF) per component, each satisfying a local-global alignment property, i.e., each component's local reward correlates with the global system performance. In each iteration, Optimas efficiently adapts the LRFs to maintain this property while simultaneously maximizing each component's local reward. This approach enables independent updates of heterogeneous configurations using the designated optimization method, while ensuring that local improvements consistently lead to performance gains. We present extensive evaluations across five real-world compound systems to demonstrate that Optimas outperforms strong baselines by an average improvement of 11.92%, offering a general and effective approach for improving compound systems. Our website is at https://optimas.stanford.edu.",
    "published": "Jul 03",
    "pdf_url": "https://arxiv.org/pdf/2507.03041v4",
    "arxiv_url": "http://arxiv.org/abs/2507.03041v4",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Challenges for AI in Multimodal STEM Assessments: a Human-AI Comparison",
    "authors": [
      "Aymeric de Chillaz",
      "Anna Sotnikova",
      "Patrick Jermann",
      "Antoine Bosselut"
    ],
    "summary": "Generative AI systems have rapidly advanced, with multimodal input capabilities enabling reasoning beyond text-based tasks. In education, these advancements could influence assessment design and question answering, presenting both opportunities and challenges. To investigate these effects, we introduce a high-quality dataset of 201 university-level STEM questions, manually annotated with features such as image type, role, problem complexity, and question format. Our study analyzes how these features affect generative AI performance compared to students. We evaluate four model families with five prompting strategies, comparing results to the average of 546 student responses per question. Although the best model correctly answers on average 58.5 % of the questions using majority vote aggregation, human participants consistently outperform AI on questions involving visual components. Interestingly, human performance remains stable across question features but varies by subject, whereas AI performance is susceptible to both subject matter and question features. Finally, we provide actionable insights for educators, demonstrating how question design can enhance academic integrity by leveraging features that challenge current AI systems without increasing the cognitive burden for students.",
    "published": "Jul 02",
    "pdf_url": "https://arxiv.org/pdf/2507.03013v1",
    "arxiv_url": "http://arxiv.org/abs/2507.03013v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing",
    "authors": [
      "Inyoung Cheong",
      "Alicia Guo",
      "Mina Lee",
      "Zhehui Liao",
      "Kowe Kadoma",
      "Dongyoung Go",
      "Joseph Chee Chang",
      "Peter Henderson",
      "Mor Naaman",
      "Amy X. Zhang"
    ],
    "summary": "As AI integrates in various types of human writing, calls for transparency around AI assistance are growing. However, if transparency operates on uneven ground and certain identity groups bear a heavier cost for being honest, then the burden of openness becomes asymmetrical. This study investigates how AI disclosure statement affects perceptions of writing quality, and whether these effects vary by the author's race and gender. Through a large-scale controlled experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated a single human-written news article while disclosure statements and author demographics were systematically varied. This approach reflects how both human and algorithmic decisions now influence access to opportunities (e.g., hiring, promotion) and social recognition (e.g., content recommendation algorithms). We find that both human and LLM raters consistently penalize disclosed AI use. However, only LLM raters exhibit demographic interaction effects: they favor articles attributed to women or Black authors when no disclosure is present. But these advantages disappear when AI assistance is revealed. These findings illuminate the complex relationships between AI disclosure and author identity, highlighting disparities between machine and human evaluation patterns.",
    "published": "Jul 02",
    "pdf_url": "https://arxiv.org/pdf/2507.01418v1",
    "arxiv_url": "http://arxiv.org/abs/2507.01418v1",
    "queried_author": "Peter Henderson",
    "matching_authors": [
      "Peter Henderson"
    ]
  },
  {
    "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks",
    "authors": [
      "Xinxi Lyu",
      "Michael Duan",
      "Rulin Shao",
      "Pang Wei Koh",
      "Sewon Min"
    ],
    "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, support...",
    "published": "Jul 02",
    "pdf_url": "https://arxiv.org/pdf/2507.01297v2",
    "arxiv_url": "http://arxiv.org/abs/2507.01297v2",
    "queried_author": "Pang Wei Koh",
    "matching_authors": [
      "Pang Wei Koh"
    ]
  },
  {
    "title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks",
    "authors": [
      "Yilun Zhao",
      "Kaiyan Zhang",
      "Tiansheng Hu",
      "Sihong Wu",
      "Ronan Le Bras",
      "Charles McGrady",
      "Taira Anderson",
      "Jonathan Bragg",
      "Joseph Chee Chang",
      "Jesse Dodge",
      "Matt Latzke",
      "Yixin Liu",
      "Xiangru Tang",
      "Zihang Wang",
      "Chen Zhao",
      "Hannaneh Hajishirzi",
      "Doug Downey",
      "Arman Cohan"
    ],
    "summary": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.",
    "published": "Jul 01",
    "pdf_url": "https://arxiv.org/pdf/2507.01001v2",
    "arxiv_url": "http://arxiv.org/abs/2507.01001v2",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Jesse Dodge"
    ]
  },
  {
    "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America",
    "authors": [
      "Mar\u00eda Grandury",
      "Javier Aula-Blasco",
      "J\u00falia Falc\u00e3o",
      "Cl\u00e9mentine Fourrier",
      "Miguel Gonz\u00e1lez",
      "Gonzalo Mart\u00ednez",
      "Gonzalo Santamar\u00eda",
      "Rodrigo Agerri",
      "Nuria Aldama",
      "Luis Chiruzzo",
      "Javier Conde",
      "Helena G\u00f3mez",
      "Marta Guerrero",
      "Guido Ivetta",
      "Natalia L\u00f3pez",
      "Flor Miriam Plaza-del-Arco",
      "Mar\u00eda Teresa Mart\u00edn-Valdivia",
      "Helena Montoro",
      "Carmen Mu\u00f1oz",
      "Pedro Reviriego",
      "Leire Rosado",
      "Alejandro Vaca",
      "Mar\u00eda Estrella Vallecillo-Rodr\u00edguez",
      "Jorge Vallego",
      "Irune Zubiaga"
    ],
    "summary": "Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Basque, Catalan, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.",
    "published": "Jul 01",
    "pdf_url": "https://arxiv.org/pdf/2507.00999v1",
    "arxiv_url": "http://arxiv.org/abs/2507.00999v1",
    "queried_author": "Cl\u00e9mentine Fourrier",
    "matching_authors": [
      "Cl\u00e9mentine Fourrier"
    ]
  },
  {
    "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing",
    "authors": [
      "Daniel Fein",
      "Sebastian Russo",
      "Violet Xiang",
      "Kabir Jolly",
      "Rafael Rafailov",
      "Nick Haber"
    ],
    "summary": "Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.",
    "published": "Jul 01",
    "pdf_url": "https://arxiv.org/pdf/2507.00769v1",
    "arxiv_url": "http://arxiv.org/abs/2507.00769v1",
    "queried_author": "Rafael Rafailov",
    "matching_authors": [
      "Rafael Rafailov"
    ]
  },
  {
    "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning",
    "authors": [
      "Maggie Huan",
      "Yuetai Li",
      "Tuney Zheng",
      "Xiaoyu Xu",
      "Seungone Kim",
      "Minxin Du",
      "Radha Poovendran",
      "Graham Neubig",
      "Xiang Yue"
    ],
    "summary": "Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.",
    "published": "Jul 01",
    "pdf_url": "https://arxiv.org/pdf/2507.00432v2",
    "arxiv_url": "http://arxiv.org/abs/2507.00432v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig",
      "Seungone Kim"
    ]
  },
  {
    "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context",
    "authors": [
      "Joongwon Kim",
      "Anirudh Goyal",
      "Liang Tan",
      "Hannaneh Hajishirzi",
      "Srinivasan Iyer",
      "Tianlu Wang"
    ],
    "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.",
    "published": "Jul 01",
    "pdf_url": "https://arxiv.org/pdf/2507.00417v1",
    "arxiv_url": "http://arxiv.org/abs/2507.00417v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Linearly Decoding Refused Knowledge in Aligned Language Models",
    "authors": [
      "Aryan Shrivastava",
      "Ari Holtzman"
    ],
    "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely \"leftover\" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving ...",
    "published": "Jun 30",
    "pdf_url": "https://arxiv.org/pdf/2507.00239v1",
    "arxiv_url": "http://arxiv.org/abs/2507.00239v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Prompting as Scientific Inquiry",
    "authors": [
      "Ari Holtzman",
      "Chenhao Tan"
    ],
    "summary": "Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.",
    "published": "Jun 30",
    "pdf_url": "https://arxiv.org/pdf/2507.00163v2",
    "arxiv_url": "http://arxiv.org/abs/2507.00163v2",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions",
    "authors": [
      "Xianzhe Fan",
      "Xuhui Zhou",
      "Chuanyang Jin",
      "Kolby Nottingham",
      "Hao Zhu",
      "Maarten Sap"
    ],
    "summary": "Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1%...",
    "published": "Jun 29",
    "pdf_url": "https://arxiv.org/pdf/2506.23046v3",
    "arxiv_url": "http://arxiv.org/abs/2506.23046v3",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds",
    "authors": [
      "Edward Chen",
      "Sang T. Truong",
      "Natalie Dullerud",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "summary": "High-stakes decision-making involves navigating multiple competing objectives with expensive evaluations. For instance, in brachytherapy, clinicians must balance maximizing tumor coverage (e.g., an aspirational target or soft bound of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard bound of <601 cGy to the bladder), with each plan evaluation being resource-intensive. Selecting Pareto-optimal solutions that match implicit preferences is challenging, as exhaustive Pareto frontier exploration is computationally and cognitively prohibitive, necessitating interactive frameworks to guide users. While decision-makers (DMs) often possess domain knowledge to narrow the search via such soft-hard bounds, current methods often lack systematic approaches to iteratively refine these multi-faceted preference structures. Critically, DMs must trust their final decision, confident they haven't missed superior alternatives; this trust is paramount in high-consequence scenarios. We present Active-MoSH, an interactive local-global framework designed for this process. Its local component integrates soft-hard bounds with probabilistic preference learning, maintaining distributions over DM preferences and bounds for adaptive Pareto subset refinement. This is guided by an active sampling strategy optimizing exploration-exploitation while minimizing cognitive burden. To build DM trust, Active-MoSH's global component, T-MoSH, leverages multi-objective sensitivity analysis to identify potentially overlooked, high-value points beyond immediate feedback. We demonstrate Activ...",
    "published": "Jun 27",
    "pdf_url": "https://arxiv.org/pdf/2506.21887v1",
    "arxiv_url": "http://arxiv.org/abs/2506.21887v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Can Gradient Descent Simulate Prompting?",
    "authors": [
      "Eric Zhang",
      "Leshem Choshen",
      "Jacob Andreas"
    ],
    "summary": "There are two primary ways of incorporating new information into a language model (LM): changing its prompt or changing its parameters, e.g. via fine-tuning. Parameter updates incur no long-term storage cost for model changes. However, for many model updates, prompting is significantly more effective: prompted models can generalize robustly from single examples and draw logical inferences that do not occur under standard fine-tuning. Can models be modified so that fine-tuning does emulate prompting? This paper describes a method for meta-training LMs such that gradient updates emulate the effects of conditioning on new information. Our approach uses tools from gradient-based meta-learning but uses an LM's own prompted predictions as targets, eliminating the need for ground-truth labels. Subsequent gradient descent training recovers some (and occasionally all) of prompted model performance -- showing improvement on the ``reversal curse'' tasks, and answering questions about text passages after a single gradient update. These results suggest that, with appropriate initialization, gradient descent can be surprisingly expressive. Our results suggest new avenues for long-context modeling and offer insight into the generalization capabilities of gradient-based learning.",
    "published": "Jun 26",
    "pdf_url": "https://arxiv.org/pdf/2506.20989v1",
    "arxiv_url": "http://arxiv.org/abs/2506.20989v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language",
    "authors": [
      "Guilherme Penedo",
      "Hynek Kydl\u00ed\u010dek",
      "Vinko Sabol\u010dec",
      "Bettina Messmer",
      "Negar Foroutan",
      "Amir Hossein Kargaran",
      "Colin Raffel",
      "Martin Jaggi",
      "Leandro Von Werra",
      "Thomas Wolf"
    ],
    "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.",
    "published": "Jun 26",
    "pdf_url": "https://arxiv.org/pdf/2506.20920v1",
    "arxiv_url": "http://arxiv.org/abs/2506.20920v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas",
    "authors": [
      "Chenglei Si",
      "Tatsunori Hashimoto",
      "Diyi Yang"
    ],
    "summary": "Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, we conduct an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, we even observe that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes.",
    "published": "Jun 25",
    "pdf_url": "https://arxiv.org/pdf/2506.20803v1",
    "arxiv_url": "http://arxiv.org/abs/2506.20803v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Tatsunori Hashimoto"
    ]
  },
  {
    "title": "Multiple Streams of Knowledge Retrieval: Enriching and Recalling in Transformers",
    "authors": [
      "Todd Nief",
      "David Reber",
      "Sean Richardson",
      "Ari Holtzman"
    ],
    "summary": "When an LLM learns a new fact during finetuning (e.g., new movie releases, newly elected pope, etc.), where does this information go? Are entities enriched with relation information, or do models recall information just-in-time before a prediction? Or, are ``all of the above'' true with LLMs implementing multiple redundant heuristics? Existing localization approaches (e.g., activation patching) are ill-suited for this analysis because they usually \\textit{replace} parts of the residual stream, thus overriding previous information. To fill this gap, we propose \\emph{dynamic weight grafting}, a technique that selectively grafts weights from a finetuned model onto a pretrained model. Using this technique, we show two separate pathways for retrieving finetuned relation information: 1) ``enriching\" the residual stream with relation information while processing the tokens that correspond to an entity (e.g., ``Zendaya'' in ``Zendaya co-starred with John David Washington'') and 2) ``recalling\" this information at the final token position before generating a target fact. In some cases, models need information from both of these pathways to correctly generate finetuned facts while, in other cases, either the ``enrichment\" or ``recall\" pathway alone is sufficient. We localize the ``recall'' pathway to model components -- finding that ``recall\" occurs via both task-specific attention mechanisms and an entity-specific extraction step in the feedforward networks of the final layers before the target prediction. By targeting model components and parameters, as opposed to just activations,...",
    "published": "Jun 25",
    "pdf_url": "https://arxiv.org/pdf/2506.20746v2",
    "arxiv_url": "http://arxiv.org/abs/2506.20746v2",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "The Singapore Consensus on Global AI Safety Research Priorities",
    "authors": [
      "Yoshua Bengio",
      "Tegan Maharaj",
      "Luke Ong",
      "Stuart Russell",
      "Dawn Song",
      "Max Tegmark",
      "Lan Xue",
      "Ya-Qin Zhang",
      "Stephen Casper",
      "Wan Sie Lee",
      "S\u00f6ren Mindermann",
      "Vanessa Wilfred",
      "Vidhisha Balachandran",
      "Fazl Barez",
      "Michael Belinsky",
      "Imane Bello",
      "Malo Bourgon",
      "Mark Brakel",
      "Sim\u00e9on Campos",
      "Duncan Cass-Beggs",
      "Jiahao Chen",
      "Rumman Chowdhury",
      "Kuan Chua Seah",
      "Jeff Clune",
      "Juntao Dai",
      "Agnes Delaborde",
      "Nouha Dziri",
      "Francisco Eiras",
      "Joshua Engels",
      "Jinyu Fan",
      "Adam Gleave",
      "Noah Goodman",
      "Fynn Heide",
      "Johannes Heidecke",
      "Dan Hendrycks",
      "Cyrus Hodes",
      "Bryan Low Kian Hsiang",
      "Minlie Huang",
      "Sami Jawhar",
      "Wang Jingyu",
      "Adam Tauman Kalai",
      "Meindert Kamphuis",
      "Mohan Kankanhalli",
      "Subhash Kantamneni",
      "Mathias Bonde Kirk",
      "Thomas Kwa",
      "Jeffrey Ladish",
      "Kwok-Yan Lam",
      "Wan Lee Sie",
      "Taewhi Lee",
      "Xiaojian Li",
      "Jiajun Liu",
      "Chaochao Lu",
      "Yifan Mai",
      "Richard Mallah",
      "Julian Michael",
      "Nick Mo\u00ebs",
      "Simon M\u00f6ller",
      "Kihyuk Nam",
      "Kwan Yee Ng",
      "Mark Nitzberg",
      "Besmira Nushi",
      "Se\u00e1n O h\u00c9igeartaigh",
      "Alejandro Ortega",
      "Pierre Peign\u00e9",
      "James Petrie",
      "Benjamin Prud'Homme",
      "Reihaneh Rabbany",
      "Nayat Sanchez-Pi",
      "Sarah Schwettmann",
      "Buck Shlegeris",
      "Saad Siddiqui",
      "Aradhana Sinha",
      "Mart\u00edn Soto",
      "Cheston Tan",
      "Dong Ting",
      "William Tjhi",
      "Robert Trager",
      "Brian Tse",
      "Anthony Tung K. H.",
      "Vanessa Wilfred",
      "John Willes",
      "Denise Wong",
      "Wei Xu",
      "Rongwu Xu",
      "Yi Zeng",
      "HongJiang Zhang",
      "Djordje \u017dikeli\u0107"
    ],
    "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of transformation, but are also driving vigorous debate on how to ensure that AI is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem is therefore essential -- it helps people embrace AI with confidence and gives maximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific Exchange on AI Safety\" aimed to support research in this space by bringing together AI scientists across geographies to identify and synthesise research priorities in AI safety. This resulting report builds on the International AI Safety Report chaired by Yoshua Bengio and backed by 33 governments. By adopting a defence-in-depth model, this report organises AI safety research domains into three types: challenges with creating trustworthy AI systems (Development), challenges with evaluating their risks (Assessment), and challenges with monitoring and intervening after deployment (Control).",
    "published": "Jun 25",
    "pdf_url": "https://arxiv.org/pdf/2506.20702v2",
    "arxiv_url": "http://arxiv.org/abs/2506.20702v2",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman"
    ]
  },
  {
    "title": "Probing AI Safety with Source Code",
    "authors": [
      "Ujwal Narayan",
      "Shreyas Chaudhari",
      "Ashwin Kalyan",
      "Tanmay Rajpurohit",
      "Karthik Narasimhan",
      "Ameet Deshpande",
      "Vishvak Murahari"
    ],
    "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater safety measures to align these models with human values and preferences. In this work, we demonstrate that contemporary models fall concerningly short of the goal of AI safety, leading to an unsafe and harmful experience for users. We introduce a prompting strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT converts natural language inputs to simple code that represents the same intent. For instance, CoDoT transforms the natural language prompt \"Make the statement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT results in a consistent failure of a wide range of state-of-the-art LLMs. For example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of the time, and toxicity increases 300% on average across seven modern LLMs. Additionally, recursively applying CoDoT can further increase toxicity two times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together.",
    "published": "Jun 25",
    "pdf_url": "https://arxiv.org/pdf/2506.20471v1",
    "arxiv_url": "http://arxiv.org/abs/2506.20471v1",
    "queried_author": "Karthik Narasimhan",
    "matching_authors": [
      "Karthik Narasimhan"
    ]
  },
  {
    "title": "Report on NSF Workshop on Science of Safe AI",
    "authors": [
      "Rajeev Alur",
      "Greg Durrett",
      "Hadas Kress-Gazit",
      "Corina P\u0103s\u0103reanu",
      "Ren\u00e9 Vidal"
    ],
    "summary": "Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasoning and inner workings of today's complex AI models are not transparent to the user, and there are no safety guarantees regarding their predictions. Consequently, to fulfill the promise of AI, we must address the following scientific challenge: how to develop AI-based systems that are not only accurate and performant but also safe and trustworthy?\n  The criticality of safe operation is particularly evident for autonomous systems for control and robotics, and was the catalyst for the Safe Learning Enabled Systems (SLES) program at NSF. For the broader class of AI applications, such as users interacting with chatbots and clinicians receiving treatment recommendations, safety is, while no less important, less well-defined with context-dependent interpretations. This motivated the organization of a day-long workshop, held at University of Pennsylvania on February 26, 2025, to bring together investigators funded by the NSF SLES program with a broader pool of researchers studying AI safety. This report is the result of the discussions in the working groups that addressed different aspects of safety at the workshop. The report articulates a new research agenda focused on developing theory, methods, and tools that will provide the foundations of the next generation of AI-enabled systems.",
    "published": "Jun 24",
    "pdf_url": "https://arxiv.org/pdf/2506.22492v1",
    "arxiv_url": "http://arxiv.org/abs/2506.22492v1",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "Yegor Denisov-Blanch",
      "Brando Miranda",
      "Matthias Gerstgrasser",
      "Susan Zhang",
      "Andreas Haupt",
      "Isha Gupta",
      "Elyas Obbad",
      "Jesse Dodge",
      "Jessica Zosa Forde",
      "Francesco Orabona",
      "Sanmi Koyejo",
      "David Donoho"
    ],
    "summary": "Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are made. This position paper argues that ML conferences should establish a dedicated \"Refutations and Critiques\" (R&C) Track. This R&C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.",
    "published": "Jun 24",
    "pdf_url": "https://arxiv.org/pdf/2506.19882v3",
    "arxiv_url": "http://arxiv.org/abs/2506.19882v3",
    "queried_author": "Jesse Dodge",
    "matching_authors": [
      "Jesse Dodge",
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Thought Anchors: Which LLM Reasoning Steps Matter?",
    "authors": [
      "Paul C. Bogdan",
      "Uzay Macar",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "summary": "Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \\textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our ...",
    "published": "Jun 23",
    "pdf_url": "https://arxiv.org/pdf/2506.19143v4",
    "arxiv_url": "http://arxiv.org/abs/2506.19143v4",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations",
    "authors": [
      "Brian Siyuan Zheng",
      "Alisa Liu",
      "Orevaoghene Ahia",
      "Jonathan Hayase",
      "Yejin Choi",
      "Noah A. Smith"
    ],
    "summary": "Modern tokenizers employ deterministic algorithms to map text into a single \"canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the tokenizer vocabulary. In this work, we investigate the robustness of LMs to text encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization. We see that overall stronger models tend to be more robust, and robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we then identify settings where non-canonical tokenization schemes can *improve* performance, finding that character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We show that while both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings), base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less tied to their tokenizer than previously believed, and demonstrate the promise of intervening on tokenization at in...",
    "published": "Jun 23",
    "pdf_url": "https://arxiv.org/pdf/2506.19004v2",
    "arxiv_url": "http://arxiv.org/abs/2506.19004v2",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith",
      "Yejin Choi"
    ]
  },
  {
    "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization",
    "authors": [
      "Yiyou Sun",
      "Shawn Hu",
      "Georgia Zhou",
      "Ken Zheng",
      "Hannaneh Hajishirzi",
      "Nouha Dziri",
      "Dawn Song"
    ],
    "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformativ...",
    "published": "Jun 23",
    "pdf_url": "https://arxiv.org/pdf/2506.18880v1",
    "arxiv_url": "http://arxiv.org/abs/2506.18880v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi"
    ]
  },
  {
    "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
    "authors": [
      "Jon Saad-Falcon",
      "E. Kelly Buchanan",
      "Mayee F. Chen",
      "Tzu-Heng Huang",
      "Brendan McLaughlin",
      "Tanvir Bhathal",
      "Shang Zhu",
      "Ben Athiwaratkun",
      "Frederic Sala",
      "Scott Linderman",
      "Azalia Mirhoseini",
      "Christopher R\u00e9"
    ],
    "summary": "Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and rewa...",
    "published": "Jun 22",
    "pdf_url": "https://arxiv.org/pdf/2506.18203v2",
    "arxiv_url": "http://arxiv.org/abs/2506.18203v2",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
    "authors": [
      "Constantin Venhoff",
      "Iv\u00e1n Arcuschin",
      "Philip Torr",
      "Arthur Conmy",
      "Neel Nanda"
    ],
    "summary": "Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.",
    "published": "Jun 22",
    "pdf_url": "https://arxiv.org/pdf/2506.18167v4",
    "arxiv_url": "http://arxiv.org/abs/2506.18167v4",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies",
    "authors": [
      "Pranav Atreya",
      "Karl Pertsch",
      "Tony Lee",
      "Moo Jin Kim",
      "Arhan Jain",
      "Artur Kuramshin",
      "Clemens Eppner",
      "Cyrus Neary",
      "Edward Hu",
      "Fabio Ramos",
      "Jonathan Tremblay",
      "Kanav Arora",
      "Kirsty Ellis",
      "Luca Macesanu",
      "Marcel Torne Villasevil",
      "Matthew Leonard",
      "Meedeum Cho",
      "Ozgur Aslan",
      "Shivin Dass",
      "Jie Wang",
      "William Reger",
      "Xingfang Yuan",
      "Xuning Yang",
      "Abhishek Gupta",
      "Dinesh Jayaraman",
      "Glen Berseth",
      "Kostas Daniilidis",
      "Roberto Martin-Martin",
      "Youngwoon Lee",
      "Percy Liang",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "summary": "Comprehensive, unbiased, and comparable evaluation of modern generalist policies is uniquely challenging: existing approaches for robot benchmarking typically rely on heavy standardization, either by specifying fixed evaluation tasks and environments, or by hosting centralized ''robot challenges'', and do not readily scale to evaluating generalist policies across a broad range of tasks and environments. In this work, we propose RoboArena, a new approach for scalable evaluation of generalist robot policies in the real world. Instead of standardizing evaluations around fixed tasks, environments, or locations, we propose to crowd-source evaluations across a distributed network of evaluators. Importantly, evaluators can freely choose the tasks and environments they evaluate on, enabling easy scaling of diversity, but they are required to perform double-blind evaluations over pairs of policies. Then, by aggregating preference feedback from pairwise comparisons across diverse tasks and environments, we can derive a ranking of policies. We instantiate our approach across a network of evaluators at seven academic institutions using the DROID robot platform. Through more than 600 pairwise real-robot evaluation episodes across seven generalist policies, we demonstrate that our crowd-sourced approach can more accurately rank the performance of existing generalist policies than conventional, centralized evaluation approaches, while being more scalable, resilient, and trustworthy. We open our evaluation network to the community and hope that it can enable more accessible comparisons of ...",
    "published": "Jun 22",
    "pdf_url": "https://arxiv.org/pdf/2506.18123v2",
    "arxiv_url": "http://arxiv.org/abs/2506.18123v2",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "LLM Probability Concentration: How Alignment Shrinks the Generative Horizon",
    "authors": [
      "Chenghao Yang",
      "Ari Holtzman"
    ],
    "summary": "Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this consistency in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the *Branching Factor* (BF)--a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this consistency has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., ``Sure'') that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show prompting base models with such tokens can similarly reduce BF. T...",
    "published": "Jun 22",
    "pdf_url": "https://arxiv.org/pdf/2506.17871v2",
    "arxiv_url": "http://arxiv.org/abs/2506.17871v2",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Resource Rational Contractualism Should Guide AI Alignment",
    "authors": [
      "Sydney Levine",
      "Matija Franklin",
      "Tan Zhi-Xuan",
      "Secil Yanik Guyot",
      "Lionel Wong",
      "Daniel Kilov",
      "Yejin Choi",
      "Joshua B. Tenenbaum",
      "Noah Goodman",
      "Seth Lazar",
      "Iason Gabriel"
    ],
    "summary": "AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI. We therefore propose Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy. An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world.",
    "published": "Jun 20",
    "pdf_url": "https://arxiv.org/pdf/2506.17434v1",
    "arxiv_url": "http://arxiv.org/abs/2506.17434v1",
    "queried_author": "Noah Goodman",
    "matching_authors": [
      "Noah Goodman",
      "Yejin Choi"
    ]
  },
  {
    "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?",
    "authors": [
      "Adithya Bhaskar",
      "Alexander Wettig",
      "Tianyu Gao",
      "Yihe Dong",
      "Danqi Chen"
    ],
    "summary": "Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.",
    "published": "Jun 20",
    "pdf_url": "https://arxiv.org/pdf/2506.17121v1",
    "arxiv_url": "http://arxiv.org/abs/2506.17121v1",
    "queried_author": "Alexander Wettig",
    "matching_authors": [
      "Alexander Wettig",
      "Danqi Chen"
    ]
  },
  {
    "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
    "authors": [
      "Murtaza Nazir",
      "Matthew Finlayson",
      "John X. Morris",
      "Xiang Ren",
      "Swabha Swayamdipta"
    ],
    "summary": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a consid...",
    "published": "Jun 20",
    "pdf_url": "https://arxiv.org/pdf/2506.17090v3",
    "arxiv_url": "http://arxiv.org/abs/2506.17090v3",
    "queried_author": "John X. Morris",
    "matching_authors": [
      "John X. Morris"
    ]
  },
  {
    "title": "Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations",
    "authors": [
      "Ananth Agarwal",
      "Jasper Jian",
      "Christopher D. Manning",
      "Shikhar Murty"
    ],
    "summary": "Large Language Models (LLMs) exhibit a robust mastery of syntax when processing and generating text. While this suggests internalized understanding of hierarchical syntax and dependency relations, the precise mechanism by which they represent syntactic structure is an open area within interpretability research. Probing provides one way to identify the mechanism of syntax being linearly encoded in activations, however, no comprehensive study has yet established whether a model's probing accuracy reliably predicts its downstream syntactic performance. Adopting a \"mechanisms vs. outcomes\" framework, we evaluate 32 open-weight transformer models and find that syntactic features extracted via probing fail to predict outcomes of targeted syntax evaluations across English linguistic phenomena. Our results highlight a substantial disconnect between latent syntactic representations found via probing and observable syntactic behaviors in downstream tasks.",
    "published": "Jun 20",
    "pdf_url": "https://arxiv.org/pdf/2506.16678v2",
    "arxiv_url": "http://arxiv.org/abs/2506.16678v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning"
    ]
  },
  {
    "title": "PPMI: Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases",
    "authors": [
      "Yubeen Bae",
      "Minchan Kim",
      "Jaejin Lee",
      "Sangbum Kim",
      "Jaehyung Kim",
      "Yejin Choi",
      "Niloofar Mireshghallah"
    ],
    "summary": "Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.",
    "published": "Jun 19",
    "pdf_url": "https://arxiv.org/pdf/2506.17336v3",
    "arxiv_url": "http://arxiv.org/abs/2506.17336v3",
    "queried_author": "Niloofar Mireshghallah",
    "matching_authors": [
      "Niloofar Mireshghallah",
      "Yejin Choi"
    ]
  },
  {
    "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues",
    "authors": [
      "Myke C. Cohen",
      "Zhe Su",
      "Hsien-Te Kao",
      "Daniel Nguyen",
      "Spencer Lynch",
      "Maarten Sap",
      "Svitlana Volkova"
    ],
    "summary": "This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advanc...",
    "published": "Jun 19",
    "pdf_url": "https://arxiv.org/pdf/2506.15928v3",
    "arxiv_url": "http://arxiv.org/abs/2506.15928v3",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "Approximating Language Model Training Data from Weights",
    "authors": [
      "John X. Morris",
      "Junjie Oscar Yin",
      "Woojeong Kim",
      "Vitaly Shmatikov",
      "Alexander M. Rush"
    ],
    "summary": "Modern language models often have open weights but closed training data. We formalize the problem of data approximation from model weights and propose several baselines and metrics. We develop a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering useful data given only weights of the original and finetuned models. Even when none of the true training data is known, our method is able to locate a small subset of public Web documents can be used to train a model to close to the original model performance given models trained for both classification and supervised-finetuning. On the AG News classification task, our method improves performance from 65% (using randomly selected data) to 80%, approaching the expert benchmark of 88%. When applied to a model trained with SFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3, compared to an expert LLAMA model's perplexity of 2.0.",
    "published": "Jun 18",
    "pdf_url": "https://arxiv.org/pdf/2506.15553v1",
    "arxiv_url": "http://arxiv.org/abs/2506.15553v1",
    "queried_author": "Alexander M Rush",
    "matching_authors": [
      "Alexander M Rush",
      "Alexander M. Rush",
      "John X. Morris",
      "Junjie Oscar Yin"
    ]
  },
  {
    "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification",
    "authors": [
      "Negar Foroutan",
      "Jakhongir Saydaliev",
      "Ye Eun Kim",
      "Antoine Bosselut"
    ],
    "summary": "Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.",
    "published": "Jun 18",
    "pdf_url": "https://arxiv.org/pdf/2506.15304v1",
    "arxiv_url": "http://arxiv.org/abs/2506.15304v1",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework",
    "authors": [
      "Shayan Talaei",
      "Meijin Li",
      "Kanu Grover",
      "James Kent Hippler",
      "Diyi Yang",
      "Amin Saberi"
    ],
    "summary": "Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.",
    "published": "Jun 17",
    "pdf_url": "https://arxiv.org/pdf/2506.14159v2",
    "arxiv_url": "http://arxiv.org/abs/2506.14159v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization",
    "authors": [
      "Chengyu Huang",
      "Tanya Goyal"
    ],
    "summary": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.",
    "published": "Jun 17",
    "pdf_url": "https://arxiv.org/pdf/2506.14157v1",
    "arxiv_url": "http://arxiv.org/abs/2506.14157v1",
    "queried_author": "Tanya Goyal",
    "matching_authors": [
      "Tanya Goyal"
    ]
  },
  {
    "title": "Sampling from Your Language Model One Byte at a Time",
    "authors": [
      "Jonathan Hayase",
      "Alisa Liu",
      "Noah A. Smith",
      "Sewoong Oh"
    ],
    "summary": "Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations, an issue known as the Prompt Boundary Problem (PBP). For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. While this heuristic is effective in English, the underlying PBP continues to affect languages such as Chinese as well as code generation, where tokens often do not line up with word and syntactic boundaries. In this work, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM. Our method efficiently solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time or transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals. Code is available at https://github.com/SewoongLab/byte-sampler .",
    "published": "Jun 17",
    "pdf_url": "https://arxiv.org/pdf/2506.14123v2",
    "arxiv_url": "http://arxiv.org/abs/2506.14123v2",
    "queried_author": "Noah A. Smith",
    "matching_authors": [
      "Noah A. Smith"
    ]
  },
  {
    "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers",
    "authors": [
      "Wooseok Seo",
      "Seungju Han",
      "Jaehun Jung",
      "Benjamin Newman",
      "Seungwon Lim",
      "Seungbeen Lee",
      "Ximing Lu",
      "Yejin Choi",
      "Youngjae Yu"
    ],
    "summary": "Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers.",
    "published": "Jun 16",
    "pdf_url": "https://arxiv.org/pdf/2506.13342v2",
    "arxiv_url": "http://arxiv.org/abs/2506.13342v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization",
    "authors": [
      "Badr AlKhamissi",
      "C. Nicol\u00f2 De Sabbata",
      "Greta Tuckute",
      "Zeming Chen",
      "Martin Schrimpf",
      "Antoine Bosselut"
    ],
    "summary": "Human cognitive behavior arises from the interaction of specialized brain networks dedicated to distinct functions, such as language, logic, and social reasoning. Inspired by this organization, we propose Mixture of Cognitive Reasoners (MiCRo): a modular, transformer-based architecture post-trained with a curriculum that induces functional specialization across experts. Concretely, we partition the layers of a pretrained language model into four expert modules aligned with well-studied cognitive networks in the human brain. MiCRo offers three key advantages over standard language models. (1) The specialized experts are interpretable and causally meaningful -- ablating a module causes substantial drops on benchmarks requiring its specialized domain. (2) MiCRo's behavior can be dynamically steered at inference time by routing tokens to particular experts (e.g., favoring social over logical reasoning), enabling fine-grained control over outputs. (3) MiCRo outperforms or matches comparable baselines on both machine-learning reasoning benchmarks (e.g., GSM8K, BBH) and alignment to human behavior (CogBench), while maintaining interpretability. Taken together, cognitively grounded functional specialization yields models that are both more human-like and more human-interpretable.",
    "published": "Jun 16",
    "pdf_url": "https://arxiv.org/pdf/2506.13331v2",
    "arxiv_url": "http://arxiv.org/abs/2506.13331v2",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions",
    "authors": [
      "Devin Kwok",
      "G\u00fcl Sena Alt\u0131nta\u015f",
      "Colin Raffel",
      "David Rolnick"
    ],
    "summary": "Neural network training is inherently sensitive to initialization and the randomness induced by stochastic gradient descent. However, it is unclear to what extent such effects lead to meaningfully different networks, either in terms of the models' weights or the underlying functions that were learned. In this work, we show that during the initial \"chaotic\" phase of training, even extremely small perturbations reliably causes otherwise identical training trajectories to diverge-an effect that diminishes rapidly over training time. We quantify this divergence through (i) $L^2$ distance between parameters, (ii) the loss barrier when interpolating between networks, (iii) $L^2$ and barrier between parameters after permutation alignment, and (iv) representational similarity between intermediate activations; revealing how perturbations across different hyperparameter or fine-tuning settings drive training trajectories toward distinct loss minima. Our findings provide insights into neural network training stability, with practical implications for fine-tuning, model merging, and diversity of model ensembles.",
    "published": "Jun 16",
    "pdf_url": "https://arxiv.org/pdf/2506.13234v1",
    "arxiv_url": "http://arxiv.org/abs/2506.13234v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel"
    ]
  },
  {
    "title": "ZINA: Multimodal Fine-grained Hallucination Detection and Editing",
    "authors": [
      "Yuiga Wada",
      "Kazuki Matsuda",
      "Komei Sugiura",
      "Graham Neubig"
    ],
    "summary": "Multimodal Large Language Models (MLLMs) often generate hallucinations, where the output deviates from the visual content. Given that these hallucinations can take diverse forms, detecting hallucinations at a fine-grained level is essential for comprehensive evaluation and analysis. To this end, we propose a novel task of multimodal fine-grained hallucination detection and editing for MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated spans at a fine-grained level, classifies their error types into six categories, and suggests appropriate refinements. To train and evaluate models for this task, we constructed VisionHall, a dataset comprising 6.9k outputs from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic samples generated using a graph-based method that captures dependencies among error types. We demonstrated that ZINA outperformed existing methods, including GPT-4o and LLama-3.2, in both detection and editing tasks.",
    "published": "Jun 16",
    "pdf_url": "https://arxiv.org/pdf/2506.13130v1",
    "arxiv_url": "http://arxiv.org/abs/2506.13130v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics",
    "authors": [
      "Jiarui Liu",
      "Yueqi Song",
      "Yunze Xiao",
      "Mingqian Zheng",
      "Lindia Tjuatja",
      "Jana Schaich Borg",
      "Mona Diab",
      "Maarten Sap"
    ],
    "summary": "As large language models (LLMs) are increasingly used in morally sensitive domains, it is crucial to understand how persona traits affect their moral reasoning and persuasive behavior. We present the first large-scale study of multi-dimensional persona effects in AI-AI debates over real-world moral dilemmas. Using a 6-dimensional persona space (age, gender, country, class, ideology, and personality), we simulate structured debates between AI agents over 131 relationship-based cases. Our results show that personas affect initial moral stances and debate outcomes, with political ideology and personality traits exerting the strongest influence. Persuasive success varies across traits, with liberal and open personalities reaching higher consensus and win rates. While logit-based confidence grows during debates, emotional and credibility-based appeals diminish, indicating more tempered argumentation over time. These trends mirror findings from psychology and cultural studies, reinforcing the need for persona-aware evaluation frameworks for AI moral reasoning.",
    "published": "Jun 14",
    "pdf_url": "https://arxiv.org/pdf/2506.12657v2",
    "arxiv_url": "http://arxiv.org/abs/2506.12657v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  },
  {
    "title": "OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics",
    "authors": [
      "Vineeth Dorna",
      "Anmol Mekala",
      "Wenlong Zhao",
      "Andrew McCallum",
      "Zachary C. Lipton",
      "J. Zico Kolter",
      "Pratyush Maini"
    ],
    "summary": "Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 13 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research.",
    "published": "Jun 14",
    "pdf_url": "https://arxiv.org/pdf/2506.12618v2",
    "arxiv_url": "http://arxiv.org/abs/2506.12618v2",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being",
    "authors": [
      "Yutong Zhang",
      "Dora Zhao",
      "Jeffrey T. Hancock",
      "Robert Kraut",
      "Diyi Yang"
    ],
    "summary": "As large language models (LLMs)-enhanced chatbots grow increasingly expressive and socially responsive, many users are beginning to form companionship-like bonds with them, particularly with simulated AI partners designed to mimic emotionally attuned interlocutors. These emerging AI companions raise critical questions: Can such systems fulfill social needs typically met by human relationships? How do they shape psychological well-being? And what new risks arise as users develop emotional ties to non-human agents? This study investigates how people interact with AI companions, especially simulated partners on CharacterAI, and how this use is associated with users' psychological well-being. We analyzed survey data from 1,131 users and 4,363 chat sessions (413,509 messages) donated by 244 participants, focusing on three dimensions of use: nature of the interaction, interaction intensity, and self-disclosure. By triangulating self-reports primary motivation, open-ended relationship descriptions, and annotated chat transcripts, we identify patterns in how users engage with AI companions and its associations with well-being. Findings suggest that people with smaller social networks are more likely to turn to chatbots for companionship, but that companionship-oriented chatbot usage is consistently associated with lower well-being, particularly when people use the chatbots more intensively, engage in higher levels of self-disclosure, and lack strong human social support. Even though some people turn to chatbots to fulfill social needs, these uses of chatbots do not fully substitute...",
    "published": "Jun 14",
    "pdf_url": "https://arxiv.org/pdf/2506.12605v4",
    "arxiv_url": "http://arxiv.org/abs/2506.12605v4",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index",
    "authors": [
      "Hao Xu",
      "Jiacheng Liu",
      "Yejin Choi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi"
    ],
    "summary": "Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora - counting string appearances and retrieving the enclosing documents - yet the high storage overhead hinders their application on Internet-scale data. We present infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, our system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18$\\times$) and memory use during both indexing (3.2$\\times$ reduction) and querying (down to a negligible amount). We index 83TB of Internet text in 99 days with a single CPU node with 128 vCPUs (or 19 hours if using 137 such nodes). We show one important use case of infini-gram mini in a large-scale analysis of benchmark contamination. We find several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 74.2% in GSM8K), which could lead to overestimating the capabilities of language models if trained on such data. We host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. We also release a web interface and an API endpoint to serve general search queries on infini-gram mini indexes.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.12229v5",
    "arxiv_url": "http://arxiv.org/abs/2506.12229v5",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Noah A. Smith",
      "Yejin Choi"
    ]
  },
  {
    "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
    "authors": [
      "Been Kim",
      "John Hewitt",
      "Neel Nanda",
      "Noah Fiedel",
      "Oyvind Tafjord"
    ],
    "summary": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.12152v1",
    "arxiv_url": "http://arxiv.org/abs/2506.12152v1",
    "queried_author": "John Hewitt",
    "matching_authors": [
      "John Hewitt",
      "Neel Nanda"
    ]
  },
  {
    "title": "How Visual Representations Map to Language Feature Space in Multimodal LLMs",
    "authors": [
      "Constantin Venhoff",
      "Ashkan Khakzar",
      "Sonia Joseph",
      "Philip Torr",
      "Neel Nanda"
    ],
    "summary": "Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly understood. Following the LiMBeR framework, we deliberately maintain a frozen large language model (LLM) and a frozen vision transformer (ViT), connected solely by training a linear adapter during visual instruction tuning. By keeping the language model frozen, we ensure it maintains its original language representations without adaptation to visual data. Consequently, the linear adapter must map visual features directly into the LLM's existing representational space rather than allowing the language model to develop specialized visual understanding through fine-tuning. Our experimental design uniquely enables the use of pre-trained sparse autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned with the unchanged language model and serve as a snapshot of the learned language feature-representations. Through systematic analysis of SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we reveal the layer-wise progression through which visual representations gradually align with language feature representations, converging in middle-to-later layers. This suggests a fundamental misalignment between ViT outputs and early LLM layers, raising important questions about whether current adapter-based architectures optimally facilitate cross-modal representation learning.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.11976v2",
    "arxiv_url": "http://arxiv.org/abs/2506.11976v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?",
    "authors": [
      "Zihan Zheng",
      "Zerui Cheng",
      "Zeyu Shen",
      "Shang Zhou",
      "Kaiyuan Liu",
      "Hansen He",
      "Dongruixuan Li",
      "Stanley Wei",
      "Hangyi Hao",
      "Jianzhu Yao",
      "Peiyao Sheng",
      "Zixuan Wang",
      "Wenhao Chai",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Sanjeev Arora",
      "Pramod Viswanath",
      "Jingbo Shang",
      "Saining Xie"
    ],
    "summary": "Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.11928v1",
    "arxiv_url": "http://arxiv.org/abs/2506.11928v1",
    "queried_author": "Peter Henderson",
    "matching_authors": [
      "Peter Henderson"
    ]
  },
  {
    "title": "Convergent Linear Representations of Emergent Misalignment",
    "authors": [
      "Anna Soligo",
      "Edward Turner",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.11618v2",
    "arxiv_url": "http://arxiv.org/abs/2506.11618v2",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "Model Organisms for Emergent Misalignment",
    "authors": [
      "Edward Turner",
      "Anna Soligo",
      "Mia Taylor",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.11613v1",
    "arxiv_url": "http://arxiv.org/abs/2506.11613v1",
    "queried_author": "Neel Nanda",
    "matching_authors": [
      "Neel Nanda"
    ]
  },
  {
    "title": "AbsenceBench: Language Models Can't Tell What's Missing",
    "authors": [
      "Harvey Yiyun Fu",
      "Aryan Shrivastava",
      "Jared Moore",
      "Peter West",
      "Chenhao Tan",
      "Ari Holtzman"
    ],
    "summary": "Large language models (LLMs) are increasingly capable of processing long inputs and locating specific information within them, as evidenced by their performance on the Needle in a Haystack (NIAH) test. However, while models excel at recalling surprising information, they still struggle to identify clearly omitted information. We introduce AbsenceBench to assesses LLMs' capacity to detect missing information across three domains: numerical sequences, poetry, and GitHub pull requests. AbsenceBench asks models to identify which pieces of a document were deliberately removed, given access to both the original and edited contexts. Despite the apparent straightforwardness of these tasks, our experiments reveal that even state-of-the-art models like Claude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context length of 5K tokens. Our analysis suggests this poor performance stems from a fundamental limitation: Transformer attention mechanisms cannot easily attend to \"gaps\" in documents since these absences don't correspond to any specific keys that can be attended to. Overall, our results and analysis provide a case study of the close proximity of tasks where models are already superhuman (NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).",
    "published": "Jun 13",
    "pdf_url": "https://arxiv.org/pdf/2506.11440v1",
    "arxiv_url": "http://arxiv.org/abs/2506.11440v1",
    "queried_author": "Ari Holtzman",
    "matching_authors": [
      "Ari Holtzman"
    ]
  },
  {
    "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
    "authors": [
      "Rulin Shao",
      "Shuyue Stella Li",
      "Rui Xin",
      "Scott Geng",
      "Yiping Wang",
      "Sewoong Oh",
      "Simon Shaolei Du",
      "Nathan Lambert",
      "Sewon Min",
      "Ranjay Krishna",
      "Yulia Tsvetkov",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh",
      "Luke Zettlemoyer"
    ],
    "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain language models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR training with GRPO improves MATH-500 performance for Qwen2.5-Math-7B by 21.4 percentage points using randomly assigned rewards, nearly matching the 29.1-point gain from ground-truth rewards. To explain this counterintuitive observation, we show that GRPO exhibits a clipping bias from the clip term, which can amplify high-prior behaviors learned during pretraining even without informative rewards. As a case study, we identify one such behavior in Qwen2.5-Math models, which we call code reasoning -- reasoning in code without actual code execution; code-reasoning frequency increases from 65 percent to over 90 percent with spurious rewards. However, the presence of such amplifiable behaviors is highly model-dependent. In practice, spurious rewards that are effective for Qwen models often fail to produce gains for other model families, such as Llama3 or OLMo2. Our results highlight the importance of validating RL methods across diverse models rather than relying on a single de facto choice: large gains can arise on Qwen models even from random rewards that do not reflect genuine capability improvements.",
    "published": "Jun 12",
    "pdf_url": "https://arxiv.org/pdf/2506.10947v2",
    "arxiv_url": "http://arxiv.org/abs/2506.10947v2",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Self-Adapting Language Models",
    "authors": [
      "Adam Zweiger",
      "Jyothish Pari",
      "Han Guo",
      "Ekin Aky\u00fcrek",
      "Yoon Kim",
      "Pulkit Agrawal"
    ],
    "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.",
    "published": "Jun 12",
    "pdf_url": "https://arxiv.org/pdf/2506.10943v2",
    "arxiv_url": "http://arxiv.org/abs/2506.10943v2",
    "queried_author": "Yoon Kim",
    "matching_authors": [
      "Yoon Kim"
    ]
  },
  {
    "title": "Sequential-Parallel Duality in Prefix Scannable Models",
    "authors": [
      "Morris Yau",
      "Sharut Gupta",
      "Valerie Engelmayer",
      "Kazuki Irie",
      "Stefanie Jegelka",
      "Jacob Andreas"
    ],
    "summary": "Modern neural sequence models are designed to meet the dual mandate of parallelizable training and fast sequential inference. Recent developments have given rise to various models, such as Gated Linear Attention (GLA) and Mamba, that achieve such ``sequential-parallel duality.'' This raises a natural question: can we characterize the full class of neural sequence models that support near-constant-time parallel evaluation and linear-time, constant-space sequential inference? We begin by describing a broad class of such models -- state space models -- as those whose state updates can be computed using the classic parallel prefix scan algorithm with a custom associative aggregation operator. We then define a more general class, Prefix-Scannable Models (PSMs), by relaxing the state aggregation operator to allow arbitrary (potentially non-associative) functions such as softmax attention. This generalization unifies many existing architectures, including element-wise RNNs (e.g., Mamba) and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new models with softmax-like operators that achieve O(1) amortized compute per token and log(N) memory for sequence length N. We empirically evaluate such models on illustrative small-scale language modeling and canonical synthetic tasks, including state tracking and associative recall. Empirically, we find that PSMs retain the expressivity of transformer-based architectures while matching the inference efficiency of state space models -- in some cases exhibiting better length generalization than either.",
    "published": "Jun 12",
    "pdf_url": "https://arxiv.org/pdf/2506.10918v1",
    "arxiv_url": "http://arxiv.org/abs/2506.10918v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "When Large Language Models are Reliable for Judging Empathic Communication",
    "authors": [
      "Aakriti Kumar",
      "Nalin Poungpeth",
      "Diyi Yang",
      "Erina Farrell",
      "Bruce Lambert",
      "Matthew Groh"
    ],
    "summary": "Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? We investigate this question by comparing how experts, crowdworkers, and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing, and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess inter-rater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks' sub-components depending on their clarity, complexity, and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.",
    "published": "Jun 11",
    "pdf_url": "https://arxiv.org/pdf/2506.10150v2",
    "arxiv_url": "http://arxiv.org/abs/2506.10150v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "Unsupervised Elicitation of Language Models",
    "authors": [
      "Jiaxin Wen",
      "Zachary Ankner",
      "Arushi Somani",
      "Peter Hase",
      "Samuel Marks",
      "Jacob Goldman-Wetzler",
      "Linda Petrini",
      "Henry Sleight",
      "Collin Burns",
      "He He",
      "Shi Feng",
      "Ethan Perez",
      "Jan Leike"
    ],
    "summary": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \\emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden labels and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 4 Sonnet-based assistant. The resulting assistant matches its counterpart trained on production-grade human labels on average, with higher scores on chat and safety yet lower scores on math and coding.",
    "published": "Jun 11",
    "pdf_url": "https://arxiv.org/pdf/2506.10139v2",
    "arxiv_url": "http://arxiv.org/abs/2506.10139v2",
    "queried_author": "Ethan Perez",
    "matching_authors": [
      "Ethan Perez"
    ]
  },
  {
    "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking",
    "authors": [
      "Wuwei Zhang",
      "Fangcong Yin",
      "Howard Yen",
      "Danqi Chen",
      "Xi Ye"
    ],
    "summary": "Recent work has identified retrieval heads, a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needlein-a-Haystack tasks. In this paper, we introduce QRHead (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHead by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QRRetriever, an efficient and effective retriever that uses the accumulated attention mass of QRHead as retrieval scores. We use QRRetriever for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRetriever as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the query-context attention scoring and task selection are crucial for identifying QRHead with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.",
    "published": "Jun 11",
    "pdf_url": "https://arxiv.org/pdf/2506.09944v2",
    "arxiv_url": "http://arxiv.org/abs/2506.09944v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Learning to Reason Across Parallel Samples for LLM Reasoning",
    "authors": [
      "Jianing Qi",
      "Xi Ye",
      "Hao Tang",
      "Zhigang Zhu",
      "Eunsol Choi"
    ],
    "summary": "Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on five reasoning datasets demonstrate both the efficacy and efficiency of SSA. Notably, SSA improves over naive majority voting by 8% pass@5 on MATH. Furthermore, our 3B SSA surpasses model-based re-ranking with a much larger 72B process reward model. Our analysis also shows promising generalization ability of SSA, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently.",
    "published": "Jun 10",
    "pdf_url": "https://arxiv.org/pdf/2506.09014v2",
    "arxiv_url": "http://arxiv.org/abs/2506.09014v2",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi"
    ]
  },
  {
    "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation",
    "authors": [
      "Arnav Yayavaram",
      "Siddharth Yayavaram",
      "Simran Khanuja",
      "Michael Saxon",
      "Graham Neubig"
    ],
    "summary": "As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, an evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 22% F1 points. Additionally, we construct two datasets for culturally universal concepts, one comprising T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.",
    "published": "Jun 10",
    "pdf_url": "https://arxiv.org/pdf/2506.09109v2",
    "arxiv_url": "http://arxiv.org/abs/2506.09109v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions",
    "authors": [
      "David Acuna",
      "Ximing Lu",
      "Jaehun Jung",
      "Hyunwoo Kim",
      "Amlan Kar",
      "Sanja Fidler",
      "Yejin Choi"
    ],
    "summary": "Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning -- akin to the success observed in language models -- via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces -- without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer pairs into the model's output stream. We show that framing reasoning as a search process -- where subquestions act as latent decisions within a broader inference trajectory -- helps the model \"connect the dots\" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts.",
    "published": "Jun 10",
    "pdf_url": "https://arxiv.org/pdf/2506.08927v1",
    "arxiv_url": "http://arxiv.org/abs/2506.08927v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs",
    "authors": [
      "Zeyu Leo Liu",
      "Greg Durrett",
      "Eunsol Choi"
    ],
    "summary": "Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.",
    "published": "Jun 10",
    "pdf_url": "https://arxiv.org/pdf/2506.08920v1",
    "arxiv_url": "http://arxiv.org/abs/2506.08920v1",
    "queried_author": "Eunsol Choi",
    "matching_authors": [
      "Eunsol Choi",
      "Greg Durrett"
    ]
  },
  {
    "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?",
    "authors": [
      "Zhanke Zhou",
      "Xiao Feng",
      "Zhaocheng Zhu",
      "Jiangchao Yao",
      "Sanmi Koyejo",
      "Bo Han"
    ],
    "summary": "While existing benchmarks probe the reasoning abilities of large language models (LLMs) across diverse domains, they predominantly assess passive reasoning, providing models with all the information needed to reach a solution. By contrast, active reasoning-where an LLM must interact with external systems to acquire missing evidence or data-has received little systematic attention. To address this shortfall, we present AR-Bench, a novel benchmark designed explicitly to evaluate an LLM's active reasoning skills. AR-Bench comprises three task families-detective cases, situation puzzles, and guessing numbers-that together simulate real-world, agentic scenarios and measure performance across commonsense, logical, and symbolic reasoning challenges. Empirical evaluation on AR-Bench demonstrates that contemporary LLMs exhibit pronounced difficulties with active reasoning: they frequently fail to acquire or leverage the information needed to solve tasks. This gap highlights a stark divergence between their passive and active reasoning abilities. Moreover, ablation studies indicate that even advanced strategies, such as tree-based searching or post-training approaches, yield only modest gains and fall short of the levels required for real-world deployment. Collectively, these findings highlight the critical need to advance methodology for active reasoning, e.g., incorporating interactive learning, real-time feedback loops, and environment-aware objectives for training. The benchmark is publicly available at: https://github.com/tmlr-group/AR-Bench.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.08295v1",
    "arxiv_url": "http://arxiv.org/abs/2506.08295v1",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "$\u03c4^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment",
    "authors": [
      "Victor Barres",
      "Honghua Dong",
      "Soham Ray",
      "Xujie Si",
      "Karthik Narasimhan"
    ],
    "summary": "Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\u03c4^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.\n  In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\u03c4^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07982v1",
    "arxiv_url": "http://arxiv.org/abs/2506.07982v1",
    "queried_author": "Karthik Narasimhan",
    "matching_authors": [
      "Karthik Narasimhan"
    ]
  },
  {
    "title": "Cost-Optimal Active AI Model Evaluation",
    "authors": [
      "Anastasios N. Angelopoulos",
      "Jacob Eisenstein",
      "Jonathan Berant",
      "Alekh Agarwal",
      "Adam Fisch"
    ],
    "summary": "The development lifecycle of generative AI systems requires continual evaluation, data acquisition, and annotation, which is costly in both resources and time. In practice, rapid iteration often makes it necessary to rely on synthetic annotation data because of the low cost, despite the potential for substantial bias. In this paper, we develop novel, cost-aware methods for actively balancing the use of a cheap, but often inaccurate, weak rater -- such as a model-based autorater that is designed to automatically assess the quality of generated content -- with a more expensive, but also more accurate, strong rater alternative such as a human. More specifically, the goal of our approach is to produce a low variance, unbiased estimate of the mean of the target \"strong\" rating, subject to some total annotation budget. Building on recent work in active and prediction-powered statistical inference, we derive a family of cost-optimal policies for allocating a given annotation budget between weak and strong raters so as to maximize statistical efficiency. Using synthetic and real-world data, we empirically characterize the conditions under which these policies yield improvements over prior methods. We find that, especially in tasks where there is high variability in the difficulty of examples, our policies can achieve the same estimation precision at a far lower total annotation budget than standard evaluation methods.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07949v1",
    "arxiv_url": "http://arxiv.org/abs/2506.07949v1",
    "queried_author": "Jacob Eisenstein",
    "matching_authors": [
      "Jacob Eisenstein"
    ]
  },
  {
    "title": "Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation",
    "authors": [
      "Xintong Duan",
      "Yutong He",
      "Fahim Tajwar",
      "Ruslan Salakhutdinov",
      "J. Zico Kolter",
      "Jeff Schneider"
    ],
    "summary": "Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cloning or rely on complex concurrent training of multiple networks under the actor-critic framework. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method achieves single-step sampling while generating higher-reward action trajectories through decoupled training and noise-free reward signals. Empirical evaluations on the Gym MuJoCo, FrankaKitchen, and long horizon planning benchmarks demonstrate that our approach can achieve a 9.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07822v3",
    "arxiv_url": "http://arxiv.org/abs/2506.07822v3",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
    "authors": [
      "Silin Gao",
      "Antoine Bosselut",
      "Samy Bengio",
      "Emmanuel Abbe"
    ],
    "summary": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07751v4",
    "arxiv_url": "http://arxiv.org/abs/2506.07751v4",
    "queried_author": "Antoine Bosselut",
    "matching_authors": [
      "Antoine Bosselut"
    ]
  },
  {
    "title": "Premise Selection for a Lean Hammer",
    "authors": [
      "Thomas Zhu",
      "Joshua Clune",
      "Jeremy Avigad",
      "Albert Qiaochu Jiang",
      "Sean Welleck"
    ],
    "summary": "Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. A hammer is a tool that integrates premise selection, translation to external automatic theorem provers, and proof reconstruction into one overarching tool to automate tedious reasoning steps. We present LeanPremise, a novel neural premise selection system, and we combine it with existing translation and proof reconstruction components to create LeanHammer, the first end-to-end domain general hammer for the Lean proof assistant. Unlike existing Lean premise selectors, LeanPremise is specifically trained for use with a hammer in dependent type theory. It also dynamically adapts to user-specific contexts, enabling it to effectively recommend premises from libraries outside LeanPremise's training data as well as lemmas defined by the user locally. With comprehensive evaluations, we show that LeanPremise enables LeanHammer to solve 21% more goals than existing premise selectors and generalizes well to diverse domains. Our work helps bridge the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07477v2",
    "arxiv_url": "http://arxiv.org/abs/2506.07477v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models",
    "authors": [
      "Mickel Liu",
      "Liwei Jiang",
      "Yancheng Liang",
      "Simon Shaolei Du",
      "Yejin Choi",
      "Tim Althoff",
      "Natasha Jaques"
    ],
    "summary": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-impro...",
    "published": "Jun 09",
    "pdf_url": "https://arxiv.org/pdf/2506.07468v3",
    "arxiv_url": "http://arxiv.org/abs/2506.07468v3",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "Certified Unlearning for Neural Networks",
    "authors": [
      "Anastasia Koloskova",
      "Youssef Allouah",
      "Animesh Jha",
      "Rachid Guerraoui",
      "Sanmi Koyejo"
    ],
    "summary": "We address the problem of machine unlearning, where the goal is to remove the influence of specific training data from a model upon request, motivated by privacy concerns and regulatory requirements such as the \"right to be forgotten.\" Unfortunately, existing methods rely on restrictive assumptions or lack formal guarantees. To this end, we propose a novel method for certified machine unlearning, leveraging the connection between unlearning and privacy amplification by stochastic post-processing. Our method uses noisy fine-tuning on the retain data, i.e., data that does not need to be removed, to ensure provable unlearning guarantees. This approach requires no assumptions about the underlying loss function, making it broadly applicable across diverse settings. We analyze the theoretical trade-offs in efficiency and accuracy and demonstrate empirically that our method not only achieves formal unlearning guarantees but also performs effectively in practice, outperforming existing baselines. Our code is available at https://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025",
    "published": "Jun 08",
    "pdf_url": "https://arxiv.org/pdf/2506.06985v2",
    "arxiv_url": "http://arxiv.org/abs/2506.06985v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "Causal Graph based Event Reasoning using Semantic Relation Experts",
    "authors": [
      "Mahnaz Koupaee",
      "Xueying Bai",
      "Mudan Chen",
      "Greg Durrett",
      "Nathanael Chambers",
      "Niranjan Balasubramanian"
    ],
    "summary": "Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.",
    "published": "Jun 07",
    "pdf_url": "https://arxiv.org/pdf/2506.06910v1",
    "arxiv_url": "http://arxiv.org/abs/2506.06910v1",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Contextual Experience Replay for Self-Improvement of Language Agents",
    "authors": [
      "Yitao Liu",
      "Chenglei Si",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ],
    "summary": "Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",
    "published": "Jun 07",
    "pdf_url": "https://arxiv.org/pdf/2506.06698v1",
    "arxiv_url": "http://arxiv.org/abs/2506.06698v1",
    "queried_author": "Karthik Narasimhan",
    "matching_authors": [
      "Karthik Narasimhan"
    ]
  },
  {
    "title": "Transferring Linear Features Across Language Models With Model Stitching",
    "authors": [
      "Alan Chen",
      "Jack Merullo",
      "Alessandro Stolfo",
      "Ellie Pavlick"
    ],
    "summary": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. In particular, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.",
    "published": "Jun 07",
    "pdf_url": "https://arxiv.org/pdf/2506.06609v3",
    "arxiv_url": "http://arxiv.org/abs/2506.06609v3",
    "queried_author": "Jack Merullo",
    "matching_authors": [
      "Jack Merullo"
    ]
  },
  {
    "title": "Precise Information Control in Long-Form Text Generation",
    "authors": [
      "Jacqueline He",
      "Howard Yen",
      "Margaret Li",
      "Shuyue Stella Li",
      "Zhiyuan Zeng",
      "Weijia Shi",
      "Yulia Tsvetkov",
      "Danqi Chen",
      "Pang Wei Koh",
      "Luke Zettlemoyer"
    ],
    "summary": "A central challenge in language models (LMs) is faithfulness hallucination: the generation of information unsubstantiated by input context. To study this problem, we propose Precise Information Control (PIC), a new task formulation that requires models to generate long-form outputs grounded in a provided set of short self-contained statements, without adding any unsupported ones. PIC includes a full setting that tests a model's ability to include exactly all input claims, and a partial setting that requires the model to selectively incorporate only relevant claims. We present PIC-Bench, a benchmark of eight long-form generation tasks (e.g., summarization, biography generation) adapted to the PIC setting, where LMs are supplied with well-formed, verifiable input claims. Our evaluation of a range of open and proprietary LMs on PIC-Bench reveals that, surprisingly, state-of-the-art LMs still hallucinate against user-provided input in over 70% of generations. To alleviate this lack of faithfulness, we introduce a post-training framework that uses a weakly supervised preference data construction method to train an 8B PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full PIC setting. When integrated into end-to-end factual generation pipelines, PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and factual precision by 30.5% on a birthplace fact-checking task, underscoring the potential of precisely grounded generation.",
    "published": "Jun 06",
    "pdf_url": "https://arxiv.org/pdf/2506.06589v2",
    "arxiv_url": "http://arxiv.org/abs/2506.06589v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen",
      "Luke Zettlemoyer",
      "Pang Wei Koh"
    ]
  },
  {
    "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce",
    "authors": [
      "Yijia Shao",
      "Humishka Zope",
      "Yucheng Jiang",
      "Jiaxin Pei",
      "David Nguyen",
      "Erik Brynjolfsson",
      "Diyi Yang"
    ],
    "summary": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation \"Green Light\" Zone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings unde...",
    "published": "Jun 06",
    "pdf_url": "https://arxiv.org/pdf/2506.06576v3",
    "arxiv_url": "http://arxiv.org/abs/2506.06576v3",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang"
    ]
  },
  {
    "title": "The Optimization Paradox in Clinical AI Multi-Agent Systems",
    "authors": [
      "Suhana Bedi",
      "Iddah Mlauzi",
      "Daniel Shin",
      "Sanmi Koyejo",
      "Nigam H. Shah"
    ],
    "summary": "Multi-agent artificial intelligence systems are increasingly deployed in clinical settings, yet the relationship between component-level optimization and system-wide performance remains poorly understood. We evaluated this relationship using 2,400 real patient cases from the MIMIC-CDM dataset across four abdominal pathologies (appendicitis, pancreatitis, cholecystitis, diverticulitis), decomposing clinical diagnosis into information gathering, interpretation, and differential diagnosis. We evaluated single agent systems (one model performing all tasks) against multi-agent systems (specialized models for each task) using comprehensive metrics spanning diagnostic outcomes, process adherence, and cost efficiency. Our results reveal a paradox: while multi-agent systems generally outperformed single agents, the component-optimized or Best of Breed system with superior components and excellent process metrics (85.5% information accuracy) significantly underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent system). This finding underscores that successful integration of AI in healthcare requires not just component level optimization but also attention to information flow and compatibility between agents. Our findings highlight the need for end to end system validation rather than relying on component metrics alone.",
    "published": "Jun 06",
    "pdf_url": "https://arxiv.org/pdf/2506.06574v2",
    "arxiv_url": "http://arxiv.org/abs/2506.06574v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
    "authors": [
      "Zeqi Zhou",
      "Fang Wu",
      "Shayan Talaei",
      "Haokai Zhao",
      "Cheng Meixin",
      "Tinson Xu",
      "Amin Saberi",
      "Yejin Choi"
    ],
    "summary": "Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.",
    "published": "Jun 06",
    "pdf_url": "https://arxiv.org/pdf/2506.06020v1",
    "arxiv_url": "http://arxiv.org/abs/2506.06020v1",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs",
    "authors": [
      "Michael J Ryan",
      "Omar Shaikh",
      "Aditri Bhagirath",
      "Daniel Frees",
      "William Held",
      "Diyi Yang"
    ],
    "summary": "Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.05598v1",
    "arxiv_url": "http://arxiv.org/abs/2506.05598v1",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "William Held"
    ]
  },
  {
    "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
    "authors": [
      "Quan Shi",
      "Carlos E. Jimenez",
      "Shunyu Yao",
      "Nick Haber",
      "Diyi Yang",
      "Karthik Narasimhan"
    ],
    "summary": "Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.05579v2",
    "arxiv_url": "http://arxiv.org/abs/2506.05579v2",
    "queried_author": "Diyi Yang",
    "matching_authors": [
      "Diyi Yang",
      "Karthik Narasimhan"
    ]
  },
  {
    "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning",
    "authors": [
      "Violet Xiang",
      "Chase Blagden",
      "Rafael Rafailov",
      "Nathan Lile",
      "Sang Truong",
      "Chelsea Finn",
      "Nick Haber"
    ],
    "summary": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.05256v2",
    "arxiv_url": "http://arxiv.org/abs/2506.05256v2",
    "queried_author": "Rafael Rafailov",
    "matching_authors": [
      "Rafael Rafailov"
    ]
  },
  {
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text",
    "authors": [
      "Nikhil Kandpal",
      "Brian Lester",
      "Colin Raffel",
      "Sebastian Majstorovic",
      "Stella Biderman",
      "Baber Abbasi",
      "Luca Soldaini",
      "Enrico Shippole",
      "A. Feder Cooper",
      "Aviya Skowron",
      "John Kirchenbauer",
      "Shayne Longpre",
      "Lintang Sutawika",
      "Alon Albalak",
      "Zhenlin Xu",
      "Guilherme Penedo",
      "Loubna Ben Allal",
      "Elie Bakouch",
      "John David Pressman",
      "Honglu Fan",
      "Dashiell Stander",
      "Guangyu Song",
      "Aaron Gokaslan",
      "Tom Goldstein",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Tyler Murray"
    ],
    "summary": "Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.05209v1",
    "arxiv_url": "http://arxiv.org/abs/2506.05209v1",
    "queried_author": "Colin Raffel",
    "matching_authors": [
      "Colin Raffel",
      "Luca Soldaini"
    ]
  },
  {
    "title": "Log-Linear Attention",
    "authors": [
      "Han Guo",
      "Songlin Yang",
      "Tarushii Goel",
      "Eric P. Xing",
      "Tri Dao",
      "Yoon Kim"
    ],
    "summary": "The attention mechanism in Transformers is an important primitive for accurate and scalable sequence modeling. Its quadratic-compute and linear-memory complexity however remain significant bottlenecks. Linear attention and state-space models enable linear-time, constant-memory sequence modeling and can moreover be trained efficiently through matmul-rich parallelization across sequence length. However, at their core these models are still RNNs, and thus their use of a fixed-size hidden state to model the context is a fundamental limitation. This paper develops log-linear attention, an attention mechanism that balances linear attention's efficiency and the expressiveness of softmax attention. Log-linear attention replaces the fixed-size hidden state with a logarithmically growing set of hidden states. We show that with a particular growth function, log-linear attention admits a similarly matmul-rich parallel form whose compute cost is log-linear in sequence length. Log-linear attention is a general framework and can be applied on top of existing linear attention variants. As case studies, we instantiate log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet -- and find they perform well compared to their linear-time variants.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.04761v2",
    "arxiv_url": "http://arxiv.org/abs/2506.04761v2",
    "queried_author": "Tri Dao",
    "matching_authors": [
      "Tri Dao",
      "Yoon Kim"
    ]
  },
  {
    "title": "SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat",
    "authors": [
      "Yuru Jiang",
      "Wenxuan Ding",
      "Shangbin Feng",
      "Greg Durrett",
      "Yulia Tsvetkov"
    ],
    "summary": "We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a \"sparta tribe\" to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0% average improvement. Further analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.04721v3",
    "arxiv_url": "http://arxiv.org/abs/2506.04721v3",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett"
    ]
  },
  {
    "title": "Line of Sight: On Linear Representations in VLLMs",
    "authors": [
      "Achyuta Rajaram",
      "Sarah Schwettmann",
      "Jacob Andreas",
      "Arthur Conmy"
    ],
    "summary": "Language models can be equipped with multimodal capabilities by fine-tuning on embeddings of visual inputs. But how do such multimodal models represent images in their hidden activations? We explore representations of image concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set of ImageNet classes represented via linearly decodable features in the residual stream. We show that the features are causal by performing targeted edits on the model output. In order to increase the diversity of the studied linear features, we train multimodal Sparse Autoencoders (SAEs), creating a highly interpretable dictionary of text and image features. We find that although model representations across modalities are quite disjoint, they become increasingly shared in deeper layers.",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.04706v1",
    "arxiv_url": "http://arxiv.org/abs/2506.04706v1",
    "queried_author": "Jacob Andreas",
    "matching_authors": [
      "Jacob Andreas"
    ]
  },
  {
    "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models",
    "authors": [
      "Thao Nguyen",
      "Yang Li",
      "Olga Golovneva",
      "Luke Zettlemoyer",
      "Sewoong Oh",
      "Ludwig Schmidt",
      "Xian Li"
    ],
    "summary": "Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the \"data wall\" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge ext...",
    "published": "Jun 05",
    "pdf_url": "https://arxiv.org/pdf/2506.04689v3",
    "arxiv_url": "http://arxiv.org/abs/2506.04689v3",
    "queried_author": "Ludwig Schmidt",
    "matching_authors": [
      "Ludwig Schmidt",
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation",
    "authors": [
      "Hermann Kumbong",
      "Xian Liu",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Xihui Liu",
      "Ziwei Liu",
      "Daniel Y. Fu",
      "Christopher R\u00e9",
      "David W. Romero"
    ],
    "summary": "Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed and quality gap between autoregressive image models and diffusion models. VAR reformulates autoregressive modeling by decomposing an image into successive resolution scales. During inference, an image is generated by predicting all the tokens in the next (higher-resolution) scale, conditioned on all tokens in all previous (lower-resolution) scales. However, this formulation suffers from reduced image quality due to the parallel generation of all tokens in a resolution scale; has sequence lengths scaling superlinearly in image resolution; and requires retraining to change the sampling schedule.\n  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image generation algorithm that alleviates these issues using next-scale prediction and masked prediction to generate high-quality images with fast sampling. HMAR reformulates next-scale prediction as a Markovian process, wherein the prediction of each resolution scale is conditioned only on tokens in its immediate predecessor instead of the tokens in all predecessor resolutions. When predicting a resolution scale, HMAR uses a controllable multi-step masked generation procedure to generate a subset of the tokens in each step. On ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform parameter-matched VAR, diffusion, and autoregressive baselines. We develop efficient IO-aware block-sparse attention kernels that allow HMAR to achieve faster training and inference times over VAR by over 2.5x and 1.75x respectively, as we...",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.04421v1",
    "arxiv_url": "http://arxiv.org/abs/2506.04421v1",
    "queried_author": "Christopher R\u00e9",
    "matching_authors": [
      "Christopher R\u00e9"
    ]
  },
  {
    "title": "Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness",
    "authors": [
      "Stephen R. Pfohl",
      "Natalie Harris",
      "Chirag Nagpal",
      "David Madras",
      "Vishwali Mhasawade",
      "Olawale Salaudeen",
      "Awa Dieng",
      "Shannon Sequeira",
      "Santiago Arciniegas",
      "Lillian Sung",
      "Nnamdi Ezeanochie",
      "Heather Cole-Lewis",
      "Katherine Heller",
      "Sanmi Koyejo",
      "Alexander D'Amour"
    ],
    "summary": "Disaggregated evaluation across subgroups is critical for assessing the fairness of machine learning models, but its uncritical use can mislead practitioners. We show that equal performance across subgroups is an unreliable measure of fairness when data are representative of the relevant populations but reflective of real-world disparities. Furthermore, when data are not representative due to selection bias, both disaggregated evaluation and alternative approaches based on conditional independence testing may be invalid without explicit assumptions regarding the bias mechanism. We use causal graphical models to characterize fairness properties and metric stability across subgroups under different data generating processes. Our framework suggests complementing disaggregated evaluations with explicit causal assumptions and analysis to control for confounding and distribution shift, including conditional independence testing and weighted performance estimation. These findings have broad implications for how practitioners design and interpret model assessments given the ubiquity of disaggregated evaluation.",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.04193v2",
    "arxiv_url": "http://arxiv.org/abs/2506.04193v2",
    "queried_author": "Sanmi Koyejo",
    "matching_authors": [
      "Sanmi Koyejo"
    ]
  },
  {
    "title": "OpenThoughts: Data Recipes for Reasoning Models",
    "authors": [
      "Etash Guha",
      "Ryan Marten",
      "Sedrick Keh",
      "Negin Raoof",
      "Georgios Smyrnis",
      "Hritik Bansal",
      "Marianna Nezhurina",
      "Jean Mercat",
      "Trung Vu",
      "Zayne Sprague",
      "Ashima Suvarna",
      "Benjamin Feuer",
      "Liangyu Chen",
      "Zaid Khan",
      "Eric Frankel",
      "Sachin Grover",
      "Caroline Choi",
      "Niklas Muennighoff",
      "Shiye Su",
      "Wanjia Zhao",
      "John Yang",
      "Shreyas Pimpalgaonkar",
      "Kartik Sharma",
      "Charlie Cheng-Jie Ji",
      "Yichuan Deng",
      "Sarah Pratt",
      "Vivek Ramanujan",
      "Jon Saad-Falcon",
      "Jeffrey Li",
      "Achal Dave",
      "Alon Albalak",
      "Kushal Arora",
      "Blake Wulfe",
      "Chinmay Hegde",
      "Greg Durrett",
      "Sewoong Oh",
      "Mohit Bansal",
      "Saadia Gabriel",
      "Aditya Grover",
      "Kai-Wei Chang",
      "Vaishaal Shankar",
      "Aaron Gokaslan",
      "Mike A. Merrill",
      "Tatsunori Hashimoto",
      "Yejin Choi",
      "Jenia Jitsev",
      "Reinhard Heckel",
      "Maheswaran Sathiamoorthy",
      "Alexandros G. Dimakis",
      "Ludwig Schmidt"
    ],
    "summary": "Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThoughts3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond - improvements of 15.3, 17.2, and 20.5 percentage points compared to the DeepSeek-R1-Distill-Qwen-7B. All of our datasets and models are available on https://openthoughts.ai.",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.04178v2",
    "arxiv_url": "http://arxiv.org/abs/2506.04178v2",
    "queried_author": "Greg Durrett",
    "matching_authors": [
      "Greg Durrett",
      "Ludwig Schmidt",
      "Niklas Muennighoff",
      "Tatsunori Hashimoto",
      "Yejin Choi"
    ]
  },
  {
    "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning",
    "authors": [
      "Tim Franzmeyer",
      "Archie Sravankumar",
      "Lijuan Liu",
      "Yuning Mao",
      "Rui Hou",
      "Sinong Wang",
      "Jakob N. Foerster",
      "Luke Zettlemoyer",
      "Madian Khabsa"
    ],
    "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they can produce incorrect answers when they lack knowledge or capability -- a problem known as hallucination. We instead propose post-training an LLM to generate content only when confident in its correctness and to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned post-training data that encodes what the model can and cannot reliably generate. We generate this data by splitting responses of the pretrained LLM into factual fragments (atomic statements or reasoning steps), and use ground truth information to identify incorrect fragments. We achieve capability-aligned finetuning responses by either removing incorrect fragments or replacing them with \"Unsure from Here\" -- according to a tunable threshold that allows practitioners to trade off response completeness and mean correctness of the response's fragments. We finetune four open-source models for biography writing, mathematics, coding, and medicine with HALT for three different trade-off thresholds. HALT effectively trades off response completeness for correctness, increasing the mean correctness of response fragments by 15% on average, while resulting in a 4% improvement in the F1 score (mean of completeness and correctness of the response) compared to the relevant baselines. By tuning HALT for highest correctness, we train a single reliable Llama3-70B model with correctness increased from 51% to 87% across all four domains while maintaining 53% of the response completeness achieved with standard finetuning...",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.04051v2",
    "arxiv_url": "http://arxiv.org/abs/2506.04051v2",
    "queried_author": "Luke Zettlemoyer",
    "matching_authors": [
      "Luke Zettlemoyer"
    ]
  },
  {
    "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models",
    "authors": [
      "Alex Laitenberger",
      "Christopher D. Manning",
      "Nelson F. Liu"
    ],
    "summary": "With the rise of long-context language models (LMs) capable of processing tens of thousands of tokens in a single context window, do multi-stage retrieval-augmented generation (RAG) pipelines still offer measurable benefits over simpler, single-stage approaches? To assess this question, we conduct a controlled evaluation for QA tasks under systematically scaled token budgets, comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three baselines, including DOS RAG (Document's Original Structure RAG), a simple retrieve-then-read method that preserves original passage order. Despite its straightforward design, DOS RAG consistently matches or outperforms more intricate methods on multiple long-context QA benchmarks. We trace this strength to a combination of maintaining source fidelity and document structure, prioritizing recall within effective context windows, and favoring simplicity over added pipeline complexity. We recommend establishing DOS RAG as a simple yet strong baseline for future RAG evaluations, paired with state-of-the-art embedding and language models, and benchmarked under matched token budgets, to ensure that added pipeline complexity is justified by clear performance gains as models continue to improve.",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.03989v2",
    "arxiv_url": "http://arxiv.org/abs/2506.03989v2",
    "queried_author": "Christopher D Manning",
    "matching_authors": [
      "Christopher D Manning",
      "Nelson F. Liu"
    ]
  },
  {
    "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations",
    "authors": [
      "Vivian Nguyen",
      "Lillian Lee",
      "Cristian Danescu-Niculescu-Mizil"
    ],
    "summary": "During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen, in an online fashion. Our approach relies on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception -- counselors take significantly longer to respond during moments detected by our method -- and with the eventual conversational trajectory -- which is more likely to change course at these times. We then use our framework to explore the relation of the counselor's response during pivotal moments with the eventual outcome of the session.",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.03941v1",
    "arxiv_url": "http://arxiv.org/abs/2506.03941v1",
    "queried_author": "Lillian Lee",
    "matching_authors": [
      "Lillian Lee"
    ]
  },
  {
    "title": "Go-Browse: Training Web Agents with Structured Exploration",
    "authors": [
      "Apurva Gandhi",
      "Graham Neubig"
    ],
    "summary": "One of the fundamental problems in digital agents is their lack of understanding of their environment. For instance, a web browsing agent may get lost in unfamiliar websites, uncertain what pages must be visited to achieve its goals. To address this, we propose Go-Browse, a method for automatically collecting diverse and realistic web agent data at scale through structured exploration of web environments. Go-Browse achieves efficient exploration by framing data collection as a graph search, enabling reuse of information across exploration episodes. We instantiate our method on the WebArena benchmark, collecting a dataset of 10K successful task-solving trajectories and 40K interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on this dataset achieves a success rate of 21.7% on the WebArena benchmark, beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for sub-10B parameter models by 2.9%.",
    "published": "Jun 04",
    "pdf_url": "https://arxiv.org/pdf/2506.03533v1",
    "arxiv_url": "http://arxiv.org/abs/2506.03533v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "authors": [
      "Eliot Krzysztof Jones",
      "Alexander Robey",
      "Andy Zou",
      "Zachary Ravichandran",
      "George J. Pappas",
      "Hamed Hassani",
      "Matt Fredrikson",
      "J. Zico Kolter"
    ],
    "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .",
    "published": "Jun 03",
    "pdf_url": "https://arxiv.org/pdf/2506.03350v1",
    "arxiv_url": "http://arxiv.org/abs/2506.03350v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "HyperSteer: Activation Steering at Scale with Hypernetworks",
    "authors": [
      "Jiuding Sun",
      "Sidharth Baskaran",
      "Zhengxuan Wu",
      "Michael Sklar",
      "Christopher Potts",
      "Atticus Geiger"
    ],
    "summary": "Steering language models (LMs) by modifying internal activations is a popular approach for controlling text generation. Unsupervised dictionary learning methods, e.g., sparse autoencoders, can be scaled to produce many steering vectors, but lack guarantees on the individual efficacy of each vector and control over the coverage of relevant steering tasks. In contrast, supervised methods for constructing steering vectors are targeted and effective, but require more data collection and training for each additional steering vector produced. In this work, we introduce HyperSteer, a family of hypernetwork-based architectures which are trained end-to-end to generate steering vectors conditioned on the natural language steering prompts and the internals of the steered LM. In our evaluations, we show that scaling HyperSteer with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods, even on steering prompts never seen during training. Moreover, HyperSteer performs on par with steering-via-prompting.",
    "published": "Jun 03",
    "pdf_url": "https://arxiv.org/pdf/2506.03292v1",
    "arxiv_url": "http://arxiv.org/abs/2506.03292v1",
    "queried_author": "Christopher Potts",
    "matching_authors": [
      "Christopher Potts"
    ]
  },
  {
    "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers",
    "authors": [
      "Aditya Bharat Soni",
      "Boxuan Li",
      "Xingyao Wang",
      "Valerie Chen",
      "Graham Neubig"
    ],
    "summary": "Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.",
    "published": "Jun 03",
    "pdf_url": "https://arxiv.org/pdf/2506.03011v1",
    "arxiv_url": "http://arxiv.org/abs/2506.03011v1",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening",
    "authors": [
      "Andre He",
      "Daniel Fried",
      "Sean Welleck"
    ],
    "summary": "Reinforcement learning is emerging as a primary driver for improving language model reasoning capabilities. A fundamental question is whether current reinforcement learning algorithms -- such as Group Relative Policy Optimization (GRPO), the de facto standard algorithm used to improve language model reasoning -- merely sharpen the base model's distribution around problems it can already solve. We investigate this question in the context of formal theorem proving, which has access to a perfect verifier. We identify a degenerate rank bias in GRPO in which highly probable trajectories are reinforced and rare ones are neglected. This results in distribution sharpening: the model can solve some problems with fewer samples, but underperforms simply sampling more solutions from the original model. To overcome GRPO's rank bias we introduce unlikeliness reward, a simple method for explicitly up-weighting rare but correct solutions. We show that unlikeliness reward mitigates rank bias and improves pass@$N$ across a large range of $N$ in both synthetic and real theorem proving settings. We also uncover an unexpected link between rank bias and a seemingly mundane hyperparameter -- the number of updates per batch -- that leads to a second, complementary mitigation. We combine our insights into a revised GRPO training recipe for formal theorem proving, yielding an open pipeline that achieves competitive performance to DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation at https://github.com/AndreHe02/rewarding-unlikely-release",
    "published": "Jun 03",
    "pdf_url": "https://arxiv.org/pdf/2506.02355v2",
    "arxiv_url": "http://arxiv.org/abs/2506.02355v2",
    "queried_author": "Sean Welleck",
    "matching_authors": [
      "Sean Welleck"
    ]
  },
  {
    "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models",
    "authors": [
      "Lindia Tjuatja",
      "Graham Neubig"
    ],
    "summary": "Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\" and \"exclamation marks after emotional statements\", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.",
    "published": "Jun 02",
    "pdf_url": "https://arxiv.org/pdf/2506.02204v2",
    "arxiv_url": "http://arxiv.org/abs/2506.02204v2",
    "queried_author": "Graham Neubig",
    "matching_authors": [
      "Graham Neubig"
    ]
  },
  {
    "title": "AI Debate Aids Assessment of Controversial Claims",
    "authors": [
      "Salman Rahman",
      "Sheriff Issaka",
      "Ashima Suvarna",
      "Genglin Liu",
      "James Shiffer",
      "Jaeyoung Lee",
      "Md Rizwan Parvez",
      "Hamid Palangi",
      "Shi Feng",
      "Nanyun Peng",
      "Yejin Choi",
      "Julian Michael",
      "Liwei Jiang",
      "Saadia Gabriel"
    ],
    "summary": "As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI systems remain truthful even when their capabilities exceed those of their evaluators. Yet when humans serve as evaluators, their own beliefs and biases can impair judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial factuality claims on COVID-19 and climate change where people hold strong prior beliefs. We conduct two studies. Study I recruits human judges with either mainstream or skeptical beliefs who evaluate claims through two protocols: debate (interaction with two AI advisors arguing opposing sides) or consultancy (interaction with a single AI advisor). Study II uses AI judges with and without human-like personas to evaluate the same protocols. In Study I, debate consistently improves human judgment accuracy and confidence calibration, outperforming consultancy by 4-10% across COVID-19 and climate change claims. The improvement is most significant for judges with mainstream beliefs (up to +15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In Study II, AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without per...",
    "published": "Jun 02",
    "pdf_url": "https://arxiv.org/pdf/2506.02175v2",
    "arxiv_url": "http://arxiv.org/abs/2506.02175v2",
    "queried_author": "Yejin Choi",
    "matching_authors": [
      "Yejin Choi"
    ]
  },
  {
    "title": "RewardBench 2: Advancing Reward Model Evaluation",
    "authors": [
      "Saumya Malik",
      "Valentina Pyatkin",
      "Sander Land",
      "Jacob Morrison",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Nathan Lambert"
    ],
    "summary": "Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. In this paper, we describe our benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization.",
    "published": "Jun 02",
    "pdf_url": "https://arxiv.org/pdf/2506.01937v1",
    "arxiv_url": "http://arxiv.org/abs/2506.01937v1",
    "queried_author": "Hannaneh Hajishirzi",
    "matching_authors": [
      "Hannaneh Hajishirzi",
      "Noah A. Smith"
    ]
  },
  {
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability",
    "authors": [
      "Genta Indra Winata",
      "David Anugraha",
      "Emmy Liu",
      "Alham Fikri Aji",
      "Shou-Yi Hung",
      "Aditya Parashar",
      "Patrick Amadeus Irawan",
      "Ruochen Zhang",
      "Zheng-Xin Yong",
      "Jan Christian Blaise Cruz",
      "Niklas Muennighoff",
      "Seungone Kim",
      "Hanyang Zhao",
      "Sudipta Kar",
      "Kezia Erina Suryoraharjo",
      "M. Farid Adilazuarda",
      "En-Shiun Annie Lee",
      "Ayu Purwarianti",
      "Derry Tanti Wijaya",
      "Monojit Choudhury"
    ],
    "summary": "High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubr...",
    "published": "Jun 02",
    "pdf_url": "https://arxiv.org/pdf/2506.01789v2",
    "arxiv_url": "http://arxiv.org/abs/2506.01789v2",
    "queried_author": "Niklas Muennighoff",
    "matching_authors": [
      "Niklas Muennighoff",
      "Seungone Kim"
    ]
  },
  {
    "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning",
    "authors": [
      "Xinyu Zhu",
      "Mengzhou Xia",
      "Zhepei Wei",
      "Wei-Lin Chen",
      "Danqi Chen",
      "Yu Meng"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as Positive and Negative Sample Reinforcement (PSR and NSR), respectively. We train Qwen2.5-Math-7B, Qwen3-4B and Llama-3.1-8B-Instruct on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples -- without reinforcing correct responses -- can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum $k$ up to 256), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@1 but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves ...",
    "published": "Jun 02",
    "pdf_url": "https://arxiv.org/pdf/2506.01347v2",
    "arxiv_url": "http://arxiv.org/abs/2506.01347v2",
    "queried_author": "Danqi Chen",
    "matching_authors": [
      "Danqi Chen"
    ]
  },
  {
    "title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
    "authors": [
      "Zhili Feng",
      "Yixuan Even Xu",
      "Alexander Robey",
      "Robert Kirk",
      "Xander Davies",
      "Yarin Gal",
      "Avi Schwarzschild",
      "J. Zico Kolter"
    ],
    "summary": "Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.",
    "published": "May 31",
    "pdf_url": "https://arxiv.org/pdf/2506.00688v1",
    "arxiv_url": "http://arxiv.org/abs/2506.00688v1",
    "queried_author": "J Zico Kolter",
    "matching_authors": [
      "J Zico Kolter"
    ]
  },
  {
    "title": "WorldGym: World Model as An Environment for Policy Evaluation",
    "authors": [
      "Julian Quevedo",
      "Ansh Kumar Sharma",
      "Yixiang Sun",
      "Varad Suryavanshi",
      "Percy Liang",
      "Sherry Yang"
    ],
    "summary": "Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.",
    "published": "May 31",
    "pdf_url": "https://arxiv.org/pdf/2506.00613v3",
    "arxiv_url": "http://arxiv.org/abs/2506.00613v3",
    "queried_author": "Percy Liang",
    "matching_authors": [
      "Percy Liang"
    ]
  },
  {
    "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race",
    "authors": [
      "Lihao Sun",
      "Chengzhi Mao",
      "Valentin Hofmann",
      "Xuechunzi Bai"
    ],
    "summary": "Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.",
    "published": "May 30",
    "pdf_url": "https://arxiv.org/pdf/2506.00253v3",
    "arxiv_url": "http://arxiv.org/abs/2506.00253v3",
    "queried_author": "Valentin Hofmann",
    "matching_authors": [
      "Valentin Hofmann"
    ]
  },
  {
    "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences",
    "authors": [
      "Mingqian Zheng",
      "Wenjia Hu",
      "Patrick Zhao",
      "Motahhare Eslami",
      "Jena D. Hwang",
      "Faeze Brahman",
      "Carolyn Rose",
      "Maarten Sap"
    ],
    "summary": "Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.",
    "published": "May 30",
    "pdf_url": "https://arxiv.org/pdf/2506.00195v2",
    "arxiv_url": "http://arxiv.org/abs/2506.00195v2",
    "queried_author": "Maarten Sap",
    "matching_authors": [
      "Maarten Sap"
    ]
  }
]